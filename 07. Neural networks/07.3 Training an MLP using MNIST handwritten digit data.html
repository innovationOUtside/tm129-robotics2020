
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3 Training an MLP using MNIST handwritten digit data &#8212; TM129 Robotics Practical Activities</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4 Testing a network against synthetic data (optional)" href="07.4%20Testing%20a%20network%20against%20synthetic%20data%20%28optional%29.html" />
    <link rel="prev" title="2 Multidimensional data and the MLP" href="07.2%20Multidimensional%20data%20and%20the%20MLP.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">TM129 Robotics Practical Activities</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README_FIRST.html">
   Welcome to the TM129 Robotics block
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00_FOR_VLE/Section_00_01_Introduction.html">
   1 Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00_NOTES_FOR_TUTORS/GETTING_STARTED.html">
   Getting started
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.1%20Jupyter%20environment.html">
   1 Introduction to the TM129 Jupyter notebook environment
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.2%20Exploring%20the%20notebook%20environment.html">
     2 The interactive read-writable notebook environment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.3%20Introducing%20nbev3devsim.html">
     3 The RoboLab simulated on-screen robot (
     <code class="docutils literal notranslate">
      <span class="pre">
       nbev3devsim
      </span>
     </code>
     )
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.4%20Exploring%20nbev3devsim.html">
     4 Exploring the
     <code class="docutils literal notranslate">
      <span class="pre">
       nbev3devsim
      </span>
     </code>
     simulator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.5%20Example%20robot%20program.html">
     5 An example robot program
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.6%20Working%20With%20Simulators.html">
     6 Working with simulators
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.1%20Robot%20programming%20constructs.html">
   1 An introduction to programming robots
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.2%20Creating%20your%20own%20robot%20programs.html">
     2 Creating your own robot programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.3%20General%20programming%20concepts.html">
     3.1 Constants and variables in programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.4%20Getting%20started%20with%20sensors.html">
     4 Robot sensors and data logging
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.1%20Program%20control%20using%20for%20loops.html">
   1 Introduction to program control flow
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.2%20Program%20control%20using%20while%20loops%20and%20blocking.html">
     2 Program control flow using a
     <code class="docutils literal notranslate">
      <span class="pre">
       while...
      </span>
     </code>
     loop
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.3%20Program%20control%20flow%20using%20branches.html">
     3 Branches
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.4%20Example%20robot%20control%20programs.html">
     4 Example robot control programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.5%20Some%20RoboLab%20challenges.html">
     5 RoboLab challenges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.6%20Optional%20RoboLab%20challenges.html">
     6 Optional challenges
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.1%20Introducing%20program%20functions.html">
   1 Introduction to functions and robot control strategies
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.2%20Robot%20navigation%20using%20dead%20reckoning.html">
     2 Dead reckoning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.3%20Emergent%20robot%20behaviour%20and%20simple%20data%20charts.html">
     3 Emergent robot behaviour and simple data charts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.4%20Reasoning%20with%20Eliza.html">
     4 Reasoning with Eliza
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.5%20Reasoning%20with%20rule%20based%20systems.html">
     5 Reasoning with rule-based systems – Durable Rules Engine
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05.%20Experimenting%20with%20sensors/05.1%20Introducing%20sensor%20based%20control.html">
   Introduction to sensor-based control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06.%20Where%20in%20the%20world%20are%20we/06.1%20Introducing%20sensor%20based%20navigation.html">
   Introducing sensor-based navigation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07.1%20Introducing%20neural%20networks.html">
   1 Introducing neural networks
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.1%20Introducing%20remote%20services%20and%20multi-agent%20systems.html">
   1 An introduction to remote services and multi-agent systems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.2%20Collecting%20digit%20image%20and%20class%20data%20from%20the%20simulator.html">
     2 Collecting digit image and class data from the simulator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.3%20Recognising%20digits%20using%20a%20convolutional%20neural%20network%20%28optional%29.html">
     3 Recognising digits using a convolutional neural network (optional)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.4%20Recognising%20patterns%20on%20the%20move.html">
     4 Recognising patterns on the move
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.5%20Messaging%20in%20multi-agent%20systems.html">
     5 Messaging in multi-agent systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.6%20Conclusion.html">
     6 Conclusion
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/07. Neural networks/07.3 Training an MLP using MNIST handwritten digit data.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/07. Neural networks/07.3 Training an MLP using MNIST handwritten digit data.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extracting-mnist-training-images-from-an-image-data-file">
   3.1 Extracting MNIST training images from an image data file
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#importing-the-mnist-data-image">
     3.1.1 Importing the MNIST data image
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extracting-individual-digit-images">
     3.1.2 Extracting individual digit images
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#viewing-an-individual-digit-image-as-data">
     3.1.3 Viewing an individual digit image as data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#so-what">
     3.1.4 So what?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparing-the-mnist-image-training-data">
   3.2 Preparing the MNIST image training data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grabbing-random-test-images-and-their-labels">
     3.2.1 Grabbing random test images and their labels
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-a-simple-mlp-on-the-mnist-image-data">
   3.3 Training a simple MLP on the MNIST image data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-mlp">
     3.3.1 Training the MLP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-the-performance-of-the-network">
     3.3.2 Testing the performance of the network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-a-single-random-image">
     3.3.3 Testing a single random image
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probing-the-mlps-confidence-in-its-predictions">
     3.3.4 Probing the MLP’s confidence in its predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-the-mlp-against-multiple-images">
     3.3.5 Testing the MLP against multiple images
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-the-mlp-using-multiple-random-images">
     3.3.6 Testing the MLP using multiple random images
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rapidly-retraining-the-mlp">
   3.4 Rapidly retraining the MLP
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-the-mlp">
   3.7 Saving the MLP
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-activity-visualising-the-trained-mlp-weights">
   3.8 Optional activity – Visualising the trained MLP weights
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   3.9 Summary
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="training-an-mlp-using-mnist-handwritten-digit-data">
<h1>3 Training an MLP using MNIST handwritten digit data<a class="headerlink" href="#training-an-mlp-using-mnist-handwritten-digit-data" title="Permalink to this headline">¶</a></h1>
<p>In this notebook, you will be working with a very famous dataset known as the <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST database of handwritten images</a> (<em>Modified National Institute of Standards and Technology</em> database).</p>
<p>The complete dataset comprises a training set of 60,000 example images and a test set containing a further 10,000 examples. Each of the handwritten digit images have been ‘size-normalised’ to the same image size. In addition, each sample digit has been centred within the fixed-size image.</p>
<div class="section" id="extracting-mnist-training-images-from-an-image-data-file">
<h2>3.1 Extracting MNIST training images from an image data file<a class="headerlink" href="#extracting-mnist-training-images-from-an-image-data-file" title="Permalink to this headline">¶</a></h2>
<p>The MNIST handwritten image dataset has been widely used for benchmarking the performance of different machine-learning techniques, particularly in their early stages of development. It is also widely used for demonstration purposes, and you will meet several neural networks that were originally trained on the dataset in later notebooks.</p>
<p>In order to work with the dataset, we need to access it somehow. One common way of distributing the dataset is to encode all the handwritten digit image files, or batches or them, within another image file. In the example we will be working with, where each handwritten digit image is represented as a 28 × 28 pixel greyscale image, each row of the ‘distribution’ image file contains the 28 × 28 × 1 = 784 pixel values that represent a single 28 × 28 pixel handwritten digit image.</p>
<p>This is what one of the distribution data image files looks like:</p>
<p><img alt="" src="../_images/mnist_batch_0.png" /></p>
<p>At first glance, it doesn’t look like a lot of handwritten digits, does it?</p>
<p>So let’s investigate that large image file a little bit more.</p>
<p><em>This notebook includes quite a lot of fiddly bits of code to handle the MNIST images data. You are not expected to be able to write this sort of code, nor even to necessarily understand it. It is provided as a demonstration to illustrate the process steps and the sort of code required to actually make use of the data.</em></p>
<div class="section" id="importing-the-mnist-data-image">
<h3>3.1.1 Importing the MNIST data image<a class="headerlink" href="#importing-the-mnist-data-image" title="Permalink to this headline">¶</a></h3>
<p>When displayed as an image, the image is 784 pixels wide and 3000 pixels high, which we can see from the size of the image if we load it in to Python as an image object:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Load in the image data file</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;mnist_batch_0.png&#39;</span><span class="p">)</span>

<span class="c1"># If the image is a colour image, we can use various tools</span>
<span class="c1"># to convert it to a greyscale image</span>
<span class="c1"># .convert(&quot;L&quot;)</span>

<span class="c1"># Display the size of the image as (rows, columns)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The image size, given as (columns, rows), is </span><span class="si">{</span><span class="n">img</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This image itself contains 3000 lines of MNIST image data, corresponding to 3000 separate handwritten digit images. The 784 columns in each row represent a linearised version of the 28 × 28 = 784 values that represent the values of each pixel in each <code class="docutils literal notranslate"><span class="pre">28&amp;nbsp;×&amp;nbsp;28</span></code> pixel handwritten digit image.</p>
<p>We can preview what one of the rows looks like by running the following code cell:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># If we convert the image data to a one-dimensional array (i.e. a list of values)</span>
<span class="c1"># the first 784 elements will represent the contents of the first row</span>
<span class="c1"># That is, a linear representation of the first 28 x 28 pixel sized handwritten digit image</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">getdata</span><span class="p">())[:</span><span class="mi">784</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We can inspect the image object to see how the data has been encoded:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">img</span><span class="o">.</span><span class="n">getbands</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>In this case, from the <a class="reference external" href="https://pillow.readthedocs.io/en/4.1.x/handbook/concepts.html#modes"><code class="docutils literal notranslate"><span class="pre">PIL</span></code> package documentation</a>, we see that mode <code class="docutils literal notranslate"><span class="pre">L</span></code> corresponds to a black-and-white image encoded with 8-bit pixels, defining each pixel as an integer with one of <span class="math notranslate nohighlight">\(2^8\)</span> values, which is to say an integer in the range <code class="docutils literal notranslate"><span class="pre">0...255</span></code>, as can be seen from the preview of the first row of the image data.</p>
</div>
<div class="section" id="extracting-individual-digit-images">
<h3>3.1.2 Extracting individual digit images<a class="headerlink" href="#extracting-individual-digit-images" title="Permalink to this headline">¶</a></h3>
<p>What happens if we take one of these rows of data, cast it into its own 28 × 28 array, and convert it to an image file format?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Turn the image data into a multidimensional array</span>
<span class="c1"># of 3000 separate 28 x 28 arrays</span>
<span class="n">images_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>

<span class="c1"># Get the third item (index value 2), that is, the third 28 x 28 image data array</span>
<span class="n">image_array</span> <span class="o">=</span> <span class="n">images_array</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># And convert it to an image</span>
<span class="n">image_image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">image_array</span><span class="p">)</span>

<span class="c1"># Then display it</span>
<span class="n">image_image</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">sensor_data.image_from_array()</span></code> function will also create the image for us using the same approach:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">image_from_array</span>

<span class="n">image_from_array</span><span class="p">(</span><span class="n">image_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can zoom in on an image to look at it in more detail using the <code class="docutils literal notranslate"><span class="pre">nn_tools.sensor_data.zoom_img</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">zoom_img</span>

<span class="n">zoom_img</span><span class="p">(</span><span class="n">image_image</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We now see the handwritten digit not as a series of numbers but as an actual image.</p>
<p>For convenience, if we want to work with a list of images, we can generate one using the <code class="docutils literal notranslate"><span class="pre">get_images_list_from_images_array()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">get_images_list_from_images_array</span>

<span class="n">images_list</span> <span class="o">=</span> <span class="n">get_images_list_from_images_array</span><span class="p">(</span><span class="n">images_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="viewing-an-individual-digit-image-as-data">
<h3>3.1.3 Viewing an individual digit image as data<a class="headerlink" href="#viewing-an-individual-digit-image-as-data" title="Permalink to this headline">¶</a></h3>
<p>We can also view the image data in a <em>pandas</em> dataframe, trimming the dataframe to remove background coloured edging.</p>
<p><em>By default, the dataframe returning functions will preview the dataframe with colour highlight; pass the attribute <code class="docutils literal notranslate"><span class="pre">show=False</span></code> to disable this view.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">trim_image</span><span class="p">,</span> <span class="n">df_from_image</span>

<span class="n">trimmed_image</span> <span class="o">=</span> <span class="n">trim_image</span><span class="p">(</span> <span class="n">df_from_image</span><span class="p">(</span><span class="n">image_image</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">background</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about how the numeric values associated with each pixel correspond to the way they are rendered in the original image and in the false-colour dataframe heat map shown above.</em></p>
<p>To recap, the original <em>mnist_batch_0.png</em> file, which just happened to be an image file and could be viewed as such, was actually being used as a convenient way of transporting 3000 rows of data. In turn, each row of data could itself be transformed into a square data array that could then be rendered as a distinct handwritten digit image. The image itself, of course, is just numbers underneath.</p>
</div>
<div class="section" id="so-what">
<h3>3.1.4 So what?<a class="headerlink" href="#so-what" title="Permalink to this headline">¶</a></h3>
<p>At this point, you may be wondering what this has to do with training neural networks, let alone programming robots. What the example serves to demonstrate is that training a neural network on some test data may require a range of computer skills and knowledge to even get the data into a form where you can begin to make use of it.</p>
<p>Working with file formats and raw data representations often represents a large part of the workload associated with any data analysis, modelling, or classification task, and often requires significant computational data-handling skills. Whilst we don’t expect you to learn how to perform these data-wrangling tasks yourself as part of this module, you should be aware that when you see recipes saying things like ‘<em>just load in the dataset…</em>’, there may be quite a lot of work associated with that word, <em>just</em>.</p>
<p><em>To learn more about working with data along the whole data pipeline – from data acquisition, to data cleaning, management, storage, analysis and presentation – consider taking the Open University module <a class="reference external" href="http://www.open.ac.uk/courses/modules/tm351">TM351 <em>Data management and analysis</em></a>.</em></p>
</div>
</div>
<div class="section" id="preparing-the-mnist-image-training-data">
<h2>3.2 Preparing the MNIST image training data<a class="headerlink" href="#preparing-the-mnist-image-training-data" title="Permalink to this headline">¶</a></h2>
<p>Although MLP classifiers can struggle with large images, the 28 × 28 pixel image size used for the MNIST images is not too large to train an MLP on, although it does require an input layer containing 784 neurons, one for each pixel.</p>
<p>To train the MLP on the linearised pixel values, we need to present labelled images that identify the category (that is, the digit) that each image represents.</p>
<p>The training labels are provided in a separate file which we can load in as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="c1"># The labels.txt file contains 3000 digit labels in the same order as the image data file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;labels.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="c1"># Show the length of the label array and the value of the first 10 digits</span>
<span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">labels</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="grabbing-random-test-images-and-their-labels">
<h3>3.2.1 Grabbing random test images and their labels<a class="headerlink" href="#grabbing-random-test-images-and-their-labels" title="Permalink to this headline">¶</a></h3>
<p>The following cell imports a helper function to retrieve a random image from the images array, or the image corresponding to the provided index value.</p>
<p>Run the following cell repeatedly to try the <code class="docutils literal notranslate"><span class="pre">get_random_image()</span></code> function out:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">get_random_image</span>
                 
<span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_random_image</span><span class="p">(</span><span class="n">images_array</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Rerun the previous cell several times to preview the display of different randomly selected images.</em></p>
<p><em>Record any observations or notes here that you would like to make about the images you observed.</em></p>
<p>To access a particular image, such as the first image in the dataset, pass its index:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_random_image</span><span class="p">(</span><span class="n">images_array</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="training-a-simple-mlp-on-the-mnist-image-data">
<h2>3.3 Training a simple MLP on the MNIST image data<a class="headerlink" href="#training-a-simple-mlp-on-the-mnist-image-data" title="Permalink to this headline">¶</a></h2>
<p>We can now train a simple MLP from the MNIST data and the training labels.</p>
<p>The <em>scikit-learn</em> <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> can automatically identify from a training set the number of nodes required for the input and output layers, so all we need to provide is the hidden layer(s) definition.</p>
<p><em>One of the intuitions for making sense of how neural networks work is to consider each thing to be recognised as being represented by a vector. In the case of the MNIST images, this is a 784-dimensional vector (which is rather more dimensions than the three-dimensional world we are familiar with!).</em></p>
<p><em>The classification task is then one of trying to classify things as belonging to the same group if they are pointing in roughly the same direction in this high-dimensional space.</em></p>
<p><em>The point of normalising the lengths of the vectors is because we are most interested in the direction they are pointing, not how far along the path they are.</em></p>
<div class="section" id="training-the-mlp">
<h3>3.3.1 Training the MLP<a class="headerlink" href="#training-the-mlp" title="Permalink to this headline">¶</a></h3>
<p>For starters, let’s see if we can train the network to classify the images using a single layer containing 40 hidden neurons:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">40</span><span class="p">)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">40</span>

<span class="n">MLP</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iterations</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="c1"># For reproducibility, set the inital random state to a specified seed value</span>
                    <span class="c1">#random_state=1,</span>
                   <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We need to present the data as a list (that is, as a one-dimensional linear array) of lists (or vectors). Each vector contains 784 values, corresponding to the 28 × 28 pixels in each image.</p>
<p>To simplify training, we also normalise the length of each of these image vectors so the length of each vector is 1.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">images_list</span></code> contains the image data as a list of images. We can retrieve the pixel data for an image as a list (<code class="docutils literal notranslate"><span class="pre">list(image.getdata())</span></code>), and then normalise the image vectors using the <em>scikit-learn</em> <code class="docutils literal notranslate"><span class="pre">normalize</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">normalize</span>

<span class="n">images_data</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># For each image in the images_list</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images_list</span><span class="p">:</span>
    <span class="c1"># Get the data as a list</span>
    <span class="c1"># and append it to the images_data list</span>
    <span class="n">images_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">getdata</span><span class="p">()))</span>

<span class="c1"># The axis=1 argument normalises each individual image vector</span>
<span class="n">normalised_images_data</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">images_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># This functionality also made available as:</span>
<span class="c1"># nn_tools.sensor_data.get_images_features(images_list, normalise=True)</span>
</pre></div>
</div>
</div>
</div>
<p>When training the network, we can use the first 2900 images as a training set and hold back 100 images to use as a ‘previously unseen’ image test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_limit</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_limit</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">normalised_images_data</span><span class="p">)</span> <span class="o">-</span> <span class="n">test_limit</span>

<span class="c1"># Train the MLP on a subset of the images</span>

<span class="n">MLP</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">normalised_images_data</span><span class="p">[:</span><span class="n">train_limit</span><span class="p">],</span> <span class="n">labels</span><span class="p">[:</span><span class="n">train_limit</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Review the shape of the trained network and the loss function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">network_structure</span><span class="p">,</span> <span class="n">show_loss</span>

<span class="n">network_structure</span><span class="p">(</span><span class="n">MLP</span><span class="p">)</span>
<span class="n">show_loss</span><span class="p">(</span><span class="n">MLP</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations about the shape of the loss curve. Does it plateau towards a flat line?</em></p>
<p><em>If the line hasn’t flattened off, but is still falling steeply, consider changing the number of hidden layers, and/or the maximum number of training iterations and retraining the network. Can you flatten the loss curve?</em></p>
<p><em>Take care not to push the network too far. If you keep on training it, it will get better and better at classifying the images you present it with, but its performance against previously unseen images is likely to degrade as the network <code class="docutils literal notranslate"><span class="pre">overfits</span></code> itself to the training data.</em></p>
</div>
<div class="section" id="testing-the-performance-of-the-network">
<h3>3.3.2 Testing the performance of the network<a class="headerlink" href="#testing-the-performance-of-the-network" title="Permalink to this headline">¶</a></h3>
<p>With the network trained, we can check how well it performs on the images in the training set using the classification report and confusion matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>


<span class="n">training_data</span> <span class="o">=</span> <span class="n">normalised_images_data</span><span class="p">[:</span><span class="n">train_limit</span><span class="p">]</span>
<span class="n">training_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="n">train_limit</span><span class="p">]</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">MLP</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">classification_report</span><span class="p">(</span><span class="n">training_labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Confusion matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">training_labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><em>Recall that the precision and recall scores range from 0 (worst) to 1 (best), with the precision relating the number of true positives and false positives, and the recall score relating the number of true positives and false negatives; the f1 score attempts to combine the sense of precision and recall as a weighted average of those two scores, with a value of 1 representing the best performance and 0 the worst.</em></p>
<p>If the <em>precision</em>, <em>recall</em> and <em>f1-score</em> values are close to 1, that’s a good sign; and if large values predominate on the diagonal of the confusion matrix, that shows that most digits are identified correctly.</p>
<p><em>Record your own observations about the performance of the network. Does it appear to struggle with any particular images?</em></p>
<p><em>If the network is performing really badly, consider changing the number of hidden layers, and/or the maximum number of training iterations and then retrain the network.</em></p>
</div>
<div class="section" id="testing-a-single-random-image">
<h3>3.3.3 Testing a single random image<a class="headerlink" href="#testing-a-single-random-image" title="Permalink to this headline">¶</a></h3>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">nn_tools.network_views.predict_and_report_from_image()</span></code> function in the following example to test against a randomly selected image and its classification label.</p>
<p>Omitting the test label will just return the prediction. A zoomed view of the sample image can be seen by setting <code class="docutils literal notranslate"><span class="pre">zoomview=True</span></code> when calling the function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">predict_and_report_from_image</span> 

<span class="c1"># Get a test image</span>
<span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_random_image</span><span class="p">(</span><span class="n">images_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
 
<span class="c1"># And test the trained MLP against it</span>
<span class="n">predict_and_report_from_image</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="probing-the-mlps-confidence-in-its-predictions">
<h3>3.3.4 Probing the MLP’s confidence in its predictions<a class="headerlink" href="#probing-the-mlps-confidence-in-its-predictions" title="Permalink to this headline">¶</a></h3>
<p>Even though it may be hard for us to see from the network’s weights exactly what is going on, the network appears to be doing its job in terms of classifying digits, at least when it comes to the sample images.</p>
<p>By passing the <code class="docutils literal notranslate"><span class="pre">confidence=True</span></code> parameter to the <code class="docutils literal notranslate"><span class="pre">predict_and_report_from_image()</span></code> function, we can display a bar chart showing the confidence of the prediction for each class:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">predict_and_report_from_image</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">test_image</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Generate and test several more random image selection predictions and then record you own observations here about how confident the network was in its prediction.</em></p>
</div>
<div class="section" id="testing-the-mlp-against-multiple-images">
<h3>3.3.5 Testing the MLP against multiple images<a class="headerlink" href="#testing-the-mlp-against-multiple-images" title="Permalink to this headline">¶</a></h3>
<p>As well as testing the network against data it has already seen, we can also test it against images we held back and that it hasn’t seen before. Once again, we can review the effectiveness of the network by means of the classification report and confusion matrix, this time handled more conveniently using the <code class="docutils literal notranslate"><span class="pre">test_and_report_image_data()</span></code> function from <code class="docutils literal notranslate"><span class="pre">nn_tools.network_views</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">test_and_report_image_data</span>

<span class="n">testing_data</span> <span class="o">=</span> <span class="n">normalised_images_data</span><span class="p">[</span><span class="n">train_limit</span><span class="p">:]</span>
<span class="n">testing_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">train_limit</span><span class="p">:]</span>

<span class="n">test_and_report_image_data</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">testing_data</span><span class="p">,</span> <span class="n">testing_labels</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about the classification report and the confusion matrix. Does the network appear to be confused by any images in particular?</em></p>
<p>We can summarise the performance using the MLP <code class="docutils literal notranslate"><span class="pre">.score()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">MLP</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">MLP</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">testing_data</span><span class="p">,</span> <span class="n">testing_labels</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p><em>You will have an opportunity to explore other MLP configurations and training regimes later in this notebook to see if you can improve the performance of the network.</em></p>
</div>
<div class="section" id="testing-the-mlp-using-multiple-random-images">
<h3>3.3.6 Testing the MLP using multiple random images<a class="headerlink" href="#testing-the-mlp-using-multiple-random-images" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">nn_tools.network_views.test_and_report_random_images()</span></code> can be used to test the trained MLP against a specified number of samples, with samples picked by a specified function, such as our <code class="docutils literal notranslate"><span class="pre">get_random_image()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">test_and_report_random_images</span>

<span class="n">test_and_report_random_images</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span>
                              <span class="n">get_random_image</span><span class="p">,</span> <span class="n">images_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                              <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about how well the network performs. Does it appear to be confused by any images in particular?</em></p>
</div>
</div>
<div class="section" id="rapidly-retraining-the-mlp">
<h2>3.4 Rapidly retraining the MLP<a class="headerlink" href="#rapidly-retraining-the-mlp" title="Permalink to this headline">¶</a></h2>
<p>In defining the MLP originally, we specified a maximum number of training iterations, as well as the <em>verbose</em> reporting option. By default, a progress bar display is not available when training the MLP, but we can create one by defining a minimal MLP, training it on a single iteration to define the classes, and then training it across multiple iterations using the <code class="docutils literal notranslate"><span class="pre">.partial_fit()</span></code> method, which applies additional training iterations to the MLP.</p>
<p>This approach has been implemented as the function <code class="docutils literal notranslate"><span class="pre">network_views.progress_tracked_training()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">quick_progress_tracked_training</span>
<span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">get_images_features</span>

<span class="c1"># Specify some parameters</span>
<span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">40</span><span class="p">)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Identify training images and labels</span>
<span class="n">training_images</span> <span class="o">=</span> <span class="n">images_list</span><span class="p">[:</span><span class="n">train_limit</span><span class="p">][:</span><span class="n">train_limit</span><span class="p">]</span>
<span class="n">training_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="n">train_limit</span><span class="p">]</span>

<span class="c1"># Create a new MLP</span>
<span class="n">MLP2</span> <span class="o">=</span> <span class="n">quick_progress_tracked_training</span><span class="p">(</span><span class="n">training_images</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">,</span>
                                      <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span>
                                      <span class="n">max_iterations</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">report</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Top up an existing MLP</span>
<span class="n">MLP2</span> <span class="o">=</span> <span class="n">quick_progress_tracked_training</span><span class="p">(</span><span class="n">training_images</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">,</span>
                                      <span class="n">MLP</span><span class="o">=</span><span class="n">MLP2</span><span class="p">,</span>
                                      <span class="n">max_iterations</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">report</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Test and report on this trained network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_and_report_random_images</span><span class="p">(</span><span class="n">MLP2</span><span class="p">,</span>
                              <span class="n">get_random_image</span><span class="p">,</span> <span class="n">images_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                              <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can also create a simple end-user application to help us train the network in a more interactive fashion:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact_manual</span>

<span class="n">MLP3</span><span class="o">=</span><span class="kc">None</span>

<span class="nd">@interact_manual</span><span class="p">(</span><span class="n">iterations</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
          <span class="n">h1</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">h2</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">trainer</span><span class="p">(</span><span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">h1</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">h2</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">updater</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">MLP3</span>
    <span class="n">MLP3</span> <span class="o">=</span> <span class="n">quick_progress_tracked_training</span><span class="p">(</span><span class="n">training_images</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">,</span>
                                 <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span>
                                 <span class="n">max_iterations</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                                 <span class="n">MLP</span> <span class="o">=</span> <span class="n">MLP3</span> <span class="k">if</span> <span class="n">updater</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">loss</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># show loss function</span>
                                 <span class="n">structure</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># show network params</span>
                                <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Feel free to experiment with training the network using different setups. But don’t spend too much time playing!</em></p>
<p><em>Record any notes and observations you care to make regarding any experimentation you carried out with respect to the training and testing of the MLP.</em></p>
</div>
<div class="section" id="saving-the-mlp">
<h2>3.7 Saving the MLP<a class="headerlink" href="#saving-the-mlp" title="Permalink to this headline">¶</a></h2>
<p>We can persist the model by saving it to a file using a variant of the of Python <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module, as described in the <a class="reference external" href="https://scikit-learn.org/stable/modules/model_persistence.html"><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> documentation</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">dump</span>

<span class="n">dump</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="s1">&#39;mlp_mnist_28x28.joblib&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<p>This is particularly important if we want to share the trained model, not least in situations where it may take some considerable time to train the model.</p>
<p>We can load it back in again in the following way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">load</span>

<span class="n">MLP</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;mlp_mnist_28x28.joblib&#39;</span><span class="p">)</span>

<span class="c1"># Test that it still works...</span>
<span class="n">predict_and_report_from_image</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="optional-activity-visualising-the-trained-mlp-weights">
<h2>3.8 Optional activity – Visualising the trained MLP weights<a class="headerlink" href="#optional-activity-visualising-the-trained-mlp-weights" title="Permalink to this headline">¶</a></h2>
<p><em>This is an experimental optional activity. It is still quite brittle. Click the arrow in the sidebar, or run this cell, to view the activity.</em></p>
<p>In passing, we can plot the 28 × 28 incoming weights into the hidden layer neurons in a 28 × 28 grid to see how they filter the input values. The code is rather fiddly, so don’t try to make too much sense of it. You will notice that to human eyes at least, none of the input neurons has weights that apparently encode directly for a particular handwritten integer (1, 2, 3 etc.).</p>
<p>Note that this only works at the moment for a single-layer network with 40 hidden nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">preview_weights</span>

<span class="n">preview_weights</span><span class="p">(</span><span class="n">MLP</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="summary">
<h2>3.9 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In this notebook, we have explored the MNIST handwritten digit image dataset and trained a multi-layer perceptron against part of it. The MLP takes as input the 28 × 28 original pixel values used to represent each handwritten digit image and one of 10 predicted classification outputs. To validate the trained network, it was tested against some test images that we held back from the original training set.</p>
<p>But how robust is our network when it comes to classifying images that were perhaps not in the original dataset at all?</p>
<p>For example, will the network still recognise an image if we slightly recentre it in the original image frame?</p>
<p>The next two notebooks contain optional study material. The first provides you with an opportunity to see just how well the network is able to recognise images that may look much the same to us as the original images, but actually have quite distinct features from the original training data. The second develops further intuitions about what it actually is that we are providing as input data to a neural network.</p>
<p>The required material continues with a look ‘inside the mind’ of a neural network, exploring how we can start to visualise the network structure and behaviour to get a better understanding of what it is actually doing.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./07. Neural networks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="07.2%20Multidimensional%20data%20and%20the%20MLP.html" title="previous page">2 Multidimensional data and the MLP</a>
    <a class='right-next' id="next-link" href="07.4%20Testing%20a%20network%20against%20synthetic%20data%20%28optional%29.html" title="next page">4 Testing a network against synthetic data (optional)</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>