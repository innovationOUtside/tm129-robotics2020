
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5 Feature engineering (optional) &#8212; TM129 Robotics Practical Activities</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6 Inside the mind of a neural network" href="07.6%20Inside%20the%20mind%20of%20a%20neural%20network.html" />
    <link rel="prev" title="4 Testing a network against synthetic data (optional)" href="07.4%20Testing%20a%20network%20against%20synthetic%20data%20%28optional%29.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">TM129 Robotics Practical Activities</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README_FIRST.html">
   Welcome to the TM129 Robotics block
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00_FOR_VLE/Section_00_01_Introduction.html">
   1 Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00_NOTES_FOR_TUTORS/GETTING_STARTED.html">
   Getting started
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.1%20Jupyter%20environment.html">
   1 Introduction to the TM129 Jupyter notebook environment
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.2%20Exploring%20the%20notebook%20environment.html">
     2 The interactive read-writable notebook environment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.3%20Introducing%20nbev3devsim.html">
     3 The RoboLab simulated on-screen robot (
     <code class="docutils literal notranslate">
      <span class="pre">
       nbev3devsim
      </span>
     </code>
     )
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.4%20Exploring%20nbev3devsim.html">
     4 Exploring the
     <code class="docutils literal notranslate">
      <span class="pre">
       nbev3devsim
      </span>
     </code>
     simulator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.5%20Example%20robot%20program.html">
     5 An example robot program
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.6%20Working%20With%20Simulators.html">
     6 Working with simulators
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.1%20Robot%20programming%20constructs.html">
   1 An introduction to programming robots
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.2%20Creating%20your%20own%20robot%20programs.html">
     2 Creating your own robot programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.3%20General%20programming%20concepts.html">
     3.1 Constants and variables in programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.4%20Getting%20started%20with%20sensors.html">
     4 Robot sensors and data logging
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.1%20Program%20control%20using%20for%20loops.html">
   1 Introduction to program control flow
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.2%20Program%20control%20using%20while%20loops%20and%20blocking.html">
     2 Program control flow using a
     <code class="docutils literal notranslate">
      <span class="pre">
       while...
      </span>
     </code>
     loop
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.3%20Program%20control%20flow%20using%20branches.html">
     3 Branches
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.4%20Example%20robot%20control%20programs.html">
     4 Example robot control programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.5%20Some%20RoboLab%20challenges.html">
     5 RoboLab challenges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.6%20Optional%20RoboLab%20challenges.html">
     6 Optional challenges
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.1%20Introducing%20program%20functions.html">
   1 Introduction to functions and robot control strategies
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.2%20Robot%20navigation%20using%20dead%20reckoning.html">
     2 Dead reckoning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.3%20Emergent%20robot%20behaviour%20and%20simple%20data%20charts.html">
     3 Emergent robot behaviour and simple data charts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.4%20Reasoning%20with%20Eliza.html">
     4 Reasoning with Eliza
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.5%20Reasoning%20with%20rule%20based%20systems.html">
     5 Reasoning with rule-based systems – Durable Rules Engine
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05.%20Experimenting%20with%20sensors/05.1%20Introducing%20sensor%20based%20control.html">
   Introduction to sensor-based control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06.%20Where%20in%20the%20world%20are%20we/06.1%20Introducing%20sensor%20based%20navigation.html">
   Introducing sensor-based navigation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07.1%20Introducing%20neural%20networks.html">
   1 Introducing neural networks
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.1%20Introducing%20remote%20services%20and%20multi-agent%20systems.html">
   1 An introduction to remote services and multi-agent systems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.2%20Collecting%20digit%20image%20and%20class%20data%20from%20the%20simulator.html">
     2 Collecting digit image and class data from the simulator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.3%20Recognising%20digits%20using%20a%20convolutional%20neural%20network%20%28optional%29.html">
     3 Recognising digits using a convolutional neural network (optional)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.4%20Recognising%20patterns%20on%20the%20move.html">
     4 Recognising patterns on the move
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.5%20Messaging%20in%20multi-agent%20systems.html">
     5 Messaging in multi-agent systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08.%20Remote%20services%20and%20multi-agent%20systems/08.6%20Conclusion.html">
     6 Conclusion
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/07. Neural networks/07.5 Feature engineering (optional).ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/07. Neural networks/07.5 Feature engineering (optional).ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-a-baseline-mlp-using-an-original-image-training-set">
   5.1 Creating a baseline MLP using an original image training set
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#improving-the-network-performance-on-translated-images">
   5.2 Improving the network performance on translated images
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#identifying-new-features">
     5.2.1 Identifying new features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activity-what-distinguishes-one-digit-from-another">
     5.2.2 Activity – What distinguishes one digit from another?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-observations">
       Example observations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-an-image-signature">
     5.2.3 Generating an image signature
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-signatures-in-bulk">
     5.2.4 Creating signatures in bulk
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-an-mlp-using-the-line-crossing-signature-features">
     5.2.5 Training an MLP using the line-crossing signature features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-and-testing-the-network-against-smaller-images">
   5.3 Training and testing the network against smaller images
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-sweeping">
   5.4 Parameter sweeping
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   5.5 Summary
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>5 Feature engineering (optional)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-a-baseline-mlp-using-an-original-image-training-set">
   5.1 Creating a baseline MLP using an original image training set
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#improving-the-network-performance-on-translated-images">
   5.2 Improving the network performance on translated images
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#identifying-new-features">
     5.2.1 Identifying new features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activity-what-distinguishes-one-digit-from-another">
     5.2.2 Activity – What distinguishes one digit from another?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-observations">
       Example observations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-an-image-signature">
     5.2.3 Generating an image signature
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-signatures-in-bulk">
     5.2.4 Creating signatures in bulk
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-an-mlp-using-the-line-crossing-signature-features">
     5.2.5 Training an MLP using the line-crossing signature features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-and-testing-the-network-against-smaller-images">
   5.3 Training and testing the network against smaller images
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-sweeping">
   5.4 Parameter sweeping
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   5.5 Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><strong>This notebook contains optional study material. You are not required to work through it in order to meet the learning objectives or complete the assessments associated with this module.</strong></p>
<p><em>Brief overview: inputs to a neural network are often described as ‘features’. Determining appropriate features for presenting to a network may require significant pre-processing of the original dataset. Pre-processing may often transform input data with large numbers of dimensions to richer features defined over fewer, but more meaningful, feature dimensions.</em></p>
<div class="tex2jax_ignore mathjax_ignore section" id="feature-engineering-optional">
<h1>5 Feature engineering (optional)<a class="headerlink" href="#feature-engineering-optional" title="Permalink to this headline">¶</a></h1>
<p>In the previous notebook, you saw how a new training set created by cropping and translating the original MNIST images caused recall problems for the original MLP. In this notebook, you will explore one possible strategy for improving the network’s performance: <em>feature engineering</em>.</p>
<p>When presenting a raw image to a neural network, for example as a list of N × M values, one for each pixel in the image, each value represents a distinct <em>feature</em> that the network may use to help it generate a particular classification.</p>
<p><em>Feature engineering</em> is the name given to the process of deriving new features from the original raw dataset. These new features can then be used either to complement the original dataset, or to be presented to the network for training, and recall, instead of the original data.</p>
<p>The aim of using derived features, rather than the original pixel features, is to try to improve the performance of the network.</p>
<div class="section" id="creating-a-baseline-mlp-using-an-original-image-training-set">
<h2>5.1 Creating a baseline MLP using an original image training set<a class="headerlink" href="#creating-a-baseline-mlp-using-an-original-image-training-set" title="Permalink to this headline">¶</a></h2>
<p>Let’s just recap on what happened when we trained an MLP on the original dataset.</p>
<p>First, load in the images and labels datasets:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">load_MNIST_images_and_labels</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="c1"># Load images</span>
<span class="n">images_list</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">load_MNIST_images_and_labels</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">quick_progress_tracked_training()</span></code> function makes it even easier for us to train a dataset from the images data.</p>
<p>In the training, we hold 100 images back that the network will not see.</p>
<p>Note that we can actually get quite a reasonable-looking performance over the training data from a relatively simple, lightly trained network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">quick_progress_tracked_training</span>

<span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">40</span><span class="p">)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">held_back</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">MLP</span> <span class="o">=</span> <span class="n">quick_progress_tracked_training</span><span class="p">(</span><span class="n">images_list</span><span class="p">[:</span><span class="o">-</span><span class="n">held_back</span><span class="p">],</span> <span class="n">labels</span><span class="p">[:</span><span class="o">-</span><span class="n">held_back</span><span class="p">],</span>
                                 <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span>
                                 <span class="c1"># top up an existing pretrained MLP: MLP=MLP,</span>
                                 <span class="n">max_iterations</span><span class="o">=</span><span class="n">max_iterations</span><span class="p">,</span>
                                 <span class="n">loss</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># show loss function</span>
                                 <span class="n">structure</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># show network params</span>
                                <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about the performance of the network under training.</em></p>
<p>How does the network perform on the withheld images it wasn’t trained on and hasn’t previously seen?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">test_and_report_image_data</span>
<span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">get_images_features</span>

<span class="n">testing_data</span> <span class="o">=</span> <span class="n">get_images_features</span><span class="p">(</span><span class="n">images_list</span><span class="p">[</span><span class="n">held_back</span><span class="p">:],</span> <span class="n">normalise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_and_report_image_data</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">testing_data</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="n">held_back</span><span class="p">:])</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your observations about the network’s performance when tested using previously unseen images.</em></p>
<p>Let’s now create a jiggled dataset and see how well the network fares when tested against jiggled images:</p>
<p><em>Every time the <code class="docutils literal notranslate"><span class="pre">test_and_report_random_images()</span></code> function is run with <code class="docutils literal notranslate"><span class="pre">jiggled=True</span></code>, newly jiggled images are created and tested.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">test_and_report_random_images</span>
<span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">get_random_image</span>

<span class="n">test_and_report_random_images</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> 
                              <span class="n">get_random_image</span><span class="p">,</span> <span class="n">images_list</span><span class="p">[:</span><span class="o">-</span><span class="n">held_back</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">[:</span><span class="o">-</span><span class="n">held_back</span><span class="p">],</span>
                              <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">jiggled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cropzoom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your observations about how well the network copes with the jiggled images. How does this compare to the previous tests you ran?</em></p>
</div>
<div class="section" id="improving-the-network-performance-on-translated-images">
<h2>5.2 Improving the network performance on translated images<a class="headerlink" href="#improving-the-network-performance-on-translated-images" title="Permalink to this headline">¶</a></h2>
<p>So how might we improve the network’s ability to classify jiggled (i.e. translated) images?</p>
<p>One way might be to train a network based on the cropped images. Then if we cropped each translated test image, the translation variance wouldn’t be an issue.</p>
<p>But one problem with this approach is that different images crop to different sizes, and the network requires the presentation of images with the same dimensions each time.</p>
<p>Run the following cell to display the sizes of 10 randomly selected images that have been cropped:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">trim_image</span>                

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_random_image</span><span class="p">(</span><span class="n">images_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">cropped_test_image</span> <span class="o">=</span> <span class="n">trim_image</span><span class="p">(</span> <span class="n">test_image</span><span class="p">,</span> <span class="n">background</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">image</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">_size</span> <span class="o">=</span> <span class="n">cropped_test_image</span><span class="o">.</span><span class="n">size</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cropped image size:&quot;</span><span class="p">,</span> <span class="n">_size</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;= </span><span class="si">{</span><span class="n">_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1"> pixels&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here regarding how the images are different sizes. What might make it difficult to use these raw images to train the network?</em></p>
<div class="section" id="identifying-new-features">
<h3>5.2.1 Identifying new features<a class="headerlink" href="#identifying-new-features" title="Permalink to this headline">¶</a></h3>
<p>One constraint we have is that we require a set of input features that has a fixed size and semantics (i.e. feature number 7 from one test pattern means the same thing as feature number 7 from another).</p>
<p>In the original MLP, the features comprised the 28 × 28 = 784 individual pixel values. Using cropped images gives varying numbers of features, so unless we scale the cropped image back to a standard image size, we can’t naively use cropped images as inputs.</p>
<p>An alternative approach is to encode the original images somehow with a fixed-size signature that encodes the image in a translation-invariant way.</p>
</div>
<div class="section" id="activity-what-distinguishes-one-digit-from-another">
<h3>5.2.2 Activity – What distinguishes one digit from another?<a class="headerlink" href="#activity-what-distinguishes-one-digit-from-another" title="Permalink to this headline">¶</a></h3>
<p>Let’s ponder a random image for a moment or two. What distinguishes <em>this</em> digit from another digit?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">zoom_img</span>

<span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_random_image</span><span class="p">(</span><span class="n">images_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">zoom_img</span><span class="p">(</span><span class="n">test_image</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Write your thoughts here about what makes one handwritten digit different from another. How might we represent those differences in a concise and consistent way?</em></p>
<div class="section" id="example-observations">
<h4>Example observations<a class="headerlink" href="#example-observations" title="Permalink to this headline">¶</a></h4>
<p><em>Click the arrow in the sidebar or run this cell to reveal some example observations.</em></p>
<p>Some digits include loops (0, 6, 8, 9), some have flat bits at the top and bottom (2, 5, 7), some have vertical lines (1, 7), some have lines in the middle (3, 4, 5). Can we use that information somehow?</p>
<p>Reflecting on this a little more, it may occur to us that we can count the number of lines that appear in each row or each column of the image array to try to capture some sense of the straight line and loopiness.</p>
<p>One way of counting lines is to count the number of transitions from black to white or white to black along each row or each row and column in the dataframe.</p>
</div>
</div>
<div class="section" id="generating-an-image-signature">
<h3>5.2.3 Generating an image signature<a class="headerlink" href="#generating-an-image-signature" title="Permalink to this headline">¶</a></h3>
<p>I have created a simple function, <code class="docutils literal notranslate"><span class="pre">generate_signature()</span></code>, that can be used to generate a ‘signature’ for various sorts of input along the lines (pun intended!) of how lines appear across, and up and down, the image frame.</p>
<p>The signature is made up from sets of four values, one set per row of the image, image dataframe, or image array:</p>
<ul class="simple">
<li><p>the number of black-to-white and white-to-black transitions in the row (that is, the number of edges in the row)</p></li>
<li><p>the value of the initial pixel in the row</p></li>
<li><p>a count of the longest run of white pixels in the row (that is, the width of the broadest white band in the row)</p></li>
<li><p>a count of the longest run of black pixels in the row (that is, the width of the broadest black band in the row).</p></li>
</ul>
<p>Before the signature is generated, the image is converted to a binarised black-and-white image. The black-and-white threshold value is set by passing a parameter which defaults to a value of 127 (<code class="docutils literal notranslate"><span class="pre">threshold=127</span></code>).</p>
<p><em>Experiment with different threshold values to explore what effect it has on the binarisation of the image.</em></p>
<p><em>Run the following code cell to create a simple interactive application that provides an <code class="docutils literal notranslate"><span class="pre">index</span></code> slider to select the image from the dataframe and a <code class="docutils literal notranslate"><span class="pre">threshold</span></code> slider to set the threshold value.</em></p>
<p><em>Click the <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Interact</span></code> button to see the effect of creating the black-and-white version of the selected image using the specified threshold value.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact_manual</span>
<span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">make_image_black_and_white</span>

<span class="nd">@interact_manual</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span>
                 <span class="n">index</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">images_list</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">bw_preview</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">_original_img</span> <span class="o">=</span> <span class="n">images_list</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    
    <span class="c1"># Generate a black and white image</span>
    <span class="n">_demo_img</span> <span class="o">=</span> <span class="n">make_image_black_and_white</span><span class="p">(</span><span class="n">_original_img</span><span class="p">,</span>
                                           <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">)</span>
    <span class="c1"># %sim_bw_image_data --index -1 --threshold 100 --crop 3.3,17,17</span>
    <span class="n">zoom_img</span><span class="p">(</span> <span class="n">_original_img</span><span class="p">)</span>
    <span class="n">zoom_img</span><span class="p">(</span> <span class="n">_demo_img</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When you think you have a feel for a sensible threshold, use it in the following code cell to generate a sample signature:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">generate_signature</span>

<span class="c1"># Set your threshold value here</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mi">127</span>

<span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_random_image</span><span class="p">(</span><span class="n">images_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># The fill value identifies the background colour</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">generate_signature</span><span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can preview the signature to see how it looks:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">signature</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about how each line of the signature describes each row of the image.</em></p>
<p><em>What elements of the image can you still ‘see’ in its signature?</em></p>
<p>This signature gives us 28 × 4 = 112 distinct features. If we pass the <code class="docutils literal notranslate"><span class="pre">linear=True</span></code> argument to <code class="docutils literal notranslate"><span class="pre">generate_signature()</span></code> then it returns the values from the dataframe as a list of features that can be used to train the MLP:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">generate_signature</span><span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Whilst it is not completely translation invariant, certain features do tend to be preserved by this signature.</p>
<p>As an example, consider the digit 1: the top and bottom and left-most and right-most columns are typically black, the number of horizontal black–white plus white–black changes is typically either 0 or 2, and so on.</p>
</div>
<div class="section" id="creating-signatures-in-bulk">
<h3>5.2.4 Creating signatures in bulk<a class="headerlink" href="#creating-signatures-in-bulk" title="Permalink to this headline">¶</a></h3>
<p>To create the signature bases training data for the MLP, we can iterate through the list of images loaded in from the original <code class="docutils literal notranslate"><span class="pre">images_array</span></code> and create the signature for each image, returning the signature of each image as a list that can be used to train the network:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">signatures_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">images_list</span><span class="p">[:</span><span class="mi">500</span><span class="p">]):</span>
    <span class="n">signatures_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generate_signature</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> 
</pre></div>
</div>
<p><em>Running that code may take some time – the signature-generating code is far from optimal! – so as declared above it would only generate signatures for some of the sample images.</em></p>
<p><em>To save time creating the signatures lists, I ran the above code and saved a copy of the signatures using the Python <code class="docutils literal notranslate"><span class="pre">pickle()</span></code> function:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/signatures.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">signatures_list</span><span class="p">,</span> <span class="n">outfile</span><span class="p">)</span>
</pre></div>
</div>
<p>If you choose to do so, you may copy the signature-generating code into a code cell and run it to generate the signatures yourself. Alternatively, run the following code cell to load in a version I prepared earlier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/signatures.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
    <span class="n">signatures_list</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">infile</span><span class="p">)</span>
    
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/jiggled_signatures.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
    <span class="n">jiggled_signatures_list</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">infile</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can also generate signatures for a set of jiggled images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">jiggle</span>

<span class="n">jiggled_signatures_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">images_list</span><span class="p">[:</span><span class="mi">500</span><span class="p">]):</span>
    <span class="n">jiggled_signatures_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generate_signature</span><span class="p">(</span><span class="n">jiggle</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">fill</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<p>Once again, I created a version of these jiggled signatures that you can load in directly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/signatures.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
    <span class="n">signatures_list</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">infile</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/jiggled_signatures.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
    <span class="n">jiggled_signatures_list</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">infile</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-an-mlp-using-the-line-crossing-signature-features">
<h3>5.2.5 Training an MLP using the line-crossing signature features<a class="headerlink" href="#training-an-mlp-using-the-line-crossing-signature-features" title="Permalink to this headline">¶</a></h3>
<p>Let’s create a simple MLP network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">max_iterations</span><span class="o">=</span><span class="mi">1500</span>

<span class="n">MLP4</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iterations</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And train it on the jiggled signatures data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">normalize</span>
<span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">network_structure</span>


<span class="n">training_signature_data</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">jiggled_signatures_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">training_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>


<span class="n">MLP4</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_signature_data</span><span class="p">,</span> <span class="n">labels</span><span class="p">[:</span><span class="mi">500</span><span class="p">])</span>

<span class="c1"># Quick status report</span>
<span class="n">network_structure</span><span class="p">(</span><span class="n">MLP4</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about the network’s performance under training. If it is performing badly, consider changing the number of hidden layers, and/or the maximum number of training iterations and retrain the network.</em></p>
<p>Let’s see how well it behaves on the training data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">test_and_report</span>

<span class="n">test_and_report</span><span class="p">(</span><span class="n">MLP4</span><span class="p">,</span> <span class="n">training_signature_data</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about how well the network performs on the training data.</em></p>
<p>How well does it perform if we present it with the original unjiggled image signatures?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_and_report</span><span class="p">(</span><span class="n">MLP4</span><span class="p">,</span> <span class="n">normalize</span><span class="p">(</span><span class="n">signatures_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">training_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about how well the network performs on the original, previously unseen, unjiggled data.</em></p>
<p><em>How well does this network perform compared to the original network trained on the original image pixel data array?</em></p>
<p>My network was far from perfect, but it performed much better than the network trained on the original images and tested against jiggled images.</p>
<p>If we pass the <code class="docutils literal notranslate"><span class="pre">use_signature=True</span></code> parameter into the <code class="docutils literal notranslate"><span class="pre">test_and_report_random_images()</span></code> function, then we can randomly select images, find their signature, and then test those against the network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_and_report_random_images</span><span class="p">(</span><span class="n">MLP4</span><span class="p">,</span>
                              <span class="n">get_random_image</span><span class="p">,</span> <span class="n">images_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                              <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">jiggled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_signature</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about how well the network performs on the randomly selected and jiggled data.</em></p>
<p>When I tried it, the network was getting the correct classification a bit more than half the time. Not ideal, but far better than when testing the naive classifier trained on the original image pixel data against jiggled images.</p>
<p>Experimenting with hidden layer sizes and the number of training iterations, and perhaps even a larger training set, might also improve matters.</p>
</div>
</div>
<div class="section" id="training-and-testing-the-network-against-smaller-images">
<h2>5.3 Training and testing the network against smaller images<a class="headerlink" href="#training-and-testing-the-network-against-smaller-images" title="Permalink to this headline">¶</a></h2>
<p>How well does the signature method work if we use it on smaller images, such as the original images reduced in size from 28 × 28 pixels to 14 × 14 pixels?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">resized_images_pipeline</span>

<span class="n">resized_images</span> <span class="o">=</span> <span class="n">resized_images_pipeline</span><span class="p">(</span><span class="n">images_list</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">quick_progress_tracked_training()</span></code> function has support for signature-based training if you pass the <code class="docutils literal notranslate"><span class="pre">use_signature=True</span></code> parameter. We can also choose to jiggle the images that are created for training:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_samples</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="n">MLP_small</span> <span class="o">=</span> <span class="n">quick_progress_tracked_training</span><span class="p">(</span><span class="n">resized_images</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">],</span> <span class="n">labels</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">],</span>
                                 <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span>
                                 <span class="c1"># top up an existing pretrained MLP: MLP=MLP,</span>
                                 <span class="n">max_iterations</span><span class="o">=</span><span class="n">max_iterations</span><span class="p">,</span>
                                 <span class="n">loss</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># show loss function</span>
                                 <span class="n">structure</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># show network params</span>
                                 <span class="n">jiggled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">use_signature</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about the network’s performance under training. If it is performing badly, consider changing the number of hidden layers, and/or the maximum number of training iterations and retrain the network.</em></p>
<p>We can test the network on images sampled from the original list of unjiggled resized images, which are likely to differ from the jiggled images the network was trained on:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_and_report_random_images</span><span class="p">(</span><span class="n">MLP_small</span><span class="p">,</span>
                              <span class="n">get_random_image</span><span class="p">,</span> <span class="n">resized_images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                              <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">jiggled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_signature</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations about how well the network performs on the original unjiggled images.</em></p>
<p>When I tried it, the performance wasn’t brilliant, but changing the network structure and number of training iterations may improve matters.</p>
<p>We can also test against randomly jiggled images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_and_report_random_images</span><span class="p">(</span><span class="n">MLP_small</span><span class="p">,</span>
                              <span class="n">get_random_image</span><span class="p">,</span> <span class="n">resized_images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                              <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">jiggled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_signature</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations about how well the network performs this time.</em></p>
<p>To save time, I have already generated some small image signature data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
               
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/small_signatures.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
    <span class="n">small_signatures_list</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">infile</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/small_jiggled_signatures.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
    <span class="n">small_jiggled_signatures_list</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">infile</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can train and test an MLP using just the signature data rather than having to generate it from images each time if we pass the <code class="docutils literal notranslate"><span class="pre">image_data=False</span></code> parameter to the <code class="docutils literal notranslate"><span class="pre">quick_progress_tracked_training()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_samples</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">1500</span>

<span class="n">MLP_small2</span> <span class="o">=</span> <span class="n">quick_progress_tracked_training</span><span class="p">(</span><span class="n">small_jiggled_signatures_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">],</span>
                                 <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span>
                                 <span class="c1"># top up an existing pretrained MLP: MLP=MLP,</span>
                                 <span class="n">max_iterations</span><span class="o">=</span><span class="n">max_iterations</span><span class="p">,</span>
                                 <span class="n">loss</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># show loss function</span>
                                 <span class="n">structure</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># show network params</span>
                                 <span class="n">image_data</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># State that we are using signature data not image data</span>
                                <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about the network’s performance under training. If it is performing badly, consider changing the number of hidden layers, and/or the maximum number of training iterations and retrain the network.</em></p>
<p>We can test the network on the small jiggled image signatures dataset to see how it performs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_and_report</span><span class="p">(</span><span class="n">MLP_small2</span><span class="p">,</span> <span class="n">small_jiggled_signatures_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about the network’s performance on the training data.</em></p>
<p>How about if we test it on the previously unseen original, unjiggled, small images signatures?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_and_report</span><span class="p">(</span><span class="n">MLP_small2</span><span class="p">,</span> <span class="n">small_signatures_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about the network’s performance on the previously unseen, unjiggled image data.</em></p>
<p><em>How does it compare with the behaviour of the network trained on the full-size image data?</em></p>
</div>
<div class="section" id="parameter-sweeping">
<h2>5.4 Parameter sweeping<a class="headerlink" href="#parameter-sweeping" title="Permalink to this headline">¶</a></h2>
<p>At this point, you may be thinking this is all so much voodoo, and how on earth are you expected to be able to find appropriate settings for the number and arrangement of the hidden layers and the number of training iterations, let alone the selection of appropriate training images?</p>
<p>And you wouldn’t be wrong!</p>
<p>One approach to finding appropriate configuration setups is a brute-force approach know as <em>parameter sweeping</em>. Under this approach, you set up an automated experiment to create and train lots and lots of networks for increasing numbers of hidden nodes, and increasing numbers of training cycles. Then you pick the best one.</p>
<p>Performing parameter sweeps, as well as other techniques for trying to find effective network architectures and training schedules, is beyond the scope of this module.</p>
<p>If you would like to learn more about training and developing neural networks, consider taking the OU module TM358 <em>Machine learning and artificial intelligence</em>.</p>
</div>
<div class="section" id="summary">
<h2>5.5 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In this notebook you have been introduced to the idea of <em>feature engineering</em>, in which the neural network design engineer tries to find ways of encoding the input data and identifying meaningful <em>features</em> in it that the network can then use to distinguish between the presented patterns.</p>
<p>As well as improving performance by reducing the number of features, and so reducing the size and complexity of the network, some feature selections may also improve performance by ‘masking’ different sorts of irrelevant variation in the presented pattern. For example, you saw how a signature-based encoding of the MNIST image data might provide translation-invariant features in certain respects.</p>
<p>In the next notebook, you will look at another sort of neural network architecture, the convolutional neural network (CNN), before going on to explore how well CNNs cope with the MNIST data.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./07. Neural networks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="07.4%20Testing%20a%20network%20against%20synthetic%20data%20%28optional%29.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">4 Testing a network against synthetic data (optional)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="07.6%20Inside%20the%20mind%20of%20a%20neural%20network.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">6 Inside the mind of a neural network</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The Jupyter Book community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>