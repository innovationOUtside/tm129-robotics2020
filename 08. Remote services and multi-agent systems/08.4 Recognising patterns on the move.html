
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4 Recognising patterns on the move &#8212; TM129 Robotics Practical Activities</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5 Messaging in multi-agent systems" href="08.5%20Messaging%20in%20multi-agent%20systems.html" />
    <link rel="prev" title="3 Recognising digits using a convolutional neural network (optional)" href="08.3%20Recognising%20digits%20using%20a%20convolutional%20neural%20network%20%28optional%29.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">TM129 Robotics Practical Activities</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README_FIRST.html">
   Welcome to the TM129 Robotics block
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00_FOR_VLE/Section_00_01_Introduction.html">
   1 Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00_NOTES_FOR_TUTORS/GETTING_STARTED.html">
   Getting started
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.1%20Jupyter%20environment.html">
   1 Introduction to the TM129 Jupyter notebook environment
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.2%20Exploring%20the%20notebook%20environment.html">
     2 The interactive read-writable notebook environment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.3%20Introducing%20nbev3devsim.html">
     3 The RoboLab simulated on-screen robot (
     <code class="docutils literal notranslate">
      <span class="pre">
       nbev3devsim
      </span>
     </code>
     )
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.4%20Exploring%20nbev3devsim.html">
     4 Exploring the
     <code class="docutils literal notranslate">
      <span class="pre">
       nbev3devsim
      </span>
     </code>
     simulator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.5%20Example%20robot%20program.html">
     5 An example robot program
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01.%20Introducing%20notebooks%20and%20the%20RoboLab%20environment/01.6%20Working%20With%20Simulators.html">
     6 Working with simulators
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.1%20Robot%20programming%20constructs.html">
   1 An introduction to programming robots
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.2%20Creating%20your%20own%20robot%20programs.html">
     2 Creating your own robot programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.3%20General%20programming%20concepts.html">
     3.1 Constants and variables in programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02.%20Getting%20started%20with%20robot%20and%20Python%20programming/02.4%20Getting%20started%20with%20sensors.html">
     4 Robot sensors and data logging
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.1%20Program%20control%20using%20for%20loops.html">
   1 Introduction to program control flow
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.2%20Program%20control%20using%20while%20loops%20and%20blocking.html">
     2 Program control flow using a
     <code class="docutils literal notranslate">
      <span class="pre">
       while...
      </span>
     </code>
     loop
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.3%20Program%20control%20flow%20using%20branches.html">
     3 Branches
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.4%20Example%20robot%20control%20programs.html">
     4 Example robot control programs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.5%20Some%20RoboLab%20challenges.html">
     5 RoboLab challenges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03.%20Controlling%20program%20execution%20flow/03.6%20Optional%20RoboLab%20challenges.html">
     6 Optional challenges
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.1%20Introducing%20program%20functions.html">
   1 Introduction to functions and robot control strategies
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.2%20Robot%20navigation%20using%20dead%20reckoning.html">
     2 Dead reckoning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.3%20Emergent%20robot%20behaviour%20and%20simple%20data%20charts.html">
     3 Emergent robot behaviour and simple data charts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.4%20Reasoning%20with%20Eliza.html">
     4 Reasoning with Eliza
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04.%20Not%20quite%20intelligent%20robots/04.5%20Reasoning%20with%20rule%20based%20systems.html">
     5 Reasoning with rule-based systems – Durable Rules Engine
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05.%20Experimenting%20with%20sensors/05.1%20Introducing%20sensor%20based%20control.html">
   Introduction to sensor-based control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06.%20Where%20in%20the%20world%20are%20we/06.1%20Introducing%20sensor%20based%20navigation.html">
   Introducing sensor-based navigation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../07.%20Neural%20networks/07.1%20Introducing%20neural%20networks.html">
   1 Introducing neural networks
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="08.1%20Introducing%20remote%20services%20and%20multi-agent%20systems.html">
   1 An introduction to remote services and multi-agent systems
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="08.2%20Collecting%20digit%20image%20and%20class%20data%20from%20the%20simulator.html">
     2 Collecting digit image and class data from the simulator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.3%20Recognising%20digits%20using%20a%20convolutional%20neural%20network%20%28optional%29.html">
     3 Recognising digits using a convolutional neural network (optional)
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4 Recognising patterns on the move
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.5%20Messaging%20in%20multi-agent%20systems.html">
     5 Messaging in multi-agent systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.6%20Conclusion.html">
     6 Conclusion
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/08. Remote services and multi-agent systems/08.4 Recognising patterns on the move.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/08. Remote services and multi-agent systems/08.4 Recognising patterns on the move.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluating-the-possible-training-data">
   4.1 Evaluating the possible training data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#investigating-the-training-data-samples">
   4.1.1 Investigating the training data samples
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#counting-the-number-of-black-pixels-in-each-shape">
   4.1.2 Counting the number of black pixels in each shape
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activity-using-bounding-box-sizes-as-a-feature-for-distinguishing-between-shapes">
   4.1.3 Activity – Using bounding box sizes as a feature for distinguishing between shapes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-solution">
     Example solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-the-training-label-image">
   4.1.4 Decoding the training label image
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-time-data-collection">
   4.2 Real-time data collection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#identifying-when-the-robot-is-over-a-pattern-in-real-time">
     4.2.1 Identifying when the robot is over a pattern in real time
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-challenge-capturing-image-data-in-real-time">
     4.2.2 Optional challenge – Capturing image data in real time
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#capturing-image-data-in-real-time">
     4.2.3 Capturing image data in real time
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-an-mlp-to-recognise-the-patterns">
   4.3 Training an MLP to recognise the patterns
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#testing-the-network-on-a-new-set-of-collected-data">
   4.4 Testing the network on a new set of collected data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collecting-the-test-data">
     4.4.1 Collecting the test data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-the-test-set">
     4.4.2 Generating the test set
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-the-data">
     4.4.3 Testing the data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-the-mlp">
     4.4.4 Save the MLP
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   4.5 Summary
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="recognising-patterns-on-the-move">
<h1>4 Recognising patterns on the move<a class="headerlink" href="#recognising-patterns-on-the-move" title="Permalink to this headline">¶</a></h1>
<p>To be really useful a robot needs to recognise things as it goes along, or ‘on the fly’. In this notebook, you will train a neural network to use a simple MLP classifier to try to identify different shapes on the background. The training samples themselves, images <em>and</em> training labels, will be captured by the robot from the simulator background.</p>
<p>We will use the two light sensors to collect the data used to train the network:</p>
<ul class="simple">
<li><p>one light sensor will capture the shape image data</p></li>
<li><p>one light sensor will capture the training class data.</p></li>
</ul>
<p>To begin with we will contrive things somewhat to collect the data at specific locations on the background. But then you will explore how we can collect images as the robot moves more naturally within the environment.</p>
<p><em>There is quite a lot of provided code in this notebook. You are not necessarily expected to be able to create this sort of code yourself. Instead, try to focus on the process of how various tasks are broken down into smaller discrete steps, as well as how small code fragments can be combined to create ‘higher-level’ functions that perform ever more powerful tasks.</em></p>
<p>Before continuing, ensure the simulator is loaded and available:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nbev3devsim.load_nbev3devwidget</span> <span class="kn">import</span> <span class="n">roboSim</span><span class="p">,</span> <span class="n">eds</span>
<span class="o">%</span><span class="n">load_ext</span> <span class="n">nbev3devsim</span>
</pre></div>
</div>
</div>
</div>
<p>The background image <em>Simple_Shapes</em> contains several shapes arranged in a line, including a square, a circle, four equilateral triangles (arrow heads) with different orientations, a diamond and a rectangle.</p>
<p>Just below each shape is a grey square, whose fill colour is used to distinguish between the different shapes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">sim_magic</span> <span class="o">-</span><span class="n">b</span> <span class="n">Simple_Shapes</span> <span class="o">-</span><span class="n">x</span> <span class="mi">600</span> <span class="o">-</span><span class="n">y</span> <span class="mi">900</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="evaluating-the-possible-training-data">
<h2>4.1 Evaluating the possible training data<a class="headerlink" href="#evaluating-the-possible-training-data" title="Permalink to this headline">¶</a></h2>
<p>In this initial training pass, we will check whether the robot can clearly observe the potential training pairs. Each training pair consists of the actual shape image as well as a solid grey square, where the grey colour is used to represent one of six different training classes.</p>
<p>The left light sensor will be used to sample the shape image data; the right light sensor will be used to collect the simpler grey classification group pattern.</p>
<p>As we are going to be pulling data into the notebook Python environment from the simulator, ensure the local notebook datalog is cleared:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">roboSim</span><span class="o">.</span><span class="n">clear_datalog</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The <em>Simple_Shapes</em> background we are using in this notebook contains several small regular shapes, with label-encoding patterns alongside.</p>
<p>The <em>x</em> and <em>y</em> locations for sampling the eight different images, along with a designator for each shape, as are follows:</p>
<ul class="simple">
<li><p>200 900 square</p></li>
<li><p>280 900 right-facing triangle</p></li>
<li><p>360 900 left-facing triangle</p></li>
<li><p>440 900 downwards-facing triangle</p></li>
<li><p>520 900 upwards-facing triangle</p></li>
<li><p>600 900 diamond.</p></li>
</ul>
<p>We can now start to collect image data from the robot’s light sensors. The <code class="docutils literal notranslate"><span class="pre">-R</span></code> switch runs the program once it has been downloaded to the simulator.</p>
<p>If we print the message <code class="docutils literal notranslate"><span class="pre">&quot;image_data</span> <span class="pre">both&quot;</span></code> then we can collect data from both the left and the right light sensors at the same time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">sim_magic_preloaded</span> <span class="o">-</span><span class="n">b</span> <span class="n">Simple_Shapes</span> <span class="o">-</span><span class="n">AR</span> <span class="o">-</span><span class="n">x</span> <span class="mi">520</span> <span class="o">-</span><span class="n">y</span> <span class="mi">900</span> <span class="o">-</span><span class="n">O</span>

<span class="c1"># Sample the light sensor reading</span>
<span class="n">sensor_value</span> <span class="o">=</span> <span class="n">colorLeft</span><span class="o">.</span><span class="n">reflected_light_intensity</span>

<span class="c1"># This is essentially a command invocation</span>
<span class="c1"># not just a print statement!</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;image_data both&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can preview the collected image data in the usual way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">roboSim</span><span class="o">.</span><span class="n">image_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We can also collect consecutive rows of data from the dataframe and decode them as left and right images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">get_sensor_image_pair</span>
<span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">zoom_img</span>

<span class="n">pair_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="n">left_img</span><span class="p">,</span> <span class="n">right_img</span> <span class="o">=</span> <span class="n">get_sensor_image_pair</span><span class="p">(</span><span class="n">roboSim</span><span class="o">.</span><span class="n">image_data</span><span class="p">(),</span>
                                            <span class="n">pair_index</span><span class="p">)</span>
<span class="n">zoom_img</span><span class="p">(</span><span class="n">left_img</span><span class="p">),</span> <span class="n">zoom_img</span><span class="p">(</span><span class="n">right_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If you don’t see a figure image displayed, check that the robot is placed over a figure by reviewing the sensor array display in the simulator. If the image is there, then rerun the previous code cell to see if the data is now available. If it isn’t, then rerun the data-collecting magic cell, wait a view seconds, and then try to view the zoomed image display.</p>
<p>We can run the previously downloaded program again from a simple line magic that situates the robot at a specific location and then runs the program to collect the sensor data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>_x = 280

%sim_magic -x $_x -y 900 -RAH
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="investigating-the-training-data-samples">
<h2>4.1.1 Investigating the training data samples<a class="headerlink" href="#investigating-the-training-data-samples" title="Permalink to this headline">¶</a></h2>
<p>Let’s start by seeing if we can collect image data samples for each of the shapes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>from tqdm.notebook import trange
from nbev3devsim.load_nbev3devwidget import tqdma

import time

# Clear the datalog to give us a fresh start
roboSim.clear_datalog()

# x-coordinate for centreline of first shape
_x_init = 200

# Distance between shapes
_x_gap = 80

# Number of shapes
_n_shapes = 6

# y-coordinate for centreline of shapes
_y = 900

# Load in the required background
%sim_magic -b Simple_Shapes

# Generate x-coordinate for each shape in turn
for _x in trange(_x_init, _x_init+(_n_shapes*_x_gap), _x_gap):
    
    # Jump to shape and run program to collect data
    %sim_magic -x $_x -y $_y -R
    
    # Wait a short period to allow time for
    # the program to run and capture the sensor data,
    # and for the data to be passed from the simulator
    # to the notebook Python environment
    time.sleep(1)
</pre></div>
</div>
</div>
</div>
<p>We should now be able to access multiple image samples via <code class="docutils literal notranslate"><span class="pre">roboSim.image_data()</span></code>, which returns a dataframe containing as many rows as images we scanned:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clean_data_df</span> <span class="o">=</span> <span class="n">roboSim</span><span class="o">.</span><span class="n">image_data</span><span class="p">()</span>
<span class="n">clean_data_df</span>
</pre></div>
</div>
</div>
</div>
<p>The original sensor data is collected as three-channel RGB data. By default, the <code class="docutils literal notranslate"><span class="pre">get_sensor_image_pair()</span></code> function, which extracts a pair of consecutive images from the datalog, converts these to greyscale images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">get_sensor_image_pair</span>

<span class="n">pair_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="n">left_img</span><span class="p">,</span> <span class="n">right_img</span> <span class="o">=</span> <span class="n">get_sensor_image_pair</span><span class="p">(</span><span class="n">clean_data_df</span><span class="p">,</span>
                                            <span class="n">pair_index</span><span class="p">)</span>

<span class="n">zoom_img</span><span class="p">(</span><span class="n">left_img</span><span class="p">),</span> <span class="n">zoom_img</span><span class="p">(</span><span class="n">right_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can also filter the dataframe to give us a dataframe containing just the data grabbed from the left-hand image sensor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The mechanics behind how this line of code</span>
<span class="c1"># works are beyond the scope of this module.</span>
<span class="c1"># In short, we identify the rows where the</span>
<span class="c1"># &quot;side&quot; column value is equal to &quot;left&quot;</span>
<span class="c1"># and select just those rows.</span>
<span class="n">clean_left_images_df</span> <span class="o">=</span> <span class="n">clean_data_df</span><span class="p">[</span><span class="n">clean_data_df</span><span class="p">[</span><span class="s1">&#39;side&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;left&#39;</span><span class="p">]</span>
<span class="n">clean_left_images_df</span>
</pre></div>
</div>
</div>
</div>
<p>The shape names and classes are defined as follows in the order they appear going from left to right along the test track. We can also derive a map going the other way, from code to shape.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the classes</span>
<span class="n">shapemap</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;square&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s1">&#39;right facing triangle&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;left facing triangle&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s1">&#39;downwards facing triangle&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s1">&#39;upwards facing triangle&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s1">&#39;diamond&#39;</span><span class="p">:</span> <span class="mi">5</span>
           <span class="p">}</span>

<span class="n">codemap</span> <span class="o">=</span> <span class="p">{</span><span class="n">shapemap</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">shapemap</span><span class="p">}</span>
<span class="n">codemap</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="counting-the-number-of-black-pixels-in-each-shape">
<h2>4.1.2 Counting the number of black pixels in each shape<a class="headerlink" href="#counting-the-number-of-black-pixels-in-each-shape" title="Permalink to this headline">¶</a></h2>
<p>Ever mindful that we are on the lookout for features that might help us distinguish between the different shapes, let’s check a really simple measure: the number of black-filled pixels in each shape.</p>
<p>If we cast the pixel data for the image in the central focus areas of the image array to a <em>pandas</em> <em>Series</em>, then we can use the <em>Series</em> <code class="docutils literal notranslate"><span class="pre">.value_counts()</span></code> method to count the number of each unique pixel value.</p>
<p><em>Each column in a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> dataframe is a <code class="docutils literal notranslate"><span class="pre">pandas.Series</span></code> object. Casting a list of data to a <code class="docutils literal notranslate"><span class="pre">Series</span></code> provides us with many convenient tools for manipulating and summarising that data.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">generate_image</span><span class="p">,</span> <span class="n">sensor_image_focus</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clean_left_images_df</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="c1"># Get the central focal area of the image</span>
    <span class="n">left_img</span> <span class="o">=</span> <span class="n">sensor_image_focus</span><span class="p">(</span><span class="n">generate_image</span><span class="p">(</span><span class="n">clean_left_images_df</span><span class="p">,</span> <span class="n">index</span><span class="p">))</span>
    
    <span class="c1"># Count of each pixel value</span>
    <span class="n">pixel_series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">left_img</span><span class="o">.</span><span class="n">getdata</span><span class="p">()))</span>
    <span class="c1"># The .value_counts() method tallies occurrences</span>
    <span class="c1"># of each unique value in the Series</span>
    <span class="n">pixel_counts</span> <span class="o">=</span> <span class="n">pixel_series</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
    
    <span class="c1"># Display the count and the image</span>
    <span class="n">display</span><span class="p">(</span><span class="n">codemap</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">left_img</span><span class="p">,</span> <span class="n">pixel_counts</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Observing the black (<code class="docutils literal notranslate"><span class="pre">0</span></code> value) pixel counts, we see that they do not uniquely identify the shapes. For example, the left- and right-facing triangles and the diamond all have 51 black pixels, so a simple pixel count <em>does not</em> provide a way to distinguish between the shapes.</p>
</div>
<div class="section" id="activity-using-bounding-box-sizes-as-a-feature-for-distinguishing-between-shapes">
<h2>4.1.3 Activity – Using bounding box sizes as a feature for distinguishing between shapes<a class="headerlink" href="#activity-using-bounding-box-sizes-as-a-feature-for-distinguishing-between-shapes" title="Permalink to this headline">¶</a></h2>
<p>When we trained a neural network to recognise our fruit data, we used the dimensions of a bounding box drawn around the fruit as the input features to our network.</p>
<p>Will the bounding box approach used there also allow us to distinguish between the shape images?</p>
<p>Run the following code cell to convert the raw data associated with an image to a dataframe, and then prune the rows and columns around the edges that only contain white space.</p>
<p>The dimensions of the dataframe, which is to say, the <code class="docutils literal notranslate"><span class="pre">.shape</span></code> of the dataframe, given as the 2-tuple <code class="docutils literal notranslate"><span class="pre">(rows,</span> <span class="pre">columns)</span></code>, corresponds to the bounding box of the shape.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">df_from_image</span><span class="p">,</span> <span class="n">trim_image</span>

<span class="n">index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="c1"># The sensor_image_focus function crops</span>
<span class="c1"># to the central focal area of the image array</span>
<span class="n">left_img</span> <span class="o">=</span> <span class="n">sensor_image_focus</span><span class="p">(</span><span class="n">generate_image</span><span class="p">(</span><span class="n">clean_left_images_df</span><span class="p">,</span> <span class="n">index</span><span class="p">))</span>

<span class="n">trimmed_df</span> <span class="o">=</span> <span class="n">trim_image</span><span class="p">(</span> <span class="n">df_from_image</span><span class="p">(</span><span class="n">left_img</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">reindex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># dataframe shape</span>
<span class="n">trimmed_df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<p>Using the above code, or otherwise, find the shape of the bounding box for each shape as captured in the <code class="docutils literal notranslate"><span class="pre">roboSim.image_data</span></code> list.</p>
<p>You may find it useful to use the provided code as the basis of a simple function that will:</p>
<ul class="simple">
<li><p>take the index number for a particular image data scan</p></li>
<li><p>generate the image</p></li>
<li><p>find the size of the bounding box.</p></li>
</ul>
<p>Then you can iterate through all the rows in the <code class="docutils literal notranslate"><span class="pre">left_images_df</span></code> dataset, generate the corresponding image and its bounding box dimensions, and then display the image and the dimensions.</p>
<p><em>Hint: you can use a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop defined as <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(len(left_images_df)):</span></code> to iterate through each row of the dataframe and generate an appropriate index number, <code class="docutils literal notranslate"><span class="pre">i</span></code>, for each row.</em></p>
<p>Based on the shape dimensions alone, can you distinguish between the shapes?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your observations here, identifying the bounding box dimensions for each shape (square, right-facing triangle, left-facing triangle, downwards-facing triangle, upwards-facing triangle, diamond). Are the shapes distinguishable using their bounding box sizes?</em></p>
<div class="section" id="example-solution">
<h3>Example solution<a class="headerlink" href="#example-solution" title="Permalink to this headline">¶</a></h3>
<p><em>Click the arrow in the sidebar or run this cell to reveal an example solution.</em></p>
<p>Let’s start by creating a simple function inspired by the supplied code that will display an image and its bounding box dimensions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">find_bounding_box</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Find bounding box for a shape in an image.&quot;&quot;&quot;</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">sensor_image_focus</span><span class="p">(</span><span class="n">generate_image</span><span class="p">(</span><span class="n">clean_left_images_df</span><span class="p">,</span> <span class="n">index</span><span class="p">))</span>
    <span class="n">trimmed_df</span> <span class="o">=</span> <span class="n">trim_image</span><span class="p">(</span> <span class="n">df_from_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reindex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Show image and shape</span>
    <span class="n">display</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">trimmed_df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">find_bounding_box</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can then call this function by iterating through each image data record in the <code class="docutils literal notranslate"><span class="pre">roboSim.image_data</span></code> dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clean_left_images_df</span><span class="p">)):</span>
    <span class="n">find_bounding_box</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Inspecting the results from my run (yours may be slightly different), several of the shapes appear to share the same bounding box dimensions:</p>
<ul class="simple">
<li><p>the left- and right-facing triangles and the diamond have the same dimensions (<code class="docutils literal notranslate"><span class="pre">(11,</span> <span class="pre">9)</span></code>).</p></li>
</ul>
<p>The square is clearly separated from the other shapes on the basis of its bounding box dimensions, but the other shapes all have dimensions that may be hard to distinguish between.</p>
</div>
</div>
<div class="section" id="decoding-the-training-label-image">
<h2>4.1.4 Decoding the training label image<a class="headerlink" href="#decoding-the-training-label-image" title="Permalink to this headline">¶</a></h2>
<p>The grey-filled squares alongside the shape images are used to encode a label describing the associated shape.</p>
<p>The grey levels are determined by the following algorithm, in which we use the numerical class values to derive the greyscale value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">nan</span>

<span class="n">greymap</span> <span class="o">=</span> <span class="p">{</span><span class="n">nan</span><span class="p">:</span> <span class="s1">&#39;unknown&#39;</span><span class="p">}</span>

<span class="c1"># Generate greyscale value</span>
<span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">shapemap</span><span class="p">:</span>
    <span class="n">key</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">shapemap</span><span class="p">[</span><span class="n">shape</span><span class="p">]</span> <span class="o">*</span> <span class="mi">255</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">shapemap</span><span class="p">))</span>
    <span class="n">greymap</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">shape</span>
    
<span class="n">greymap</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s see if we can decode the labels from the solid grey squares.</p>
<p>To try to make sure we are using actual shape image data, we can identify images in our training set if <em>all</em> the pixels in the right-hand image are the same value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">left_img</span><span class="p">,</span> <span class="n">right_img</span> <span class="o">=</span> <span class="n">get_sensor_image_pair</span><span class="p">(</span><span class="n">clean_data_df</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Generate a set of distinct pixel values</span>
<span class="c1"># from the right-hand image.</span>
<span class="c1"># Return True if there is only one value</span>
<span class="c1"># in the set. That is, all the values are the same.</span>
<span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">right_img</span><span class="o">.</span><span class="n">getdata</span><span class="p">()))</span> <span class="o">==</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>The following function can be used to generate a greyscale image from a row of the dataframe, find the median pixel value within that image, and then try to decode it. We also return a flag (<code class="docutils literal notranslate"><span class="pre">uniform</span></code>) that identifies if all the pixels in the right-hand encoded label image are the same.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decode_shape_label</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">background</span><span class="o">=</span><span class="mi">255</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Decode the shape from the greyscale image.&quot;&quot;&quot;</span>
    <span class="c1"># Get the image greyscale pixel data</span>
    <span class="c1"># The pandas Series is a convenient representation</span>
    <span class="n">image_pixels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">getdata</span><span class="p">()))</span>
    
    <span class="c1"># Find the median pixel value</span>
    <span class="n">pixels_median</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">image_pixels</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
    
    <span class="n">shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">code</span><span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># uniform = len(set(img.getdata())) == 1</span>
    <span class="c1"># There is often more than one way to do it!</span>
    <span class="c1"># The following makes use of Series.unique()</span>
    <span class="c1"># which identifies the distinct values in a Series</span>
    <span class="n">uniform</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_pixels</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span> <span class="o">==</span> <span class="mi">1</span>
    
    <span class="k">if</span> <span class="n">pixels_median</span> <span class="ow">in</span> <span class="n">greymap</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">greymap</span><span class="p">[</span><span class="n">pixels_median</span><span class="p">]</span>
        <span class="n">code</span> <span class="o">=</span> <span class="n">shapemap</span><span class="p">[</span><span class="n">greymap</span><span class="p">[</span><span class="n">pixels_median</span><span class="p">]]</span>
        
    <span class="k">return</span> <span class="p">(</span><span class="n">pixels_median</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">uniform</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can apply that function to each row of the dataframe by iterating over pairs of rows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shapes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># The number of row pairs is half the number of rows</span>
<span class="n">num_pairs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clean_data_df</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pairs</span><span class="p">):</span>
    
    <span class="c1"># Retrieve a pair of images </span>
    <span class="c1"># from the datalog dataframe:</span>
    <span class="n">left_img</span><span class="p">,</span> <span class="n">right_img</span> <span class="o">=</span> <span class="n">get_sensor_image_pair</span><span class="p">(</span><span class="n">roboSim</span><span class="o">.</span><span class="n">image_data</span><span class="p">(),</span> <span class="n">i</span><span class="p">)</span>
    
    <span class="c1"># Decode the label image</span>
    <span class="p">(</span><span class="n">grey</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">uniform</span><span class="p">)</span> <span class="o">=</span> <span class="n">decode_shape_label</span><span class="p">(</span><span class="n">right_img</span><span class="p">)</span>
    
    <span class="c1"># Add the label to a list of labels found so far</span>
    <span class="n">shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Display the result of decoding</span>
    <span class="c1"># the median pixel value</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Grey: </span><span class="si">{</span><span class="n">grey</span><span class="si">}</span><span class="s2">; shape: </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">; code: </span><span class="si">{</span><span class="n">code</span><span class="si">}</span><span class="s2">; uniform: </span><span class="si">{</span><span class="n">uniform</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can also use the <code class="docutils literal notranslate"><span class="pre">decode_shape_label()</span></code> function as part of another function that will return a shape training image and its associated label from a left and right sensor row pair in the datalog dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_training_data</span><span class="p">(</span><span class="n">raw_df</span><span class="p">,</span> <span class="n">pair_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get training image and label from raw dataframe.&quot;&quot;&quot;</span>
    
    <span class="c1"># Get the left and right images</span>
    <span class="c1"># at specified pair index</span>
    <span class="n">left_img</span><span class="p">,</span> <span class="n">right_img</span> <span class="o">=</span> <span class="n">get_sensor_image_pair</span><span class="p">(</span><span class="n">raw_df</span><span class="p">,</span>
                                            <span class="n">pair_index</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">decode_shape_label</span><span class="p">(</span><span class="n">right_img</span><span class="p">)</span>
    <span class="p">(</span><span class="n">grey</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">uniform</span><span class="p">)</span> <span class="o">=</span> <span class="n">response</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">left_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To use the <code class="docutils literal notranslate"><span class="pre">get_training_data()</span></code> function, we pass it the datalog dataframe and the index of the desired image pair:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pair_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="c1"># Get the response tuple as a single variable</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">get_training_data</span><span class="p">(</span><span class="n">clean_data_df</span><span class="p">,</span> <span class="n">pair_index</span><span class="p">)</span>

<span class="c1"># Then unpack the tuple</span>
<span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">training_img</span><span class="p">)</span> <span class="o">=</span> <span class="n">response</span>

<span class="nb">print</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">uniform</span><span class="p">)</span>
<span class="n">zoom_img</span><span class="p">(</span><span class="n">training_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In summary, we can now:</p>
<ul class="simple">
<li><p>grab the greyscale training image</p></li>
<li><p>find the median greyscale value</p></li>
<li><p>try to decode that value to a shape label/code</p></li>
<li><p>return the shape label and code associated with that greyscale image, along with an indicator of whether the image is in view via the <code class="docutils literal notranslate"><span class="pre">uniform</span></code> training image array flag</p></li>
<li><p>label the corresponding shape image with the appropriate label.</p></li>
</ul>
</div>
<div class="section" id="real-time-data-collection">
<h2>4.2 Real-time data collection<a class="headerlink" href="#real-time-data-collection" title="Permalink to this headline">¶</a></h2>
<p>In this section, you will start to explore how to collect data in real time as the robot drives over the images, rather than being teleported directly on top of them.</p>
<div class="section" id="identifying-when-the-robot-is-over-a-pattern-in-real-time">
<h3>4.2.1 Identifying when the robot is over a pattern in real time<a class="headerlink" href="#identifying-when-the-robot-is-over-a-pattern-in-real-time" title="Permalink to this headline">¶</a></h3>
<p>If we want to collect data from the robot as it drives slowly over the images, then we need to be able to identify when it is passing over the images so we can trigger the image sampling.</p>
<p>The following program will slowly drive over the test patterns, logging the reflected light sensor values every so often. Start the program using the simulator <em>Run</em> button or the simulator <code class="docutils literal notranslate"><span class="pre">R</span></code> keyboard shortcut.</p>
<p>From the traces on the simulator chart, can you identify when the robot passes over the images?</p>
<p><em>Record your observations here.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">sim_magic_preloaded</span> <span class="o">-</span><span class="n">b</span> <span class="n">Simple_Shapes</span> <span class="o">-</span><span class="n">x</span> <span class="mi">100</span> <span class="o">-</span><span class="n">y</span> <span class="mi">900</span> <span class="o">-</span><span class="n">OAc</span>

<span class="n">say</span><span class="p">(</span><span class="s2">&quot;On my way..&quot;</span><span class="p">)</span>

<span class="c1"># Start driving forwards slowly</span>
<span class="n">tank_drive</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">SpeedPercent</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">SpeedPercent</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

<span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Drive forward no further than a specified distance</span>
<span class="k">while</span> <span class="nb">int</span><span class="p">(</span><span class="n">tank_drive</span><span class="o">.</span><span class="n">left_motor</span><span class="o">.</span><span class="n">position</span><span class="p">)</span><span class="o">&lt;</span><span class="mi">1500</span><span class="p">:</span>
    
    <span class="n">left_light</span> <span class="o">=</span> <span class="n">colorLeft</span><span class="o">.</span><span class="n">reflected_light_intensity_pc</span>
    <span class="n">right_light</span> <span class="o">=</span> <span class="n">colorRight</span><span class="o">.</span><span class="n">reflected_light_intensity_pc</span>
    
    <span class="c1"># report every fifth pass of the loop</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">count</span> <span class="o">%</span> <span class="mi">5</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Light_left: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">left_light</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Light_right: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">right_light</span><span class="p">))</span>

    <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">say</span><span class="p">(</span><span class="s1">&#39;All done&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Based on your observations, describe a strategy you might use to capture image sample data when the test images are largely in view.</em></p>
</div>
<div class="section" id="optional-challenge-capturing-image-data-in-real-time">
<h3>4.2.2 Optional challenge – Capturing image data in real time<a class="headerlink" href="#optional-challenge-capturing-image-data-in-real-time" title="Permalink to this headline">¶</a></h3>
<p>Using your observations regarding the reflected light sensor values as the robot crosses the images, or otherwise, write a program to collect image data from the simulator in real time as the robot drives over them.</p>
<p><em>Describe your program strategy and record your program design notes here.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="capturing-image-data-in-real-time">
<h3>4.2.3 Capturing image data in real time<a class="headerlink" href="#capturing-image-data-in-real-time" title="Permalink to this headline">¶</a></h3>
<p>By observation of the reflected light sensor data in the chart, the robot appears to be over a shape, as the reflected light sensor values drop below about 85%.</p>
<p>From the chart, we might also notice that the training label image (encoded as the solid grey square presented to the right-hand sensor) gives distinct readings for each shape.</p>
<p>We can therefore use a drop in the reflected light sensor value to trigger the collection of the image data.</p>
<p>First, let’s clear the datalog:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clear the datalog to give us a fresh start</span>
<span class="n">roboSim</span><span class="o">.</span><span class="n">clear_datalog</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can write a program to drive the robot forwards slowly and collect the image data when it is over an image:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">sim_magic_preloaded</span> <span class="o">-</span><span class="n">b</span> <span class="n">Simple_Shapes</span> <span class="o">-</span><span class="n">x</span> <span class="mi">100</span> <span class="o">-</span><span class="n">y</span> <span class="mi">900</span> <span class="o">-</span><span class="n">OAR</span>

<span class="n">say</span><span class="p">(</span><span class="s2">&quot;Getting started.&quot;</span><span class="p">)</span>
    
<span class="c1"># Start driving forwards slowly</span>
<span class="n">tank_drive</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">SpeedPercent</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">SpeedPercent</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># Drive forward no futher than a specified distance</span>
<span class="k">while</span> <span class="nb">int</span><span class="p">(</span><span class="n">tank_drive</span><span class="o">.</span><span class="n">left_motor</span><span class="o">.</span><span class="n">position</span><span class="p">)</span><span class="o">&lt;</span><span class="mi">1200</span><span class="p">:</span>
    
    <span class="c1"># Sample the right sensor</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">colorRight</span><span class="o">.</span><span class="n">reflected_light_intensity_pc</span>
    <span class="c1"># If we seem to be over a test label,</span>
    <span class="c1"># grab the image data into the datalog</span>
    <span class="k">if</span> <span class="n">sample</span> <span class="o">&lt;</span> <span class="mi">85</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;image_data both&quot;</span><span class="p">)</span>

<span class="n">say</span><span class="p">(</span><span class="s2">&quot;All done.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we review the images in the datalog, we should see they all contain at least a fragment of the image data (this may take a few moments to run). The following code cell grabs images where the <code class="docutils literal notranslate"><span class="pre">uniform</span></code> flag is set on the encoded label image and adds those training samples to a list (<code class="docutils literal notranslate"><span class="pre">training_images</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_images</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">roboSim</span><span class="o">.</span><span class="n">image_data</span><span class="p">())</span><span class="o">/</span><span class="mi">2</span><span class="p">)):</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">get_training_data</span><span class="p">(</span><span class="n">roboSim</span><span class="o">.</span><span class="n">image_data</span><span class="p">(),</span> <span class="n">i</span><span class="p">)</span>
    
    <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">training_img</span><span class="p">)</span> <span class="o">=</span> <span class="n">response</span>
    
    <span class="c1"># Likely shape</span>
    <span class="k">if</span> <span class="n">uniform</span><span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">training_img</span><span class="p">)</span>
        <span class="n">training_images</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">shape</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">training_img</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your own observations here about how ‘clean’ the captured training images are.</em></p>
<p>We can cast the list of training images in the convenient form of a <em>pandas</em> dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">training_images</span><span class="p">,</span>
                           <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;image&#39;</span><span class="p">])</span>

<span class="n">training_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can get training image and training label lists as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_images</span> <span class="o">=</span> <span class="n">training_df</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">training_labels</span> <span class="o">=</span> <span class="n">training_df</span><span class="p">[</span><span class="s1">&#39;code&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We are now in a position to try to use the data collected by travelling over the test track to train the neural network.</p>
</div>
</div>
<div class="section" id="training-an-mlp-to-recognise-the-patterns">
<h2>4.3 Training an MLP to recognise the patterns<a class="headerlink" href="#training-an-mlp-to-recognise-the-patterns" title="Permalink to this headline">¶</a></h2>
<p>In an earlier activity, we discovered that the bounding box method we used to distinguish fruits did not provide a set of features that we could use to distinguish the different shapes.</p>
<p>So let’s just use a ‘naive’ training approach and just train the network on the 14 × 14 pixels in the centre of each sensor image array.</p>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">quick_progress_tracked_training()</span></code> function we used previously to train an MLP using the scanned image shapes. We can optionally use the <code class="docutils literal notranslate"><span class="pre">jiggled=True</span></code> parameter to add some variation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">quick_progress_tracked_training</span>


<span class="c1"># Specify some parameters</span>
<span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">40</span><span class="p">)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">500</span>


<span class="c1"># Create a new MLP</span>
<span class="n">MLP</span> <span class="o">=</span> <span class="n">quick_progress_tracked_training</span><span class="p">(</span><span class="n">training_images</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">,</span>
                                      <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span>
                                      <span class="n">max_iterations</span><span class="o">=</span><span class="n">max_iterations</span><span class="p">,</span>
                                      <span class="n">report</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">jiggled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can use the following code cell to randomly select images from the training samples and test the network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">predict_and_report_from_image</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_images</span><span class="p">))</span>
<span class="n">test_image</span> <span class="o">=</span> <span class="n">training_images</span><span class="p">[</span><span class="n">sample</span><span class="p">]</span>
<span class="n">test_label</span> <span class="o">=</span> <span class="n">training_labels</span><span class="p">[</span><span class="n">sample</span><span class="p">]</span>

<span class="n">predict_and_report_from_image</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">test_image</span><span class="p">,</span> <span class="n">test_label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Record your observations about how well the network performs.</em></p>
</div>
<div class="section" id="testing-the-network-on-a-new-set-of-collected-data">
<h2>4.4 Testing the network on a new set of collected data<a class="headerlink" href="#testing-the-network-on-a-new-set-of-collected-data" title="Permalink to this headline">¶</a></h2>
<p>Let’s collect some data again by driving the robot over a second, slightly shorter test track at <code class="docutils literal notranslate"><span class="pre">y=700</span></code> to see if we can recognise the images.</p>
<p>There are no encoded training label images in this track, so we will either have to rely on just the reflected light sensor value to capture legitimate images for us, or we will need to pre-process the images to discard ones that are only partial image captures.</p>
<div class="section" id="collecting-the-test-data">
<h3>4.4.1 Collecting the test data<a class="headerlink" href="#collecting-the-test-data" title="Permalink to this headline">¶</a></h3>
<p>The following program will stop as soon as the reflected light value from the left sensor drops below 85. How much of the image can we see?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">sim_magic_preloaded</span> <span class="o">-</span><span class="n">b</span> <span class="n">Simple_Shapes</span> <span class="o">-</span><span class="n">x</span> <span class="mi">100</span> <span class="o">-</span><span class="n">y</span> <span class="mi">700</span> <span class="o">-</span><span class="n">OAR</span>

<span class="n">say</span><span class="p">(</span><span class="s1">&#39;Starting&#39;</span><span class="p">)</span>
<span class="c1"># Start driving forwards slowly</span>
<span class="n">tank_drive</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">SpeedPercent</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">SpeedPercent</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

<span class="c1"># Sample the left sensor</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">colorLeft</span><span class="o">.</span><span class="n">reflected_light_intensity_pc</span>
    
<span class="c1"># Drive forward no futher than a specified distance</span>
<span class="k">while</span> <span class="n">sample</span><span class="o">&gt;</span><span class="mi">85</span><span class="p">:</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">colorLeft</span><span class="o">.</span><span class="n">reflected_light_intensity_pc</span>

<span class="n">say</span><span class="p">(</span><span class="s2">&quot;All done.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>That’s perhaps a bit optimistic for a sensible attempt at image recognition.</p>
<p>However, recalling that the black pixel count for the training images ranged from 49 for the square to 60 for one of the equilateral triangles, we could tag an image as likely to contain a potentially recognisable image if its black pixel count exceeds 45.</p>
<p>To give us some data to work with, let’s collect samples for the new test set at <code class="docutils literal notranslate"><span class="pre">y=700</span></code>. First clear the datalog:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clear the datalog to give us a fresh start</span>
<span class="n">roboSim</span><span class="o">.</span><span class="n">clear_datalog</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>And then grab the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">sim_magic_preloaded</span> <span class="o">-</span><span class="n">b</span> <span class="n">Simple_Shapes</span> <span class="o">-</span><span class="n">x</span> <span class="mi">100</span> <span class="o">-</span><span class="n">y</span> <span class="mi">700</span> <span class="o">-</span><span class="n">OAR</span>

<span class="n">say</span><span class="p">(</span><span class="s2">&quot;Starting&quot;</span><span class="p">)</span>

<span class="c1"># Start driving forwards slowly</span>
<span class="n">tank_drive</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">SpeedPercent</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">SpeedPercent</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

<span class="c1"># Drive forward no futher than a specified distance</span>
<span class="k">while</span> <span class="nb">int</span><span class="p">(</span><span class="n">tank_drive</span><span class="o">.</span><span class="n">left_motor</span><span class="o">.</span><span class="n">position</span><span class="p">)</span><span class="o">&lt;</span><span class="mi">800</span><span class="p">:</span>
    
    <span class="c1"># Sample the right sensor</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">colorLeft</span><span class="o">.</span><span class="n">reflected_light_intensity_pc</span>
    <span class="c1"># If we seem to be over a test label,</span>
    <span class="c1"># grab the image data into the datalog</span>
    <span class="k">if</span> <span class="n">sample</span> <span class="o">&lt;</span> <span class="mi">85</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;image_data both&quot;</span><span class="p">)</span>

<span class="n">say</span><span class="p">(</span><span class="s2">&quot;All done.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="generating-the-test-set">
<h3>4.4.2 Generating the test set<a class="headerlink" href="#generating-the-test-set" title="Permalink to this headline">¶</a></h3>
<p>We can now generate a clean test set of images based on a minimum required number of black pixels. The following function grabs the test images and also counts the black pixels in the left image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_test_data</span><span class="p">(</span><span class="n">raw_df</span><span class="p">,</span> <span class="n">pair_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get test image and label from raw dataframe.&quot;&quot;&quot;</span>
    
    <span class="c1"># Get the left and right images</span>
    <span class="c1"># at specified pair index</span>
    <span class="n">left_img</span><span class="p">,</span> <span class="n">right_img</span> <span class="o">=</span> <span class="n">get_sensor_image_pair</span><span class="p">(</span><span class="n">raw_df</span><span class="p">,</span>
                                            <span class="n">pair_index</span><span class="p">)</span>
    
    <span class="c1"># Get the pixel count</span>
    <span class="n">left_pixel_cnt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">left_img</span><span class="o">.</span><span class="n">getdata</span><span class="p">()))</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">left_pixel_cnt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="mi">0</span> <span class="ow">in</span> <span class="n">left_pixel_cnt</span> <span class="k">else</span> <span class="mi">0</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">left_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following cell creates a filtered list of potentially recognisable images. You may recall seeing a similarly structured code fragment previously when we used the <code class="docutils literal notranslate"><span class="pre">uniform</span></code> flag to select the images. However, in this case, we only save an image to a list if we see the black pixel count decreasing.</p>
<p>Having got a candidate image, the <code class="docutils literal notranslate"><span class="pre">crop_and_pad_to_fit()</span></code> function crops it and tries to place it in the centre of the image array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.sensor_data</span> <span class="kn">import</span> <span class="n">crop_and_pad_to_fit</span>

<span class="n">test_images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">possible_img</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">possible_count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">roboSim</span><span class="o">.</span><span class="n">image_data</span><span class="p">())</span><span class="o">/</span><span class="mi">2</span><span class="p">)):</span>
    <span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">left_img</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_test_data</span><span class="p">(</span><span class="n">roboSim</span><span class="o">.</span><span class="n">image_data</span><span class="p">(),</span> <span class="n">i</span><span class="p">)</span>
    <span class="c1"># On the way in to a shape, we have</span>
    <span class="c1"># an increasing black pixel count</span>
    <span class="k">if</span> <span class="n">count</span> <span class="ow">and</span> <span class="n">count</span> <span class="o">&gt;=</span> <span class="n">possible_count</span><span class="p">:</span>
        <span class="n">possible_img</span> <span class="o">=</span> <span class="n">left_img</span>
        <span class="n">possible_count</span> <span class="o">=</span> <span class="n">count</span>
    <span class="c1"># We&#39;re perhaps now on the way out...</span>
    <span class="c1"># Do we have a possible shape?</span>
    <span class="k">elif</span> <span class="n">possible_img</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">possible_count</span> <span class="o">&gt;</span> <span class="mi">45</span><span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">possible_count</span><span class="p">,</span> <span class="n">left_img</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---&#39;</span><span class="p">)</span>
        <span class="n">possible_img</span> <span class="o">=</span> <span class="n">crop_and_pad_to_fit</span><span class="p">(</span><span class="n">possible_img</span><span class="p">)</span>
        <span class="n">test_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">possible_img</span><span class="p">)</span>
        <span class="n">possible_img</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># We have now gone past the image</span>
    <span class="k">elif</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="mi">35</span><span class="p">:</span>
        <span class="n">possible_count</span> <span class="o">=</span> <span class="mi">0</span>
        
<span class="n">test_images</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="testing-the-data">
<h3>4.4.3 Testing the data<a class="headerlink" href="#testing-the-data" title="Permalink to this headline">¶</a></h3>
<p>Having got our images, we can now try to test them with the MLP.</p>
<p>Recall that the <code class="docutils literal notranslate"><span class="pre">codemap</span></code> dictionary maps from code values to shape name:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nn_tools.network_views</span> <span class="kn">import</span> <span class="n">class_predict_from_image</span>

<span class="c1"># Random sample from the test images</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_images</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">test_img</span> <span class="o">=</span> <span class="n">test_images</span><span class="p">[</span><span class="n">sample</span><span class="p">]</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">class_predict_from_image</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">test_img</span><span class="p">)</span>

<span class="c1"># How did we do?</span>
<span class="n">display</span><span class="p">(</span><span class="n">codemap</span><span class="p">[</span><span class="n">prediction</span><span class="p">])</span>
<span class="n">zoom_img</span><span class="p">(</span><span class="n">test_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="save-the-mlp">
<h3>4.4.4 Save the MLP<a class="headerlink" href="#save-the-mlp" title="Permalink to this headline">¶</a></h3>
<p>Save the MLP so we can use it again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">dump</span>

<span class="n">dump</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="s1">&#39;mlp_shapes_14x14.joblib&#39;</span><span class="p">)</span> 

<span class="c1"># Load it back</span>
<span class="c1">#from joblib import load</span>

<span class="c1">#MLP = load(&#39;mlp_shapes_14x14.joblib&#39;)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="summary">
<h2>4.5 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In this notebook, you have seen how we can collect data in real time from the simulator by sampling images when the robot detects a change in the reflected light levels.</p>
<p>Using a special test track, with paired shape and encoded label images, we were able to collect a set of shape-based training patterns that could be used to train an MLP to recognise the shapes.</p>
<p>Investigation of the shape images revealed that simple black pixel counts and bounding box dimensions did not distinguish between the shapes, so we simply trained the network on the raw images.</p>
<p>Running the robot over a test track without any paired encoded label images, we were still able to detect when the robot was over the image based on the black pixel count of the shape image. On testing the MLP against newly collected shape data, the neural network was able to correctly classify the collected patterns.</p>
<p>In the next notebook, you will explore how the robot may be able to identify the shapes in real time as part of a multi-agent system working in partnership with a pattern-recognising agent running in the notebook Python environment.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./08. Remote services and multi-agent systems"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="08.3%20Recognising%20digits%20using%20a%20convolutional%20neural%20network%20%28optional%29.html" title="previous page">3 Recognising digits using a convolutional neural network (optional)</a>
    <a class='right-next' id="next-link" href="08.5%20Messaging%20in%20multi-agent%20systems.html" title="next page">5 Messaging in multi-agent systems</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>