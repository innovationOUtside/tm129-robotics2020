{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-danger"
    ]
   },
   "source": [
    "__This notebook contains optional study material. You are not required to work through it in order to meet the learning objectives or complete the assessments associated with this module.__\n",
    "\n",
    "*This notebook demonstrates the effectiveness of a pre-trained convolutional neural network (CNN) at classifying MLP handwritten digit images.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Recognising digits using a convolutional neural network (optional)\n",
    "\n",
    "In the previous notebook, you saw how we could collect image data sampled by the robot within the simulator into the notebook environment and then test the collected images against an \"offboard\" pre-trained multilayer perceptron run via the notebook's Python environment. However, even with an MLP tested on \"jiggled\" images, the network's classification performance degrades when \"off-center\" images are presented to it.\n",
    "\n",
    "In this notebook, you will see how we can use a convolutional neural network running in the notebook's Python environment to classify images retrieved from the robot in the simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Using a pre-trained convolutional neural network\n",
    "\n",
    "Although training a convolutional neural network can take quite a lot of time, and a *lot* of computational effort, off-the-shelf pre-trained models are also increasingly available. However, whilst this means you may be able to get started on a recognition task without the requirement to build your own model, you would do well to remember the phrase *caveat emptor*: buyer beware.\n",
    "\n",
    "When you use a pre-trained model, you may not know what data it was trained against (and what biases it may include because of that), and you may not know what weaknesses there may be in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "As with any area of IT, privacy and security concerns must always be taken into account. With the increasing number of neural networks being deployed, they are starting to become attractive to attackers, although we will not be considering such matters in this module (for an example of related concerns, see *Biggio, B. and Roli, F., 2018. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 84, pp.317-331* [[PDF](https://arxiv.org/pdf/1712.03141.pdf)]).\n",
    "\n",
    "However, you should be aware when using third party models that they may incorporate risks and threats when you come to use them. For example, __risks__ associated with *bias* in the training data used to train the network, or in its final trained performance; or __threats__ in terms of incorporating patterns that are deliberately misidentified compared to how you might ordinarily expect them to be identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses a pre-trained convolutional neural network model implemented as a TensorFlow Lite model. [*TensorFlow Lite*](https://www.tensorflow.org/lite/) is a framework developed to support the deployment of TensorFlow Model on internet of things (IoT) devices. As such, the models are optimised to be as small as possible and to be evaluated as computationally quickly and efficiently as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Loading the CNN\n",
    "\n",
    "The first thing we need to do is to load in the model. The actual TensorFlow Lite framework code is a little bit fiddly in places, so we'll use some convenience functions to make using the framework slightly easier.\n",
    "\n",
    "The first thing we need to do is to load in the CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import cnn_load\n",
    "\n",
    "cnn = cnn_load(fpath='./mnist.tflite',\n",
    "               fpath_labels='./mnist_tflite_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then preview the architecture of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import cnn_get_details\n",
    "cnn_get_details(cnn, report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main take away from this report are the items that describe the structure of the input and output arrays. In particular, we have an input array of a single 28x28 pixel greyscale image array, and an output of 10 classification classes. Each output gives the probability with which the CNN believes the image represents the corresponding digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Testing the network\n",
    "\n",
    "We'll test the network with images retrieved from the simulator.\n",
    "\n",
    "First, load in the simulator in the normal way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbev3devsim.load_nbev3devwidget import roboSim, eds\n",
    "\n",
    "%load_ext nbev3devsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the *MNIST_Digits* background and sample an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b MNIST_Digits_Black -OA -R -x 400 -y 850\n",
    "\n",
    "# Configure a light sensor\n",
    "colorLeft = ColorSensor(INPUT_2)\n",
    "\n",
    "#Sample the light sensor reading\n",
    "sensor_value = colorLeft.reflected_light_intensity\n",
    "\n",
    "# This is a command invocation rather than a print statement\n",
    "print(\"image_data left\")\n",
    "# The command is responded to by\n",
    "# the \"Image data logged...\" message display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the data from the simulator into the notebook Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roboSim.image_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the last image collected, cropping it to the central focal area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import generate_image, zoom_img\n",
    "index = -1 # Get the last image in the dataframe\n",
    "\n",
    "img = generate_image(roboSim.image_data(),\n",
    "                     index,\n",
    "                     crop=(3, 3, 17, 17),\n",
    "                     resize=(28, 28))\n",
    "zoom_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now present this image to the CNN and see what digit it thinks the image corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nn_tools.network_views import cnn_test_with_image\n",
    "\n",
    "# Pass rank=N to print top N ranked results\n",
    "cnn_test_with_image(cnn, img, rank=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perturb that image slightly by randomly jiggling it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import jiggle\n",
    "\n",
    "jiggled_img = jiggle(img)\n",
    "\n",
    "zoom_img(img), zoom_img(jiggled_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test how well the network copes with classifying it when it has been randomly translated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_with_image(cnn, jiggled_img, rank=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the network perform if we offset the image retrieved from the simulator?\n",
    "\n",
    "Use some magic to relocate the robot slightly off-center from one of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sim_magic -OAR -x 398 -y 848"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grab the sensor data and generate an image from it. Can you see how the image is now offset from the central location, as some of the jiggled images were?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = generate_image(roboSim.image_data(),\n",
    "                     index,\n",
    "                     crop=(3, 3, 17, 17),\n",
    "                     resize=(28, 28))\n",
    "zoom_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this offset image to see if our convolutional neural network can still correctly identify the digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_with_image(cnn, img, rank=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 3.1.3 Activity â€” Testing the CNN using robot collected image samples\n",
    "\n",
    "The `ipywidget` powered end user application defined in the code cell below will place the robot at a randomly selected digit location and display and then test the image grabbed from *the previous location* using the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "Run the application several times. How successful is the CNN at classifying the image?\n",
    "\n",
    "Tick the *location_noise* box to add a small amount of perturbation to where the robot is placed. How well does the CNN perform?\n",
    "\n",
    "Can the CNN still recognised the image in the presence of sensor noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "@interact_manual(location_noise=True)\n",
    "def random_MNIST_location(location_noise = False):\n",
    "    \"\"\"Place the robot at a random MNIST digit location.\"\"\"\n",
    "    _x = random.randint(0, 19)*100+100\n",
    "    _y = random.randint(0, 10)*100+50\n",
    "    \n",
    "    if location_noise:\n",
    "        _x += random.randint(-3, 3)\n",
    "        _y += random.randint(-3, 3)\n",
    "\n",
    "    %sim_magic -OAR -x $_x -y $_y\n",
    "    img = generate_image(roboSim.image_data(),\n",
    "                     -1,\n",
    "                     crop=(3, 3, 17, 17),\n",
    "                     resize=(28, 28))\n",
    "    zoom_img(img)\n",
    "    cnn_test_with_image(cnn, img, rank=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Summary\n",
    "\n",
    "In this notebook, you have seen how we can use a convolutional neural network to identify handwritten digits scanned by the robot in the simulator.\n",
    "\n",
    "In the next notebook, you will see how we can collect data from a new dataset, along with training labels, and then use that data to train a new neural network."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "formats": "ipynb,.md//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
