{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# 4 Recognising patterns on the move\n",
    "\n",
    "To be really useful a robot needs to recognise things as it goes along, or ‘on the fly’. In this notebook, you will train a neural network to use a simple MLP classifier to try to identify different shapes on the background. The training samples themselves, images *and* training labels, will be captured by the robot from the simulator background.\n",
    "\n",
    "We will use the two light sensors to collect the data used to train the network:\n",
    "\n",
    "- one light sensor will capture the shape image data\n",
    "- one light sensor will capture the training class data.\n",
    "\n",
    "To begin with we will contrive things somewhat to collect the data at specific locations on the background. But then you will explore how we can collect images as the robot moves more naturally within the environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "*There is quite a lot of provided code in this notebook. You are not necessarily expected to be able to create this sort of code yourself. Instead, try to focus on the process of how various tasks are broken down into smaller discrete steps, as well as how small code fragments can be combined to create \"higher level\" functions that perform ever more powerful tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Before continuing, ensure the simulator is loaded and available:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbev3devsim.load_nbev3devwidget import roboSim, eds\n",
    "%load_ext nbev3devsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The background image *Simple_Shapes* contains several shapes arranged in a line, including a square, a circle, four equilateral triangles (arrow heads) with different orientations, a diamond and a rectangle.\n",
    "\n",
    "Just below each shape is a grey square, whose fill colour is used to distinguish between the different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sim_magic -b Simple_Shapes -x 600 -y 900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Evaluating the possible training data\n",
    "\n",
    "In this initial training pass, we will check whether the robot can clearly observe the potential training pairs. Each training pair consists of the actual shape image as well as a solid grey square, where the grey colour is used to represent one of six different training classes.\n",
    "\n",
    "The left light sensor will be used to sample the shape image data; the right light sensor will be used to collect the simpler grey classification group pattern.\n",
    "\n",
    "As we are going to be pulling data into the notebook Python environment from the simulator, ensure the local notebook datalog is cleared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roboSim.clear_datalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Simple_Shapes* background we are using in this notebook contains several small regular shapes, with label encoding patterns alongside.\n",
    "\n",
    "The *x* and *y* locations for sampling the eight different images, along with a designator for each shape, as are follows:\n",
    "\n",
    "- 200 900 square\n",
    "- 280 900 right-facing triangle\n",
    "- 360 900 left-facing triangle\n",
    "- 440 900 downwards-facing triangle\n",
    "- 520 900 upwards-facing triangle\n",
    "- 600 900 diamond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to collect image data from the robot's light sensors. The `-R` switch runs the program once it has been downloaded to the simulator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print the message `\"image_data both\"` then we can collect data from both the left and the right light sensors at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b Simple_Shapes -AR -x 520 -y 900 -O\n",
    "\n",
    "# Sample the light sensor reading\n",
    "sensor_value = colorLeft.reflected_light_intensity\n",
    "\n",
    "# This is essentially a command invocation\n",
    "# not just a print statement!\n",
    "print(\"image_data both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can preview the collected image data in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roboSim.image_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also collect consecutive rows of data from the dataframe and decode them as left and right images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import get_sensor_image_pair\n",
    "from nn_tools.sensor_data import zoom_img\n",
    "\n",
    "pair_index = -1\n",
    "\n",
    "left_img, right_img = get_sensor_image_pair(roboSim.image_data(),\n",
    "                                            pair_index)\n",
    "zoom_img(left_img), zoom_img(right_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't see a figure image displayed, check that the robot is placed over a figure by reviewing the sensor array display in the simulator. If the image is there, re-run the previous code cell to see if the data is now available. If it isn't, re-run the data-collecting magic cell, wait a view seconds, and then try to view the zoomed image display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the previously downloaded program again from a simple line magic that situates the robot at a specific location and then runs the program to collect the sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = 280\n",
    "\n",
    "%sim_magic -x $_x -y 900 -RAH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Investigating the training data samples\n",
    "\n",
    "Let's start by seeing if we can collect image data samples for each of the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "from nbev3devsim.load_nbev3devwidget import tqdma\n",
    "\n",
    "import time\n",
    "\n",
    "# Clear the datalog to give us a fresh start\n",
    "roboSim.clear_datalog()\n",
    "\n",
    "# x-coordinate for centreline of first shape\n",
    "_x_init = 200\n",
    "\n",
    "# Distance between shapes\n",
    "_x_gap = 80\n",
    "\n",
    "# Number of shapes\n",
    "_n_shapes = 6\n",
    "\n",
    "# y-coordinate for centreline of shapes\n",
    "_y = 900\n",
    "\n",
    "# Load in the required background\n",
    "%sim_magic -b Simple_Shapes\n",
    "\n",
    "# Generate x coordinate for each shape in turn\n",
    "for _x in trange(_x_init, _x_init+(_n_shapes*_x_gap), _x_gap):\n",
    "    \n",
    "    # Jump to shape and run program to collect data\n",
    "    %sim_magic -x $_x -y $_y -R\n",
    "    \n",
    "    # Wait a short period to allow time for\n",
    "    # the program to run and capture the sensor data,\n",
    "    # and for the data to be passed from the simulator\n",
    "    # to the notebook Python environment\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to access multiple image samples via `roboSim.image_data()`, which returns a dataframe containing as many rows as images we scanned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df = roboSim.image_data()\n",
    "clean_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original sensor data is collected as three-channel RGB data. By default, the `get_sensor_image_pair()` function, which extracts a pair of consecutive images from the datalog, converts these to greyscale images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import get_sensor_image_pair\n",
    "\n",
    "pair_index = -1\n",
    "\n",
    "left_img, right_img = get_sensor_image_pair(clean_data_df,\n",
    "                                            pair_index)\n",
    "\n",
    "zoom_img(left_img), zoom_img(right_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter the dataframe to give us a dataframe containing just the data grabbed from the left-hand image sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mechanics behind how this line of code\n",
    "# works are beyond the scope of this module.\n",
    "# In short, we identify the rows where the\n",
    "# \"side\" column value is equal to \"left\"\n",
    "# and select just those rows.\n",
    "clean_left_images_df = clean_data_df[clean_data_df['side']=='left']\n",
    "clean_left_images_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape names and classes are defined as follows in the order they appear going from left to right along the test track. We can also derive a map going the other way, from code to shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classes\n",
    "shapemap = {'square': 0,\n",
    "            'right facing triangle': 1,\n",
    "            'left facing triangle': 2,\n",
    "            'downwards facing triangle': 3,\n",
    "            'upwards facing triangle': 4,\n",
    "            'diamond': 5\n",
    "           }\n",
    "\n",
    "codemap = {shapemap[k]:k for k in shapemap}\n",
    "codemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Counting the number of black pixels in each shape\n",
    "\n",
    "Ever mindful that we are on the look out for features that might help us distinguish between the different shapes, let's check a really simple measure: the number of black-filled pixels in each shape.\n",
    "\n",
    "If we cast the pixel data for the image in the central focus areas of the image array to a *pandas* *Series*, then we can use the *Series* `.value_counts()` method to count the number of each unique pixel value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Each column in a `pandas` dataframe is a `pandas.Series` object. Casting a list of data to a `Series` provides us with many convenient tools for manipulating and summarising that data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import generate_image, sensor_image_focus\n",
    "import pandas as pd\n",
    "\n",
    "for index in range(len(clean_left_images_df)):\n",
    "    print(index)\n",
    "    # Get the central focal area of the image\n",
    "    left_img = sensor_image_focus(generate_image(clean_left_images_df, index))\n",
    "    \n",
    "    # Count of each pixel value\n",
    "    pixel_series = pd.Series(list(left_img.getdata()))\n",
    "    # The .value_counts() method tallies occurrences\n",
    "    # of each unique value in the Series\n",
    "    pixel_counts = pixel_series.value_counts()\n",
    "    \n",
    "    # Display the count and the image\n",
    "    display(codemap[index], left_img, pixel_counts)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the black (`0` value) pixel counts, we see that they do not uniquely identify the shapes. For example, the left- and right-facing triangles and the diamond all have 51 black pixels, so a simple pixel count *does not* provide a way to distinguish between the shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 4.1.3 Activity — Using bounding box sizes as a feature for distinguishing between shapes\n",
    "\n",
    "When we trained a neural network to recognise our fruit data, we used the dimensions of a bounding box drawn around the fruit as the input features to our network.\n",
    "\n",
    "Will the bounding box approach used there also allow us to distinguish between the shape images?\n",
    "\n",
    "Run the following code cell to convert the raw data associated with an image to a dataframe, and then prune the rows and columns around the edges that only contain white space.\n",
    "\n",
    "The dimensions of the dataframe, which is to say, the `.shape` of the dataframe, given as the 2-tuple `(rows, columns)`, corresponds to the bounding box of the shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import df_from_image, trim_image\n",
    "\n",
    "index = -1\n",
    "\n",
    "# The sensor_image_focus function crops\n",
    "# to the central focal area of the image array\n",
    "left_img = sensor_image_focus(generate_image(clean_left_images_df, index))\n",
    "\n",
    "trimmed_df = trim_image( df_from_image(left_img, show=False), reindex=True)\n",
    "\n",
    "# dataframe shape\n",
    "trimmed_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "Using the above code, or otherwise, find the shape of the bounding box for each shape as captured in the `roboSim.image_data` list.\n",
    "\n",
    "You may find it useful to use the provided code as the basis of a simple function that will:\n",
    "\n",
    "- take the index number for a particular image data scan\n",
    "- generate the image\n",
    "- find the size of the bounding box.\n",
    "\n",
    "Then you can iterate through all the rows in the `left_images_df` dataset, generate the corresponding image and its bounding box dimensions, and then display the image and the dimensions.\n",
    "\n",
    "*Hint: you can use a `for` loop defined as `for i in range(len(left_images_df)):` to iterate through each row of the dataframe and generate an appropriate index number, `i`, for each row.*\n",
    "\n",
    "Based on the shape dimensions alone, can you distinguish between the shapes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations here, identifying the bounding box dimensions for each shape (square, right-facing triangle, left-facing triangle, downwards-facing triangle, upwards-facing triangle, diamond). Are the shapes distinguishable using their bounding box sizes?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Example solution\n",
    "\n",
    "*Click the arrow in the sidebar or run this cell to reveal an example solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "Let's start by creating a simple function inspired by the supplied code that will display an image and its bounding box dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "def find_bounding_box(index):\n",
    "    \"\"\"Find bounding box for a shape in an image.\"\"\"\n",
    "    img = sensor_image_focus(generate_image(clean_left_images_df, index))\n",
    "    trimmed_df = trim_image( df_from_image(img, show=False), show=False, reindex=True)\n",
    "\n",
    "    # Show image and shape\n",
    "    display(img, trimmed_df.shape)\n",
    "\n",
    "find_bounding_box(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "We can then call this function by iterating through each image data record in the `roboSim.image_data` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(clean_left_images_df)):\n",
    "    find_bounding_box(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "Inspecting the results from my run (yours may be slightly different), several of the shapes appear to share the same bounding box dimensions:\n",
    "\n",
    "- the left- and right-facing triangles and the diamond have the same dimensions (`(11, 9)`).\n",
    "\n",
    "The square is clearly separated from the other shapes on the basis of its bounding box dimensions, but the other shapes all have dimensions that may be hard to distinguish between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Decoding the training label image\n",
    "\n",
    "The grey-filled squares alongside the shape images are used to encode a label describing the associated shape.\n",
    "\n",
    "The grey levels are determined by the following algorithm, in which we use the numerical class values to derive the greyscale value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "\n",
    "greymap = {nan: 'unknown'}\n",
    "\n",
    "# Generate greyscale value\n",
    "for shape in shapemap:\n",
    "    key = int(shapemap[shape] * 255/len(shapemap))\n",
    "    greymap[key] = shape\n",
    "    \n",
    "greymap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can decode the labels from the solid grey squares.\n",
    "\n",
    "To to try to make sure we are using actual shape image data, we can identify images in our training set if *all* the pixels in the right-hand image are the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img, right_img = get_sensor_image_pair(clean_data_df, -1)\n",
    "\n",
    "# Generate a set of distinct pixel values\n",
    "# from the right-hand image.\n",
    "# Return True if there is only one value\n",
    "# in the set. That is, all the values are the same.\n",
    "len(set(right_img.getdata())) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can be used to generate a greyscale image from a row of the dataframe, find the median pixel value within that image, and then try to decode it. We also return a flag (`uniform`) that identifies if all the pixels in the right-hand encoded label image are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_shape_label(img, background=255):\n",
    "    \"\"\"Decode the shape from the greyscale image.\"\"\"\n",
    "    # Get the image greyscale pixel data\n",
    "    # The pandas Series is a convenient representation\n",
    "    image_pixels = pd.Series(list(img.getdata()))\n",
    "    \n",
    "    # Find the median pixel value\n",
    "    pixels_median = int(image_pixels.median())\n",
    "    \n",
    "    shape = None\n",
    "    code= None\n",
    "    # uniform = len(set(img.getdata())) == 1\n",
    "    # There is often more than one way to do it!\n",
    "    # The following makes use of Series.unique()\n",
    "    # which identifies the distinct values in a Series\n",
    "    uniform = len(image_pixels.unique()) == 1\n",
    "    \n",
    "    if pixels_median in greymap:\n",
    "        shape = greymap[pixels_median]\n",
    "        code = shapemap[greymap[pixels_median]]\n",
    "        \n",
    "    return (pixels_median, shape, code, uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply that function to each row of the dataframe by iterating over pairs of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "# The number of row pairs is half the number of rows\n",
    "num_pairs = int(len(clean_data_df)/2)\n",
    "\n",
    "for i in range(num_pairs):\n",
    "    \n",
    "    # Retrieve a pair of images \n",
    "    # from the datalog dataframe:\n",
    "    left_img, right_img = get_sensor_image_pair(roboSim.image_data(), i)\n",
    "    \n",
    "    # Decode the label image\n",
    "    (grey, shape, code, uniform) = decode_shape_label(right_img)\n",
    "    \n",
    "    # Add the label to a list of labels found so far\n",
    "    shapes.append(shape)\n",
    "\n",
    "    # Display the result of decoding\n",
    "    # the median pixel value\n",
    "    print(f\"Grey: {grey}; shape: {shape}; code: {code}; uniform: {uniform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `decode_shape_label()` function as part of another function that will return a shape training image and its associated label from a left and right sensor row pair in the datalog dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(raw_df, pair_index):\n",
    "    \"\"\"Get training image and label from raw dataframe.\"\"\"\n",
    "    \n",
    "    # Get the left and right images\n",
    "    # at specified pair index\n",
    "    left_img, right_img = get_sensor_image_pair(raw_df,\n",
    "                                            pair_index)\n",
    "    response = decode_shape_label(right_img)\n",
    "    (grey, shape, code, uniform) = response\n",
    "    return (shape, code, uniform, left_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `get_training_data()` function, we pass it the datalog dataframe and the index of the desired image pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_index = -1\n",
    "\n",
    "# Get the response tuple as a single variable\n",
    "response = get_training_data(clean_data_df, pair_index)\n",
    "\n",
    "# Then unpack the tuple\n",
    "(shape, code, uniform, training_img) = response\n",
    "\n",
    "print(shape, code, uniform)\n",
    "zoom_img(training_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we can now:\n",
    "    \n",
    "- grab the greyscale training image\n",
    "- find the median greyscale value\n",
    "- try to decode that value to a shape label / code\n",
    "- return the shape label and code associated with that greyscale image, along with an indicator of whether the image is in view via the `uniform` training image array flag\n",
    "- label the corresponding shape image with the appropriate label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Real-time data collection\n",
    "\n",
    "In this section, you will start to explore how to collect data in real time as the robot drives over the images, rather than being teleported directly on top of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Identifying when the robot is over a pattern in real time\n",
    "\n",
    "If we want to collect data from the robot as it drives slowly over the images, then we need to be able to identify when it is passing over the images so we can trigger the image sampling.\n",
    "\n",
    "The following program will slowly drive over the test patterns, logging the reflected light sensor values every so often. Start the program using the simulator *Run* button or the simulator `R` keyboard shortcut.\n",
    "\n",
    "From the traces on the simulator chart, can you identify when the robot passes over the images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b Simple_Shapes -x 100 -y 900 -OAc\n",
    "\n",
    "say(\"On my way..\")\n",
    "\n",
    "# Start driving forwards slowly\n",
    "tank_drive.on(SpeedPercent(10), SpeedPercent(10))\n",
    "\n",
    "count = 1\n",
    "\n",
    "# Drive forward no further than a specified distance\n",
    "while int(tank_drive.left_motor.position)<1500:\n",
    "    \n",
    "    left_light = colorLeft.reflected_light_intensity_pc\n",
    "    right_light = colorRight.reflected_light_intensity_pc\n",
    "    \n",
    "    # report every fifth pass of the loop\n",
    "    if not (count % 5):\n",
    "        print('Light_left: ' + str(left_light))\n",
    "        print('Light_right: ' + str(right_light))\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "say('All done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Based on your observations, describe a strategy you might use to capture image sample data when the test images are largely in view.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 4.2.2 Optional challenge — capturing image data in real time\n",
    "\n",
    "Using your observations regarding the reflected light sensor values as the robot crosses the images, or otherwise, write a program to collect image data from the simulator in real time as the robot drives over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Describe your program strategy and record your program design notes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Capturing image data in real time\n",
    "\n",
    "By observation of the reflected light sensor data in the chart, the robot appears to be over a shape, as the reflected light sensor values drop below about 85%.\n",
    "\n",
    "From the chart, we might also notice that the training label image (encoded as the solid grey square presented to the right-hand sensor) gives distinct readings for each shape.\n",
    "\n",
    "We can therefore use a drop in the reflected light sensor value to trigger the collection of the image data.\n",
    "\n",
    "First, let's clear the datalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the datalog to give us a fresh start\n",
    "roboSim.clear_datalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a program to drive the robot forwards slowly and collect the image data when it is over an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b Simple_Shapes -x 100 -y 900 -OAR\n",
    "\n",
    "say(\"Getting started.\")\n",
    "    \n",
    "# Start driving forwards slowly\n",
    "tank_drive.on(SpeedPercent(10), SpeedPercent(10))\n",
    "\n",
    "# Drive forward no futher than a specified distance\n",
    "while int(tank_drive.left_motor.position)<1200:\n",
    "    \n",
    "    # Sample the right sensor\n",
    "    sample = colorRight.reflected_light_intensity_pc\n",
    "    # If we seem to be over a test label,\n",
    "    # grab the image data into the datalof\n",
    "    if sample < 85:\n",
    "        print(\"image_data both\")\n",
    "\n",
    "say(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we review the images in the datalog, we should see they all contain at least a fragment of the image data (this may take a few moments to run). The following code cell grabs images where the `uniform` flag is set on the encoded label image and adds those training samples to a list (`training_images`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = []\n",
    "\n",
    "for i in trange(int(len(roboSim.image_data())/2)):\n",
    "    \n",
    "    response = get_training_data(roboSim.image_data(), i)\n",
    "    \n",
    "    (shape, code, uniform, training_img) = response\n",
    "    \n",
    "    # Likely shape\n",
    "    if uniform:\n",
    "        display(shape, training_img)\n",
    "        training_images.append((shape, code, training_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your own observations here about how \"clean\" the captured training images are.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cast the list of training images in the convenient form of a *pandas* dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame(training_images,\n",
    "                           columns=['shape', 'code', 'image'])\n",
    "\n",
    "training_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get training image and training label lists as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_df['image'].to_list()\n",
    "training_labels = training_df['code'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to try to use the data collected by travelling over the test track to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Training an MLP to recognise the patterns\n",
    "\n",
    "In an earlier activity, we discovered that the bounding box method we used to distinguish fruits did not provide a set of features that we could use to distinguish the different shapes.\n",
    "\n",
    "So let's just use a \"naive\" training approach and just train the network on the 14 × 14 pixels in the centre of each sensor image array.\n",
    "\n",
    "We can use the `quick_progress_tracked_training()` function we used previously to train an MLP using the scanned image shapes. We can optionally use the `jiggled=True` parameter to add some variation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import quick_progress_tracked_training\n",
    "\n",
    "\n",
    "# Specify some parameters\n",
    "hidden_layer_sizes = (40)\n",
    "max_iterations = 500\n",
    "\n",
    "\n",
    "# Create a new MLP\n",
    "MLP = quick_progress_tracked_training(training_images, training_labels,\n",
    "                                      hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                      max_iterations=max_iterations,\n",
    "                                      report=True,\n",
    "                                      jiggled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following code cell to randomly select images from the training samples and test the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import predict_and_report_from_image\n",
    "import random\n",
    "\n",
    "sample = random.randint(0, len(training_images))\n",
    "test_image = training_images[sample]\n",
    "test_label = training_labels[sample]\n",
    "\n",
    "predict_and_report_from_image(MLP, test_image, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations about how well the network performs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Testing the network on a new set of collected data\n",
    "\n",
    "Let's collect some data again by driving the robot over a second, slightly shorter test track at `y=700` to see if we can recognise the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no encoded training label images in this track, so we will either have to rely on just the reflected light sensor value to capture legitimate images for us, or we will need to pre-process the images to discard ones that are only partial image captures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Collecting the test data\n",
    "\n",
    "The following program will stop as soon as the reflected light value from the left sensor drops below 85. How much of the image can we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b Simple_Shapes -x 100 -y 700 -OAR\n",
    "\n",
    "say('Starting')\n",
    "# Start driving forwards slowly\n",
    "tank_drive.on(SpeedPercent(5), SpeedPercent(5))\n",
    "\n",
    "# Sample the left sensor\n",
    "sample = colorLeft.reflected_light_intensity_pc\n",
    "    \n",
    "# Drive forward no futher than a specified distance\n",
    "while sample>85:\n",
    "    sample = colorLeft.reflected_light_intensity_pc\n",
    "\n",
    "say(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's perhaps a bit optimistic for a sensible attempt at image recognition.\n",
    "\n",
    "However, recalling that the black pixel count for the training images ranged from 49 for the square to 60 for one of the equilateral triangles, we could tag an image as likely to contain a potentially recognisable image if its black pixel count exceeds 45.\n",
    "\n",
    "To give us some data to work with, let's collect samples for the new test set at `y=700`. First clear the datalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the datalog to give us a fresh start\n",
    "roboSim.clear_datalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then grab the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b Simple_Shapes -x 100 -y 700 -OAR\n",
    "\n",
    "say(\"Starting\")\n",
    "\n",
    "# Start driving forwards slowly\n",
    "tank_drive.on(SpeedPercent(5), SpeedPercent(5))\n",
    "\n",
    "# Drive forward no futher than a specified distance\n",
    "while int(tank_drive.left_motor.position)<800:\n",
    "    \n",
    "    # Sample the right sensor\n",
    "    sample = colorLeft.reflected_light_intensity_pc\n",
    "    # If we seem to be over a test label,\n",
    "    # grab the image data into the datalog\n",
    "    if sample < 85:\n",
    "        print(\"image_data both\")\n",
    "\n",
    "say(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Generating the test set\n",
    "\n",
    "We can now generate a clean test set of images based on a minimum required number of black pixels. The following function grabs the test images and also counts the black pixels in the left image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(raw_df, pair_index):\n",
    "    \"\"\"Get test image and label from raw dataframe.\"\"\"\n",
    "    \n",
    "    # Get the left and right images\n",
    "    # at specified pair index\n",
    "    left_img, right_img = get_sensor_image_pair(raw_df,\n",
    "                                            pair_index)\n",
    "    \n",
    "    # Get the pixel count\n",
    "    left_pixel_cnt = pd.Series(list(left_img.getdata())).value_counts()\n",
    "    count = left_pixel_cnt[0] if 0 in left_pixel_cnt else 0\n",
    "    \n",
    "    return (count, left_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a filtered list of potentially recognisable images. You may recall seeing a similarly structured code fragment previously when we used the `uniform` flag to select the images. However, in this case, we only save an image to a list if we see the black pixel count decreasing.\n",
    "\n",
    "Having got a candidate image, the `crop_and_pad_to_fit()` function crops it and tries to place it in the centre of the image array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import crop_and_pad_to_fit\n",
    "\n",
    "test_images = []\n",
    "possible_img = None\n",
    "possible_count = 0\n",
    "\n",
    "for i in trange(int(len(roboSim.image_data())/2)):\n",
    "    (count, left_img) = get_test_data(roboSim.image_data(), i)\n",
    "    # On the way in to a shape, we have\n",
    "    # an increasing black pixel count\n",
    "    if count and count >= possible_count:\n",
    "        possible_img = left_img\n",
    "        possible_count = count\n",
    "    # We're perhaps now on the way out...\n",
    "    # Do we have a possible shape?\n",
    "    elif possible_img is not None and possible_count > 45:\n",
    "        display(possible_count, left_img)\n",
    "        print('---')\n",
    "        possible_img = crop_and_pad_to_fit(possible_img)\n",
    "        test_images.append(possible_img)\n",
    "        possible_img = None\n",
    "    # We have now gone past the image\n",
    "    elif count < 35:\n",
    "        possible_count = 0\n",
    "        \n",
    "test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Testing the data\n",
    "\n",
    "Having got our images, we can now try to test them with the MLP.\n",
    "\n",
    "Recall that the `codemap` dictionary maps from code values to shape name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import class_predict_from_image\n",
    "\n",
    "# Random sample from the test images\n",
    "sample = random.randint(0, len(test_images)-1)\n",
    "\n",
    "test_img = test_images[sample]\n",
    "\n",
    "prediction = class_predict_from_image(MLP, test_img)\n",
    "\n",
    "# How did we do?\n",
    "display(codemap[prediction])\n",
    "zoom_img(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Save the MLP\n",
    "\n",
    "Save the MLP so we can use it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(MLP, 'mlp_shapes_14x14.joblib') \n",
    "\n",
    "# Load it back\n",
    "#from joblib import load\n",
    "\n",
    "#MLP = load('mlp_shapes_14x14.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Summary\n",
    "\n",
    "In this notebook, you have seen how we can collect data in real time from the simulator by sampling images when the robot detects a change in the reflected light levels.\n",
    "\n",
    "Using a special test track, with paired shape and encoded label images, we were able to collect a set of shape-based training patterns that could be used to train an MLP to recognise the shapes.\n",
    "\n",
    "Investigation of the shape images revealed that simple black pixel counts and bounding box dimensions did not distinguish between the shapes, so we simply trained the network on the raw images.\n",
    "\n",
    "Running the robot over a test track without any paired encoded label images, we were still able to detect when the robot was over the image based on the black pixel count of the shape image. On testing the MLP  against newly collected shape data, the neural network was able to correctly classify the collected patterns.\n",
    "\n",
    "In the next notebook, you will explore how the robot may be able to identify the shapes in real time as part of a multi-agent system working in partnership with  a pattern-recognising agent running in the notebook Python environment."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "formats": "ipynb,.md//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
