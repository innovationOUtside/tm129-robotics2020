{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collecting digit image and class data from the simulator\n",
    "\n",
    "If we wanted to collect image data from the background and then train a network using those images, we would need to generate the training label somehow. We could do this manually, looking at each image and then by observation recording the digit value, associating it with the image location co-ordinates. But could we also encode the digit value explicitly somehow?\n",
    "\n",
    "If you look carefully at the *MNIST_Digits* background in the simulator, you will see that alongside each digit is a solid coloured area. This area is a greyscale value that represents the value of the digit represented by the image. That is, it represents a training label for the digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "*The greyscale encoding is quite a crude encoding method that is perhaps subject to noise. Another approach might be to use a simple QR code to encode the digit value.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, load in the simulator in the normal way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbev3devsim.load_nbev3devwidget import roboSim, eds\n",
    "\n",
    "%load_ext nbev3devsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear the datalog just to ensure we have a clean datalog to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sim_data --clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid greyscale areas are arranged so that when the left light sensor is over the image, the right sensor is over the training label area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b MNIST_Digits -O -R -AH -x 400 -y 50\n",
    "\n",
    "# Sample the light sensor reading\n",
    "sensor_value = colorLeft.reflected_light_intensity\n",
    "\n",
    "# This is essentially a command invocation\n",
    "# not just a print statement!\n",
    "print(\"image_data both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the last pair of images from the `roboSim.image_data()` dataframe using the `get_sensor_image_pair()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import zoom_img\n",
    "from nn_tools.sensor_data import get_sensor_image_pair\n",
    "\n",
    "# The sample pair we want from the logged image data\n",
    "pair_index = -1\n",
    "\n",
    "left_img, right_img = get_sensor_image_pair(roboSim.image_data(),\n",
    "                                            pair_index)\n",
    "\n",
    "zoom_img(left_img), zoom_img(right_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "The image labels are encoded as follows:\n",
    "\n",
    "`greyscale_value = 25 * digit_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of decoding the label is as follows:\n",
    "\n",
    "- divide each of the greyscale pixel values collected from the right-hand sensor array by 25\n",
    "- take the median of these values and round to the nearest integer; *in a noise-free environment, using the median should give a reasonable estimate of the dominant pixel value in the frame*\n",
    "- ensure we have an integer by casting the result to an integer.\n",
    "\n",
    "The *pandas* package has some operators that can help us with that if we put all the data into a *pandas* *Series* (essentially, a single-column dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_training_label_from_sensor(img):\n",
    "    \"\"\"Return a training class label from a sensor image.\"\"\"\n",
    "    # Get the pixels data as a pandas series\n",
    "    # (similar to a single column dataframe)\n",
    "    image_pixels = pd.Series(list(img.getdata()))\n",
    "\n",
    "    # Divide each value in the first column (name: 0) by 25\n",
    "    image_pixels = image_pixels / 25\n",
    "\n",
    "    # Find the median value\n",
    "    pixels_median = image_pixels.median()\n",
    "\n",
    "    # Find the nearest integer and return it\n",
    "    return int( pixels_median.round(0))\n",
    "\n",
    "# Try it out\n",
    "get_training_label_from_sensor(right_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will grab the right and left images from the datalog, decode the label from the right-hand image, and return the handwritten digit from the left light sensor along with the training label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(raw_df, pair_index):\n",
    "    \"\"\"Get training image and label from raw dataframe.\"\"\"\n",
    "    \n",
    "    # Get the left and right images\n",
    "    # at specified pair index\n",
    "    left_img, right_img = get_sensor_image_pair(raw_df,\n",
    "                                            pair_index)\n",
    "    \n",
    "    # Find the training label value as the median\n",
    "    # value of the right habd image.\n",
    "    # Really, we should probably try to check that\n",
    "    # we do have a proper training image, for example\n",
    "    # by encoding a recognisable pattern \n",
    "    # such as a QR code\n",
    "    training_label = get_training_label_from_sensor(right_img)\n",
    "    return training_label, left_img\n",
    "    \n",
    "\n",
    "# Try it out\n",
    "label, img = get_training_data(roboSim.image_data(),\n",
    "                               pair_index)\n",
    "print(f'Label: {label}')\n",
    "zoom_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-danger"
    ]
   },
   "source": [
    "We're actually taking quite a lot on trust in extracting the data from the dataframe in this way. Ideally, we would have unique identifiers that reliably associate the left and right images as having been sampled from the same location. As it is, we assume the left and right image datasets appear in that order, one after the other, so we can count back up the dataframe to collect different pairs of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in our previously trained MLP classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from joblib import load\n",
    "\n",
    "MLP = load('mlp_mnist14x14.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test that image against the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import image_class_predictor\n",
    "\n",
    "image_class_predictor(MLP, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 2.3.1 Activity — Testing the ability to recognise images slight off-centre in the image array\n",
    "\n",
    "Write a simple program to collect sample data at a particular location and then display the digit image and the decoded label value.\n",
    "\n",
    "Modify the *x* or *y*-co-ordinates used to locate the robot by a few pixel values away from the sampling point origins and test the ability of the network to recognise digits that are lightly off-centre in the image array.\n",
    "\n",
    "How well does the network perform?\n",
    "\n",
    "*Hint: when you have run your program to collect the data in the simulator, run the `get_training_data()` function with the `roboSim.image_data()` to generate the test image and retrieve its decoded training label.*\n",
    "\n",
    "*Hint: use the `image_class_predictor()` function with the test image to see if the classifier can recognise the image.*\n",
    "\n",
    "*Hint: if you seem to have more data in the dataframe than you thought you had collected, did you remember to clear the datalog before collecting your data?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 2.3.2 Optional activity — Collecting image sample data from the *MNIST_Digits* background\n",
    "\n",
    "In this activity, you will need to collect a complete set of sample data from the simulator to test the ability of the network to correctly identify the handwritten digit images.\n",
    "\n",
    "Recall that the sampling positions are arranged along rows 100 pixels apart, starting at *x=100* and ending at *x=2000*;\n",
    "along columns 100 pixels apart, starting at *y=50* and ending at *y=1050*.\n",
    "\n",
    "Write a program to automate the collection of data at each of these locations.\n",
    "\n",
    "How would you then retrieve the handwritten digit image and it's decoded training label?\n",
    "\n",
    "*Hint: import the `time` package and use the `time.sleep` function to provide a short delay between each sample collection. You may also find it convenient to import the `trange` function to provide a progress bar indicator when iterating through the list of collection locations: `from tqdm.notebook import trange`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Your program design notes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your program code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Describe here how you would retrieve the handwritten digit image and it's decoded training label.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "heading_collapsed": true
   },
   "source": [
    "#### Example solution\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal an example solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "To collect the data, I use two `range()` commands, one inside the other, to iterate through the *x* and *y*-coordinate values. The outer loop generates the *x* values and the inner loop generates the *y*-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make use of the progress bar indicated range\n",
    "from tqdm.notebook import trange\n",
    "import time\n",
    "\n",
    "# Clear the datalog so we know it's empty\n",
    "%sim_data --clear\n",
    "\n",
    "\n",
    "# Generate a list of integers with desired range and gap\n",
    "min_value = 50\n",
    "max_value = 1050\n",
    "step = 100\n",
    "\n",
    "for _x in trange(100, 501, 100):\n",
    "    for _y in range(min_value, max_value+1, step):\n",
    "\n",
    "        %sim_magic -R -x $_x -y $_y\n",
    "        # Give the data time to synchronise\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "We can now grab and view the data we have collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_df = roboSim.image_data()\n",
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "The `get_training_data()` function provides a convenient way of retrieving the handwritten digit image and the decoded training label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label, img = get_training_data(training_df, pair_index)\n",
    "zoom_img(img), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Summary\n",
    "\n",
    "In this notebook, you have automated the collection of hand-written digit and encoded label image data from the simulator and seen how this can be used to generate training data made up of scanned handwritten digit and image label pairs. In principle, we could use the image and test label data collected in this way as a training data set for an MLP or convolutional neural network.\n",
    "\n",
    "The next notebook in the series is optional and demonstrates the performance of a CNN on the MNIST dataset. The required content continues with a look at how we can start to collect image data using the simulated robot whilst it is on the move."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,.md//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
