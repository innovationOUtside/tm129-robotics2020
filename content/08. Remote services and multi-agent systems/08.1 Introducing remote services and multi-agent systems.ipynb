{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 An introduction to remote services and multi-agent systems\n",
    "\n",
    "In the previous session, you had an opportunity to experiment hands-on with some neural networks. You may have finished that lab session wondering how neural networks can be built into real robots, particularly low-cost robots rather than expensive cutting-edge robots found only in research labs. In this session, you will find out.\n",
    "\n",
    "The robot simulator we're using was originally designed to simulate the behaviour of a Lego Mindstorms EV3 controlled robot. The EV3 brick is excellent for introductory robotics, but it has limitations in terms of processor speed and memory capacity. This makes it impractical to train anything other than a small neural network on a Lego EV3 robot, although we may be able to use pre-trained models to perform \"on-device\" classification tasks.\n",
    "\n",
    "In general, we are often faced with the problem that we may want to run powerful programs on low-cost hardware that really isn't up to the job. Upgrading a robot with a more powerful processor might not be a solution because it adds cost and may demand extra electrical power or create heat management issues: driving a processor hard can generate a lot of heat, and you need to remove that heat somehow. Heatsinks are heavy and take up physical space, and cooling fans are heavy, take up space, and need access to power. As you add more mass, you need more powerful motors, which themselves add mass and require more power. Bigger batteries add more mass, so you can see where this argument leads...\n",
    "\n",
    "A possible alternative is to think about using remote services or *multi-agent* systems approaches. In either case, we might use a low-cost robot as a mobile agent to gather data and send that back to a more powerful computer for processing.\n",
    "\n",
    "In the first case, we might think of the robot as a remote data collector, collecting data on our behalf and then returning that data to us in response to a request for it. In the RoboLab environment we might think of the notebook Python environment as our local computing environment and the robot as a remote service. Every so often, we might *pull* data from the robot by making a request for a copy of it from the notebook so that we can then analyse the data at our leisure. Alternatively, each time the robot collects a dataset, it might *push* a copy of the data to the notebook's Python environment. In each case, we might think of this as \"uploading\" data from the simulated robot back to the notebook.\n",
    "\n",
    "The model is a bit like asking a research librarian for some specific information, the research librarian researching the topic, perhaps using resources you don't have direct access to, and then the research librarian providing you with the information you requested.\n",
    "\n",
    "In a more dynamic multi-agent case we might consider the robot and the notebook environment to be acting as peers sending messages as and when they can between each other. For example, we might have two agents: a Lego mobile robot and a personal computer (PC), or the simulated robot and the notebook.  In computational terms, *agents* are long-lived computational systems that can deliberate on the actions they may take in pursuit of their own goals based on their own internal state (often referred to as \"beliefs\") and sensory inputs. Their actions are then performed by means of some sort of effector system that can act on to change the state of the environment within which they reside.\n",
    "\n",
    "In a multi-agent system, two or more agents may work together to combine to perform some task that not only meets the (sub)goals of each individual agent, but that might also strive to attain some goal agreed upon by each member of the multi-agent system. Agents may communicate by making changes to the environment, for example, by leaving a trail that other agents may follow (an effect known as *stigmergy*), or by passing messages between themselves directly.\n",
    "\n",
    "To a limited extent, we may view our simulated robot / Python system as a model for a simple multi-agent system where two agents — the robot, and the classifying neural network or deliberative rule based system, for example — work together to perform the task classifying and identifying patterns in some environment that the individual agents could not achieve by themselves. We let the robot do what it does best – move around while logging data – and then it actively sends the data back to the Python for processing. The Python environment processes the data using a trained neural network, or perhaps a complex rule based system, and sends back a message to the robot giving an appropriate response. In each case, the two agents act independently, sending messages to the other party when they have data or a classification to share, rather than just responding to requests for data or a particular service as thy occur.\n",
    "\n",
    "In this session, we will explore how we might use the simulated robot as a remote data collector. Every so often, we will grab a copy of the logged data into the notebook Python environment and analyse it within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "*There is quite a lot of provided code in this week's notebooks. You are not necessarily expected to be able to create this sort of code yourself. Instead, try to focus on the process of how various tasks are broken down into smaller discrete steps, as well as how small code fragments can be combined to create \"higher level\" functions that perform ever more powerful tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Using a pre-trained MLP to categorise light sensor array data logged from the simulator\n",
    "\n",
    "The *MNIST_Digits* simulator background includes various digit images from the MNIST dataset, arranged in a grid.\n",
    "\n",
    "Alongside each digit is a grey square, where the grey level is used to encode the actual label associated with the image. (You can see how the background was created in the `Background Image Generator.ipynb` notebook in the top-level `backgrounds` folder.)\n",
    "\n",
    "In this notebook, you will use the light sensor as a simple low resolution camera, working with the pixel array data rather then the single value reflected light value.\n",
    "\n",
    "*Note that this functionality is not supported by the real Lego light sensor.*\n",
    "\n",
    "Let's start by loading in the simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbev3devsim.load_nbev3devwidget import roboSim, eds\n",
    "\n",
    "%load_ext nbev3devsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect the sensor image data, if the simulated robot program `print()` message starts with the word `image_data`, then we can send light sensor array data from the left, right or both light sensors to a data log in the notebook Python environment.\n",
    "\n",
    "The `-R` switch in magic at the start of the following code cell will run the program in the simulator once it has been downloaded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b MNIST_Digits -OA -R -x 400 -y 50\n",
    "\n",
    "# Configure a light sensor\n",
    "colorLeft = ColorSensor(INPUT_2)\n",
    "\n",
    "#Sample the light sensor reading\n",
    "sensor_value = colorLeft.reflected_light_intensity\n",
    "\n",
    "# This is a command invocation rather than a print statement\n",
    "print(\"image_data left\")\n",
    "# The command is responded to by\n",
    "# the \"Image data logged...\" message display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're going to be collecting data from the simulator into the notebook Python environment, we should take the precaution of clearing the notebook datalog before we start using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sim_data --clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Pushing the sensor array data log from the simulator to the notebook\n",
    "\n",
    "We can now run the data collection routine by calling a simple line magic that teleports the robot to a specific location, runs the data collection program (`-R`) and pushes the light sensor array data to the notebook Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sim_magic -RA -x 400 -y 850\n",
    "\n",
    "# Wait a moment to give data time to synchronise\n",
    "import time\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may need to wait a few moments for the program to execute and the data to be sent to the notebook Python environment.\n",
    "\n",
    "In the current example, the simulator is *pushing* the light sensor array data to the notebook each time the robot sends a particular message to the simulator output window.\n",
    "\n",
    "With the data pushed from the simulator to the notebook Python environment, we should be able to see a dataframe containing the retrieved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roboSim.image_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Previewing the sampled sensor array data (optional)\n",
    "\n",
    "Having grabbed the data, we can explore the data as rendered images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data representing the image is a long list of RGB (red green, blue) values. We can generate an image from a the a specific row of the dataframe, given it the row index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import generate_image, zoom_img\n",
    "index = -1 # Get the last image in the dataframe\n",
    "\n",
    "img = generate_image(roboSim.image_data(),\n",
    "                     index, mode='rgb')\n",
    "zoom_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "If you don't see a figure image displayed, check that the robot is placed over a figure by reviewing the sensor array display in the simulator. If the image is there, rerun the previous code cell to see if the data is now available. If it isn't, rerun the data collecting magic cell, wait a view seconds, and then try to view the zoomed image display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the color depth of the image by calling the `.getbands()` method on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.getbands()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might expect from the robot color sensor, this is a tri-band, RGB image.\n",
    "\n",
    "Alternatively, we can generate an image directly as a greyscale image, either by setting the mode explicitly or by omitting it (`mode=greyscale` is the default setting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = generate_image(roboSim.image_data(), index)\n",
    "zoom_img(img)\n",
    "\n",
    "img.getbands()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images we trained the network on were size 28 x 28 pixels. The raw images retrieved from the simulator sensor are slightly smaller, coming in at 20 x 20 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collected image also represents square profile around the \"circular\" sensor view. We might thus reasonably decide that we are going to focus our attention on the 14 x 14 square area in the centre of the collected image, with top left pixel `(3, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of using the Python `PIL` package is that a range of *methods* (that is, *functions*) are defined on each image object that allow us to manipulate it *as an image*. (We can then also access the data defining the transformed image *as data* if we need it in that format.)\n",
    "\n",
    "We can preview the area in our sampled image by cropping the image to the area of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = generate_image(roboSim.image_data(), index,\n",
    "                    crop=(3, 3, 17, 17))\n",
    "\n",
    "display(img.size)\n",
    "zoom_img( img )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "If required, we can resize the image by passing the desired size to the `generate_image()` function via the `resize` parameter, setting it either to a specified size, such as `resize=(28, 28)` (that is, 28 x 28 pixels) or back to the original, uncropped image size (`resize=('auto')`)\n",
    "\n",
    "```python\n",
    "\n",
    "img = generate_image(roboSim.image_data(), index,\n",
    "                    crop=(3, 3, 17, 17),\n",
    "                    resize = (28, 28))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Collecting multiple sample images\n",
    "\n",
    "The handwritten digit image sampling point locations in the *MINIST_Digits* simulator background can be found at the following locations:\n",
    "\n",
    "- along rows `100` pixels apart, starting at `x=100` and ending at `x=2000`;\n",
    "- along columns `100` pixels apart, starting at `y=50` and ending at `y=1050`.\n",
    "\n",
    "We can collect the samples collected over a column by using line magic to teleport the simulated robot to each new location in turn and automatically run the program to log the sensor data.\n",
    "\n",
    "To start, let's just check we can generate the required *y* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of integers with desired range and gap\n",
    "min_value = 50\n",
    "max_value = 1050\n",
    "step = 100\n",
    "\n",
    "list(range(min_value, max_value+1, step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this as a pattern, we can now create a simple script to clear the datalog, then iterate through the desired *y* locations, using line magic to locate the robot at each step and run the already downloaded image sampling program.\n",
    "\n",
    "To access the value of the iterated *y* value in the magic, we need to prefix it with a `$` when we refer to it. Note that we also use the `tqdm.notebook.trange` argument to define the range: this enhance the range iterator to provide an interactive progress bar that allows us to follow the progress of the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a progress bar when iterating through the range\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "# We need to add a short delay between iterations to give\n",
    "# the data time to synchronise\n",
    "import time\n",
    "\n",
    "# Clear the datalog so we know it's empty\n",
    "%sim_data --clear\n",
    "\n",
    "for _y in trange(min_value, max_value+1, step):\n",
    "    %sim_magic -R -x 100 -y $_y\n",
    "    # Give the data time to synchronise\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the collected samples via a *pandas* dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_df = roboSim.image_data()\n",
    "image_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access a centrally cropped black and white version of an image extracted from the retrieved data by index number  (`--index / -i`) by calling the `%sim_bw_image_data` magic, optionally setting the `--threshold / -t` value away from its default value of `127`. Using the `--nocrop / -n` flag will prevent the autocropping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  convert the image to a black and white image by setting pixels above a specified threshold value to white (`255`), otherwise coloring the pixel black (`0`) using the `generate_bw_image()` function. This will select a row from the datalog at a specific location, optionally crop it to a specific area, and then pixel values greater than threshold to white (`255`), with values equal to or below the threshold to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nn_tools.sensor_data import generate_bw_image\n",
    "\n",
    "index = -1\n",
    "xx = generate_bw_image(image_data_df,\n",
    "                       index,\n",
    "                       threshold=100,\n",
    "                       crop=(3, 3, 17, 17))\n",
    "\n",
    "zoom_img(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%sim_bw_image_data` magic performs a similar operation and can also retrieve a random image from the collected data using the `--random / -r` flag. (The crop limits are also assumed by the magic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_bw_image = %sim_bw_image_data --random --threshold 100\n",
    "zoom_img(cropped_bw_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "*End user applications are simple applications created by users themselves to simplify the performance of certain tasks. Such applications may be brittle and only work in specific situations or circumstances. The code may not be as elegant, well engineered or maintainable as \"production code\" used in applications made available to other users. One of the advantages of learning to code is the ability to create your own end-user applications.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1.1.4 Activity — Observing the effect of changing threshold value when converting the image from a greyscale to a black and white image (optional) \n",
    "\n",
    "Using the following end user application to observe the effects of setting different threshold values when creating the black and white binarised version of the image from the original greyscale image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import generate_bw_image\n",
    "from ipywidgets import interact_manual\n",
    "\n",
    "@interact_manual(threshold=(0, 255),\n",
    "                 index=(0, len(image_data_df)-1))\n",
    "def bw_preview(index=0, threshold=200,\n",
    "               crop=False):\n",
    "    # Optionally crop to the centre of the image\n",
    "    _crop = (3, 3, 17, 17) if crop else None\n",
    "    _original_img = generate_image(image_data_df, index)\n",
    "    \n",
    "    # Generate a black and white image\n",
    "    _demo_img = generate_bw_image(image_data_df, index,\n",
    "                                  threshold=threshold,\n",
    "                                  crop=_crop)\n",
    "    # %sim_bw_image_data --index -1 --threshold 100 --crop 3.3,17,17\n",
    "    zoom_img( _original_img)\n",
    "    zoom_img( _demo_img )\n",
    "\n",
    "    # Preview the actual sized image\n",
    "    # display(_original_img, _demo_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "*The `sensor_image_focus()` function is another convenience function for returning the image in the centre of the sensor array.*\n",
    "\n",
    "```python\n",
    "from nn_tools.sensor_data import sensor_image_focus\n",
    "\n",
    "original_image = generate_image(image_data_df, index)\n",
    "focal_image = sensor_image_focus( original_image )\n",
    "zoom_img( focal_image )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Testing the robot sample images using a pre-retrained MLP\n",
    "\n",
    "Having grabbed the image data, we can pre-process it as required and then present it to an appropriately trained neural network to see if the network can identify the digit it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Loading in a previously saved MLP model\n",
    "\n",
    "Rather than train a new model, we can load in an MLP we have trained previously. Remember, when using a neural network model, we need to make sure that we know how many inputs it expects, which in our case matches the size of presented images.\n",
    "\n",
    "You can either use the pre-trained model that is provided in the same directory as this notebook (`mlp_mnist.joblib`), or use your own model created in an earlier notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from joblib import load\n",
    "\n",
    "MLP = load('mlp_mnist14x14.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the configuration of the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import network_structure\n",
    "network_structure(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 196 input features correspond to an input grid of 14x14 pixels.\n",
    "\n",
    "*For a square array, we get the side length as the square root of the number of features.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Using the pre-trained classifier to recognise sampled images\n",
    "\n",
    "What happens if we now try to recognise images sampled from the simulator light sensor array using our previously trained MLP classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nn_tools.network_views import image_class_predictor\n",
    "\n",
    "# Get a random image index value\n",
    "index = random.randint(0, len(image_data_df)-1)\n",
    "        \n",
    "# Generate the test image as a black and white image\n",
    "test_image = generate_bw_image(image_data_df, index,\n",
    "                               threshold=127,\n",
    "                               crop=(3, 3, 17, 17))\n",
    "\n",
    "# Display a zoomed version of the test image\n",
    "zoom_img(test_image)\n",
    "\n",
    "# Print the class prediction report\n",
    "image_class_predictor(MLP, test_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image2 = %sim_bw_image_data --index -1 \n",
    "# Display a zoomed version of the test image\n",
    "zoom_img(test_image2)\n",
    "\n",
    "# Print the class prediction report\n",
    "image_class_predictor(MLP, test_image2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did the classifier perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Make your own notes and observations about the MLP's performance here. If anything strikes you as unusual, why do you think the MLP is performing the way it is?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a simple interactive application to test the other images more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(threshold=(0, 255),\n",
    "                 index=(0, len(image_data_df)-1))\n",
    "def test_image(index=0, threshold=200, show_image=True):\n",
    "    # Create the test image\n",
    "    test_image = generate_bw_image(image_data_df, index, \n",
    "                                   threshold=threshold,\n",
    "                                   crop=(3, 3, 17, 17))\n",
    "    \n",
    "    # Generate class prediction chart\n",
    "    image_class_predictor(MLP, test_image)\n",
    "    \n",
    "    if show_image:\n",
    "        zoom_img(test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, how well does the classifier appear to perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your own notes and observations about the behaviour and performance of the MLP here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 1.2.3 Activity — Collecting image sample data at a specific location\n",
    "\n",
    "Write a simple line magic command to collect the image data for the handwritten digit centred on the location `(600, 750)`.\n",
    "\n",
    "Note that you may need to wait a short time between running the data collection program and trying to view it.\n",
    "\n",
    "Display a zoomed version of the image in the notebook. By observation, what digit does it represent?\n",
    "\n",
    "Using the `image_class_predictor()` function, how does the trained MLP classify the image? Does this match your observation?\n",
    "\n",
    "Increase the light sensor noise in the simulator to its maximum value and collect and test the data again. How well does the network perform this time?\n",
    "\n",
    "\n",
    "*Hint: data is collected into a dataframe returned by calling `roboSim.image_data()`.*\n",
    "\n",
    "*Hint: remember that you need to crop the image to a 14x14 array.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your image sampling code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your image viewing code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*From your own observation, record which digit is represented by the image here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# How does the trained MLP classify the image?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*How well does the prediction match your observation? Is the MLP confident in its prediction?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "Increase the level of light sensor noise to it's maximum value and rerun the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Collect data with noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Preview image with noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Classify image with noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Add your own notes and observations on how well the network performed the classification task in the presence of sensor noise here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Example discussion\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal an example discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "We can collect the image data by calling the `%sim_magic` with the `-R` switch so that it runs the current program directly. We also need to set the location using the `-x` and `-y` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "%sim_magic -R -x 600 -y 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "The data is available in a dataframe returned by calling `roboSim.image_data()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "To view the result, we can zoom the display of the last collected image in the notebook synched datalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "# Get data for the last image in the dataframe\n",
    "index = -1 \n",
    "my_img = generate_bw_image(roboSim.image_data(), index,\n",
    "                        crop=(3, 3, 17, 17))\n",
    "zoom_img(my_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "By my observation, the digit represented by the image at the specified location is a figure `3`.\n",
    "\n",
    "The trained MLP classifies the object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "image_class_predictor(MLP, my_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "This appears to match my prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Collecting digit image and class data from the simulator\n",
    "\n",
    "If you look carefully at the *MNIST_Digits* background in the simulator, you will see that alongside each digit is a solid coloured area. This area is a greyscale value that represents the value of the digit represented by the image. That is, it represents a training label for the digit.\n",
    "\n",
    "Before we proceed, clear out the datalog to give ourselves a clean datalog to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sim_data --clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid coloured areas are arranged so that when the left light sensor is over the image, the right sensor is over the training label area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sim_magic_preloaded -b MNIST_Digits -O -R -AH -x 400 -y 50\n",
    "\n",
    "#Sample the light sensor reading\n",
    "sensor_value = colorLeft.reflected_light_intensity\n",
    "\n",
    "# This is essentially a command invocation\n",
    "# not just a print statement!\n",
    "print(\"image_data both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the last pair of images from the `roboSim.image_data()` dataframe using the `get_sensor_image_pair()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import get_sensor_image_pair\n",
    "\n",
    "# The sample pair we want from the logged image data\n",
    "pair_index = -1\n",
    "\n",
    "left_img, right_img = get_sensor_image_pair(roboSim.image_data(),\n",
    "                                            pair_index)\n",
    "\n",
    "zoom_img(left_img), zoom_img(right_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "The image labels are encoded as follows:\n",
    "\n",
    "`greyscale_value = 25 * digit_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of decoding the label is as follows:\n",
    "\n",
    "- divide each of the greyscale pixel values collected from the right hand sensor array by 25;\n",
    "- take the median of these values and round to the nearest integer; *in a noise free environment, using the median should give a reasonable estimate of the dominant pixel value in the frame.*\n",
    "- ensure we have an integer by casting the result to an integer.\n",
    "\n",
    "The *pandas* package has some operators that can help us with that if we put all the data into a *pandas* *Series* (essentially, a single column dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_training_label_from_sensor(img):\n",
    "    \"\"\"Return a training class label from a sensor image.\"\"\"\n",
    "    # Get the pixels data as a pandas series\n",
    "    # (similar to a single column dataframe)\n",
    "    image_pixels = pd.Series(list(img.getdata()))\n",
    "\n",
    "    # Divide each value in the first column (name: 0) by 25\n",
    "    image_pixels = image_pixels / 25\n",
    "\n",
    "    # Find the median value\n",
    "    pixels_median = image_pixels.median()\n",
    "\n",
    "    # Find the nearest integer and return it\n",
    "    return int( pixels_median.round(0))\n",
    "\n",
    "# Try it out\n",
    "get_training_label_from_sensor(right_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will grab right and left image from the data log, decode the label from the right hand image, and return the handwritten digit from the left light sensor along with the training label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(raw_df, pair_index):\n",
    "    \"\"\"Get training image and label from raw data frame.\"\"\"\n",
    "    \n",
    "    # Get the left and right images\n",
    "    # at specified pair index\n",
    "    left_img, right_img = get_sensor_image_pair(raw_df,\n",
    "                                            pair_index)\n",
    "    \n",
    "    # Find the training label value as the median\n",
    "    # value of the right habd image.\n",
    "    # Really, we should properly try to check that\n",
    "    # we do have a proper training image, for example\n",
    "    # by encoding a recognisable pattern \n",
    "    # such as a QR code\n",
    "    training_label = get_training_label_from_sensor(right_img)\n",
    "    return training_label, left_img\n",
    "    \n",
    "\n",
    "# Try it out\n",
    "label, img = get_training_data(roboSim.image_data(),\n",
    "                               pair_index)\n",
    "print(f'Label: {label}')\n",
    "zoom_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-danger"
    ]
   },
   "source": [
    "We're actually taking quite a lot on trust in extracting the data from the dataframe in this way. Ideally, we would have a unique identifiers that reliably associate the left and right images as having been sampled from the same location. As it is, we assume the left and right image datasets appear in that order, one after the other, so we can count back up the dataframe to collect different pairs of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test that image against the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_class_predictor(MLP, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 1.3.1 Activity — Testing the ability to recognise images slight off-center in the image array\n",
    "\n",
    "Write a simple program to collect sample data at a particular location and then display the digit image and the decoded label value.\n",
    "\n",
    "Modify the x or y co-ordinates used to locate the robot by by a few pixel values away from the sampling point origins and test the ability of the network to recognise digits that are lightly off-center in the image array.\n",
    "\n",
    "How well does the network perform?\n",
    "\n",
    "*Hint: when you have run your program to collect the data in the simulator, run the `get_training_data()` with the `roboSim.image_data()` to generate the test image and retrieve its decoded training label.*\n",
    "\n",
    "*Hint: use the `image_class_predictor()` function with the test image to see if the classifier can recognise the image.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 1.3.2 Activity — Collecting image sample data from the *MNIST_Digits* background (optional)\n",
    "\n",
    "In this activity, you will need to collect a complete set of sample data from the simulator to test the ability of the network to correctly identify the handwritten digit images.\n",
    "\n",
    "Recall that the sampling positions are arranged along rows 100 pixels apart, starting at x=100 and ending at x=2000;\n",
    "along columns 100 pixels apart, starting at y=50 and ending at y=1050.\n",
    "\n",
    "Write a program to automate the collection of data at each of these locations.\n",
    "\n",
    "How would you then retrieve the hand written digit image and it's decoded training label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Your program design notes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your program code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Describe here how you would retrieve the hand written digit image and it's decoded training label.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Example solution\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal an example solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "To collect the data, I use two `range()` commands, one inside the other, to iterate through the *x* and *y* coordinate values. The outer loop generates the *x* values and the inner loop generates the *y* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "# Clear the datalog so we know it's empty\n",
    "%sim_data --clear\n",
    "\n",
    "\n",
    "# Generate a list of integers with desired range and gap\n",
    "min_value = 50\n",
    "max_value = 1050\n",
    "step = 100\n",
    "\n",
    "for _x in trange(100, 501, 100):\n",
    "    for _y in range(min_value, max_value+1, step):\n",
    "\n",
    "        %sim_magic -R -x $_x -y $_y\n",
    "        # Give the data time to synchronise\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "We can now grab view the data we have collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "training_df = roboSim.image_data()\n",
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "The `get_training_data()` function provides a convenient way of retrieving the handwritten digit image and the decoded training label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "label, img = get_training_data(training_df, pair_index)\n",
    "zoom_img(img), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Summary\n",
    "\n",
    "In this notebook, you have seen how we can use the robot's light sensor as a simple low resolution camera to sample handwritten digit images from the background. Collecting the data from the robot, we can then convert it to an image and preprocess is before testing it with a pre-trained multi-layer perceptron.\n",
    "\n",
    "Using captured images that are slightly offset from the center of the image array  essentially provides us with a \"jiggled\" image, which tends to increase the classification error.\n",
    "\n",
    "You have also seen how we might automate the collection of large amounts of data by \"teleporting\" the robot to particular locations and sampling the data. With the background defined as it is, we can also pick up encoded label data an use this to generate training data made up of scanned handwritten digit and image label pairs. In principle, we could use the image and test label data collected in this way as a training data set for an MLP or convolutional neural network.\n",
    "\n",
    "The next notebook in the series is optional and demonstrates the performance of a CNN on the MNIST dataset. The required content continues with a look at how we can start to collect image data using the simulated robot whilst it is on the move."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,.md//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
