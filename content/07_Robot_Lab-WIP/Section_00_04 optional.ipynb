{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?? lot of optional?\n",
    "\n",
    "### Creating synthetic data — translating an original image\n",
    "\n",
    "We have already seen how we can get rid of the \"background\" columns and rows around the outside of an image using the `nn_tools.sensor_data.trim_image()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the parameter show=False to hide the display\n",
    "# of the untrimmed and trimmed dataframes\n",
    "trimmed_image_df = trim_image( test_image, background=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert this dataframe back to an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import image_from_df\n",
    "\n",
    "cropped_image = image_from_df(trimmed_image_df)\n",
    "zoom_img( cropped_image )\n",
    "\n",
    "cropped_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create a blank image of size 28 x 28 pixels with a grey background, we can paste a copy of our cropped image into it. The `nn_tools.sensor_data.jiggle()` function implements this approach. It will accept an image and then return randomly translated version of it.\n",
    "\n",
    "Run the following cell several times to see the effect of the `jiggle()` function on a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import jiggle\n",
    "\n",
    "jiggled_image =  jiggle(test_image)\n",
    "zoom_img(jiggled_image)\n",
    "\n",
    "# Show size\n",
    "jiggled_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of transforming the cropped images is to magnify it back to the original image size. (The rescaling employs a digital filter that is used to interpolate new pixel values in the scaled image based on the pixel values in the cropped image. As such, it may introduce digital artefacts of its own into the scaled image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import crop_and_zoom_to_fit\n",
    "\n",
    "crop_zoomed_image = crop_and_zoom_to_fit(test_image)\n",
    "\n",
    "zoom_img( crop_zoomed_image )\n",
    "\n",
    "# Show size\n",
    "crop_zoomed_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process of applying these transformations to an a test image, we can call the `predict_and_report_from_image()` with the `jiggled=True` and `cropzoom=True` parameters:\n",
    "\n",
    "```python\n",
    "# Test a jiggled version of the provided image\n",
    "predict_and_report_from_image(MLP, test_image, test_label,\n",
    "                              jiggled=True, quiet=False)\n",
    "\n",
    "# Test a cropped and then zoomed version of the provided image\n",
    "predict_and_report_from_image(MLP,\n",
    "                              test_image, test_label,\n",
    "                              cropzoom=True, quiet=False)\n",
    "```\n",
    "\n",
    "You can also pass the `zoomview=True` parameter to view the image at a larger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "## Activity — translating the digit within the image frame\n",
    "\n",
    "Explore how the `sensor_data.jiggle()` function works in practice. Run the following cell multiple times, observing what happens in each case, to see how the a differently translated versions of the image are returned each time the function is called. Is there much variation in how the digit is centered in the image frame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import jiggle\n",
    "\n",
    "# Setting quiet=False displays the original input image\n",
    "# as well as returning the jiggled image as cell output\n",
    "zoom_img( jiggle(image_image, quiet=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record any notes or observations here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal my observations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "The `jiggle` function slightly translates the image to the left, right, and up and down within the image area. However, it nevers seems to be translated so far that bits of it get cut off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "## Activity — Testing the MLP against translated digit images\n",
    "\n",
    "Now let's see how well our trained MLP responds to translated versions of the original training images.\n",
    "\n",
    "Start by testing the network against one of the original images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from nn_tools.network_views import predict_and_report_from_image\n",
    "\n",
    "test_image, test_label = get_random_image()\n",
    "\n",
    "predict_and_report_from_image(MLP, test_image,\n",
    "                              test_label, quiet=False, confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "How does the trained MLP fare if we translate the image? Run the following cell several times and see if the MLP continues to classify the digit correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "predict_and_report_from_image(MLP, test_image, test_label,\n",
    "                              jiggled=True, quiet=False, confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations about how well the MLP performs against the translated images here. Why you think the network is performing the way it does?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal my observations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "When I tested the network against the translated / jiggled images, I found that it wasn't very reliable at classifying them.\n",
    "\n",
    "Although the digits are the same size as the original digits, the original MLP has no real sense of how the pixels representing the digits relate to each other according to their *relative* location*.\n",
    "\n",
    "Instead, it is looking for pixels that overlap the pixels representing the digit that were presented in the original training set. If we translate the digit in the image frame, it may end up overlapping the pixels associated with the original location of another digit more than it overlaps the pixels associated with its own originally located image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "## Activity — rescaling the digit within the image frame\n",
    "\n",
    "How well does the trained MLP work if we rescale the image by crop it and then zooming it to fit the original image size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "predict_and_report_from_image(MLP,\n",
    "                              test_image, test_label,\n",
    "                              cropzoom=True, quiet=False, confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations about how well the MLP performs against the translated images here. Why you think the network is performing the way it does?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal my observations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "When I tested the network against the resized images, I found that it wasn't very reliable at classifying them.\n",
    "\n",
    "The original MLP has no real sense of how images are scaled across the presented image frame: it is looking for pixels that overlap the pixels representing the digit that were presented in the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO how poor is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_report_random_images(MLP, get_random_image, num_samples=100, jiggled=False, cropzoomed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network on transformed images\n",
    "\n",
    "The MNIST images we have been provided with each have dimensions of 28 x 28 pixels. If we want to try to classify a handwritten digit image using the MLP trained against these MNIST images, we need to resize the image to the same size.\n",
    "\n",
    "In a later notebook, you will be using an MLP to try to classify handwritten digit images scanned in from the simulator. These image scans have size 14 x 14 pixels. If we were to resize those collected images and then present them to our network, the scaling up of the image may introduce digital artefacts that affect the classification.\n",
    "\n",
    "So instead, lets take the opportunity now to create an MLP trained on resized hand written images, scaled down to a size of 14 x 14 pixels. This will further review the process of how we train an MLP.\n",
    "\n",
    "To being with, lets create a set of test images. The test images will be created using the following pipeline\n",
    "\n",
    "- generate an image from the image array data\n",
    "- resize image from 28 x 28 pixels down to 14 x 14 pixels\n",
    "- convert the resized image to a black and white image using a specified threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test data set made of resized images\n",
    "# Images are resized to 14 x 14\n",
    "bw_resized_image_array_list = []\n",
    "\n",
    "resize_dimensions = (14, 14)\n",
    "bw_threshold = 100\n",
    "\n",
    "# Use all the images in the original images_array\n",
    "for i in tqdm(images_array):\n",
    "    # Generate image\n",
    "    _image_from_array = Image.fromarray(i)\n",
    "    # Resize image\n",
    "    _resized_image = _image_from_array.resize(resize_dimensions, Image.LANCZOS)\n",
    "    # Convert to black and white\n",
    "    bw_resized_image = make_image_black_and_white(_resized_image, threshold=bw_threshold)\n",
    "    # Add to list\n",
    "    bw_resized_image_array_list.append(array_from_image(bw_resized_image, size=resize_dimensions))\n",
    "\n",
    "# Convert the list of training image arrays to an array\n",
    "training_arrays = np.array(bw_resized_image_array_list).reshape(len(images_array), \n",
    "                                                                resize_dimensions[0]*resize_dimensions[1])\n",
    "\n",
    "# Normalise the training data\n",
    "normalised_training_arrays = normalize(training_arrays, norm='max', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the initial network architecture. We have simplified the data both by reducing the dimensions the images (and hence the number of input nodes required) and also moved away from a discrete grey scale image representation to a binary black and white image representation.\n",
    "\n",
    "So let's use a simpler network.\n",
    "\n",
    "Let's try with just a single layer of 10 neurons to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = (10)\n",
    "max_iterations = 150\n",
    "\n",
    "MLP2 = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iterations,\n",
    "                    verbose=True,\n",
    "                    # For reproducibility, set the inital random state to a specified seed value\n",
    "                    #random_state=1,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP2.fit(normalised_training_arrays[:train_limit], labels[:train_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well did the network perform on the training samples\n",
    "test_and_report_images(MLP2, normalised_training_arrays[:train_limit], labels[:train_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well did the network perform on the test samples\n",
    "test_and_report_images(MLP2, normalised_training_arrays[train_limit:], labels[train_limit:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network\n",
    "dump(MLP2, 'mlp_mnist14x14.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "test_limit = 200\n",
    "train_limit = len(normalised_training_arrays) - test_limit\n",
    "\n",
    "from ipywidgets import interact_manual\n",
    "MLP2 = None\n",
    "\n",
    "@interact_manual(iterations=(100, 2000, 100),\n",
    "          h1=(0, 10, 1), h2=(0, 10, 1))\n",
    "def trainer(iterations=100, h1=6, h2=6):\n",
    "    global MLP2\n",
    "    MLP2 = MLPClassifier(hidden_layer_sizes=(h1, h2),\n",
    "                          max_iter=iterations)\n",
    "    \n",
    "    # Fit the model\n",
    "    MLP2.fit(normalised_training_arrays[:train_limit], labels[:train_limit])\n",
    "    \n",
    "    print(\"Training set score: {}\".format(MLP2.score(normalised_training_arrays[:train_limit], labels[:train_limit])))\n",
    "    print(\"Test set score: {}\".format(MLP2.score(normalised_training_arrays[train_limit:], labels[train_limit:])))\n",
    "    \n",
    "    #Check the prediction for each input\n",
    "    predictions = MLP2.predict((normalised_training_arrays[train_limit:]))\n",
    "\n",
    "    print(\"Test set reports\")\n",
    "    print(classification_report(labels[train_limit:], predictions))\n",
    "    print(confusion_matrix(labels[train_limit:], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def test_and_report_images(MLP, test_images, test_labels, \n",
    "                           jiggled=False, cropzoomed=False):\n",
    "    \"\"\"Test and report on pre-trained MLP using a provided set of test images.\"\"\"\n",
    "    flat_image = np.array(test_images).reshape(len(test_images), test_images[0].size)\n",
    "\n",
    "    # Normalise the values in the list\n",
    "    # to bring them into the range 0..1\n",
    "    normalised_flat_images = normalize(flat_image, norm='max')\n",
    "    predictions = MLP.predict(normalised_flat_images)\n",
    "\n",
    "    print(\"Classification report:\\n\",\n",
    "          classification_report(test_labels, predictions))\n",
    "    print(\"\\n\\nConfusion matrix:\\n\",\n",
    "          confusion_matrix(test_labels, predictions))\n",
    "\n",
    "    print(\"Training set score: {}\".format(MLP.score(normalised_flat_images, test_labels)))\n",
    "    print(\"Test set score: {}\".format(MLP.score(normalised_flat_images, test_labels)))\n",
    "\n",
    "\n",
    "def test_and_report_random_images(MLP, randfunc, num_samples=100, \n",
    "                                  jiggled=False, cropzoomed=False):\n",
    "    \"\"\"Test and report on pre-trained MLP using specified number of random images.\"\"\"\n",
    "\n",
    "    test_list, test_labels = generate_N_random_samples(randfunc=randfunc, num_samples=num_samples )\n",
    "    test_and_report_images(MLP, test_list, test_labels, jiggled, cropzoomed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_images = []\n",
    "for i in tqdm(images_array):\n",
    "    resized_images.append(array_from_image(make_image_black_and_white(Image.fromarray(i).resize((20, 20), Image.LANCZOS), threshold=100), (20, 20)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "test_and_report_images(MLP2, resized_images[train_limit:], labels[train_limit:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Activity — Improving the performance of the network\n",
    "\n",
    "__If the network doesn't work perfectly in the recognition task against the data it is trained with, try tuning the network parameters and retraining the network to see if you can improve its performance.__\n",
    "\n",
    "Once again, can you improve performance against these unseen items by tweaking the network parameters and retraining it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"girk\">\n",
    "#TO DO -  single function to train MLP.\n",
    "# Didn't I have a widget thing for this in week 6?\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "fruit = None\n",
    "\n",
    "@interact(iterations=(100, 3000, 100),\n",
    "          h1=(0, 10, 1), h2=(0, 10, 1))\n",
    "def trainer(iterations=2000, h1=6, h2=6):\n",
    "    global fruit\n",
    "    fruit = MLPClassifier(hidden_layer_sizes=(h1, h2),\n",
    "                          max_iter=iterations)\n",
    "    \n",
    "    # Fit the model\n",
    "    fruit.fit(df['Input'].to_list(), df['Fruit'])\n",
    "    \n",
    "    #Check the prediction for each input\n",
    "    predictions = fruit.predict(df['Input'].to_list())\n",
    "\n",
    "    print(classification_report(df['Fruit'], predictions))\n",
    "    print(confusion_matrix(df['Fruit'], predictions))</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to improve the performance of our MLP\n",
    "\n",
    "You may have noticed that the trained MLP did not perform particularly well when presented with the translated or resized images.\n",
    "\n",
    "Can we perhaps improve matters by increasing the size of our training dataset and \n",
    "\n",
    "Given our original training images, we can derive a set of additional training images that add further variation to the training set by translating and resizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter sweeps - to do but not in this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase size of test array\n",
    "for i in range(len(images_array)):\n",
    "    _image_array = images_array[i]\n",
    "    # And convert it to an image\n",
    "    _image_image = Image.fromarray(_image_array.astype(np.uint8))\n",
    "    \n",
    "    # Jiggle - randomly translate the image inside the image frame\n",
    "    _image_image = jiggle(_image_image)\n",
    "    \n",
    "    #Convert back to data\n",
    "    _image_array = np.array(_image_image.getdata()).astype(np.uint8)\n",
    "    _image_array = _image_array.reshape(28, 28)\n",
    "    \n",
    "    images_array = np.append(images_array, [_image_array], 0)\n",
    "    labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "len(labels), len(images_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "When presenting a raw image to a neural network, for example as a list of N x M values, one for each pixel in the image, each value represents a distinct *feature* that the network may use to help it generate a particular classification.\n",
    "\n",
    "*Feature engineering* is the name give to the process of deriving new features from the original raw data set that can used to either complement the original data set, or be presented to the network for training, and recall, instead of the original data. The aim of using derived features, rather than the original pixel features, is to try to improve the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating alternative features\n",
    "\n",
    "TO DO - convert the greyscale image to black and white\n",
    "\n",
    "\n",
    "Using the dimensions of the bounding box for each shape does not appear to provide a set of features that we can use to train a neural network on to distinguish between the shapes. We have already simplified the image data from the original RGB encoded values to a single black/white colour channel, essentially just one bit per pixel. But what other features might we identify, or even create, from the original data?\n",
    "\n",
    "One thing we might do is count the number of transitions from black to white or white to black along each row in the dataframe.\n",
    "\n",
    "I have created a simple function, `generate_signature()` that can be used to generate a \"signature\" for various sorts of input: a single image, a dataframe representing a single image, or a `numpy` array representing an images.\n",
    "\n",
    "The signature comprises sets four values, one set per row of the image, image dataframe, or image array:\n",
    "\n",
    "- the number of black to white and white to black *transitions* in the row (that is, the number of *edges* in the row);\n",
    "- the value of the *initial* pixel in the row;\n",
    "- a count of the longest run of *white* pixels in the row (that is, the width of the broadest white band in the row);\n",
    "- a count of the longest run of *black* pixels in the row (that is, the width of the broadest black band in the row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import make_image_black_and_white\n",
    "\n",
    "test_image, test_label = get_random_image()\n",
    "\n",
    "bw_img = make_image_black_and_white(test_image, thresh=200)\n",
    "\n",
    "#The image mode 1 shows it's a black and white image\n",
    "# Although if we inspect the data we see the pixel values\n",
    "# are 0 and 255 rather than 0 and 1\n",
    "bw_img.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO - can we apply this to a single row? Example\n",
    "\n",
    "# TO DO activity - apply to each row and comment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the signature for each row as follows (add the parameter `normalise=0` to normalise the values down the feature columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import generate_signature\n",
    "\n",
    "sig_df = generate_signature(bw_img, normalise=0)\n",
    "sig_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this data as the training data...\n",
    "\n",
    "Linearise  set `linear=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_df.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import generate_signature_from_series \n",
    "def generate_signature(img, thresh=200, normalise=None, linear=False, segment=False):\n",
    "    \"\"\"Generate signature from image.\"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        bw_img = make_image_black_and_white(img, thresh=thresh)\n",
    "        _rows, _cols = bw_img.size\n",
    "        _array = np.array(list(bw_img.getdata())).reshape(_rows, _cols)\n",
    "        _df = pd.DataFrame(_array)\n",
    "    elif isinstance(img, pd.DataFrame):\n",
    "        _df = img\n",
    "    else:\n",
    "        # if  array\n",
    "        _df = pd.DataFrame(img)\n",
    "        \n",
    "    if segment:\n",
    "        _df.drop(_df.index[[4, 5, 6,  9, 10, 11,  18, 19, 20, 24, 25, 26]], inplace=True)\n",
    "    _signatures = _df.apply(generate_signature_from_series, axis=1)\n",
    "    _df = pd.DataFrame(list(_signatures))\n",
    "    #Normalise down columns\n",
    "    if normalise is not None:\n",
    "        # if normalise=0 normalise down cols (features) rows\n",
    "        # if 1, normalise across rows\n",
    "        # We would expect to pass 0 here to nornalise features\n",
    "        if normalise:\n",
    "            _df = _df.T\n",
    "        _array = _df.values # Returns an array\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        scaled = min_max_scaler.fit_transform(_array)\n",
    "        _df = pd.DataFrame(scaled)\n",
    "        if normalise:\n",
    "            _df = _df.T\n",
    "    if linear:\n",
    "        return _df.values.ravel()\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing of the signatures takes some time to run (the code is far from optimal!), so we shall try to train the MLP using a collection of just 500 image signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "idfx=[]\n",
    "for i in tqdm(images_array[:500]):\n",
    "    #idfx.append(pd.DataFrame(i))\n",
    "    idfx.append(array_from_image(crop_and_zoom_to_fit(Image.fromarray(i))))\n",
    "\n",
    "idsx = []\n",
    "for i in tqdm(idfx[:500]):\n",
    "    idsx.append(generate_signature(i, linear=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal\n",
    "training_arrays = np.array(images_array[:500]).reshape(500, 28*28)\n",
    "normalised_training_arrays = normalize(training_arrays, norm='max', axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized to 20 x 20\n",
    "idfr=[]\n",
    "for i in tqdm(images_array[:500]):\n",
    "    idfr.append(array_from_image(Image.fromarray(i).resize((20, 20), Image.LANCZOS), size=(20,20)))\n",
    "    \n",
    "training_arrays = np.array(idfr).reshape(500, 20*20)\n",
    "normalised_training_arrays = normalize(training_arrays, norm='max', axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cropped resized\n",
    "training_arrays = np.array(idfx).reshape(len(idsx), 28*28)\n",
    "normalised_training_arrays = normalize(training_arrays, norm='max', axis=0)\n",
    "#normalised_training_arrays[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_signatures = np.array(idsx).reshape(len(idsx), 28*4)#16*4)\n",
    "normalised_training_arrays = normalize(flat_signatures, norm='max', axis=0)\n",
    "normalised_training_arrays[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "hidden_layer_sizes = (40)\n",
    "max_iterations = 2000\n",
    "\n",
    "MLP2 = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iterations,\n",
    "                    verbose=True,\n",
    "                    # For reproducibility, set the inital random state to a specified seed value\n",
    "                    #random_state=1,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train network\n",
    "test_limit = 50\n",
    "train_limit = len(normalised_training_arrays) - test_limit\n",
    "\n",
    "# Train the MLP on a subset of the images\n",
    "\n",
    "MLP2.fit(normalised_training_arrays[:train_limit], labels[:train_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(MLP.loss_curve_)\n",
    "plt.title(\"Loss curve for MLP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predict_from_image(MLP, img, quiet=True, zoomview=False,\n",
    "                             confidence=False, signature=False):\n",
    "    \"\"\"Class prediction from an image.\"\"\"\n",
    "    flat_image = array_from_image(img).reshape(1, img.size[0]*img.size[1])\n",
    "\n",
    "    if signature:\n",
    "        _signature = generate_signature(img, linear=True)\n",
    "        flat_signature = np.array(_signature).reshape(1, img.size[0]*4)\n",
    "        normalised_flat_image = normalize(flat_signature, norm='max', axis=1)\n",
    "    else:\n",
    "        # We can normalise the values so they fall in the range 0..1\n",
    "        normalised_flat_image = normalize(flat_image, norm='max', axis=1)\n",
    "    \n",
    "    if not quiet:\n",
    "        if zoomview:\n",
    "            zoom_img(img)\n",
    "        else:\n",
    "            display(img)\n",
    "\n",
    "    if confidence:\n",
    "        prediction_class_chart(MLP, normalised_flat_image)\n",
    "\n",
    "    return MLP.predict(normalised_flat_image)[0]\n",
    "\n",
    "\n",
    "# +\n",
    "\n",
    "def predict_and_report_from_image(MLP, img, label='',\n",
    "                                  jiggled=False, cropzoom=False,\n",
    "                                  quiet=False, zoomview=False,\n",
    "                                  confidence=False,\n",
    "                                  signature=False):\n",
    "    \"\"\"Predict the class and report on its correctness.\"\"\"\n",
    "    if jiggled:\n",
    "        img = jiggle(img)\n",
    "    if cropzoom:\n",
    "        img = crop_and_zoom_to_fit(img)\n",
    "\n",
    "    prediction = class_predict_from_image(MLP, img, quiet=quiet,\n",
    "                                          zoomview=zoomview, confidence=confidence, signature=signature)\n",
    "\n",
    "    if label:\n",
    "        print(f\"MLP predicts {prediction} compared to label {label}; classification is {prediction == label}\")\n",
    "    else:\n",
    "         print(f\"MLP predicts {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test network on single image\n",
    "test_image, test_label = get_random_image()\n",
    "predict_and_report_from_image(MLP2, test_image, test_label,\n",
    "                              jiggled=False, quiet=False, confidence=False, signature=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test network on single image signature\n",
    "\n",
    "test_image, test_label = get_random_image()\n",
    "predict_and_report_from_image(MLP2, test_image, test_label,\n",
    "                              jiggled=False, quiet=False, confidence=False, signature=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full test\n",
    "\n",
    "test_and_report_random_images(MLP2, get_random_image, num_samples=100, signature=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have reduced the amount of numbers that represent each row from 20 separate pixel values to just 4 signature values, we arguably have a much more powerful representation of the image that captures much of the useful *information* in the image.\n",
    "\n",
    "In particular, casting the image to a black and white image removes potential variation arising from noise caused by greyscale pixel values. Counting the number of edges is invariant if we shift the image slightly to the left or the right in the sensor view; the first pixel value in the row also provides information as to whether the first transition is from white to black or black to white.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO - retrain and then try again with translated and zoomed images\n",
    "\n",
    "# What effect might you expect if  we cropped the training images and then resized them to a fixed size before finding their signature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,.md//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
