{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Testing a network against synthetic data\n",
    "\n",
    "One of the problems associated with training neural networks is that we often need *a lot* of data in order to be able to train the network effectively.\n",
    "\n",
    "One way of increasing the amount of data available to us is to generate synthetic data to supplement our collected datasets. This synthetic data may be derived from our original datasets, or created \"from scratch\".\n",
    "\n",
    "For example, with the MNIST hand written digit image dataset, we can create derived datasets by cropping the original digits and translating them around the 28x28 pixel training image view. We could create completely synthetic data by taking images from computer fonts, and perhaps adding noise to them, to create new digit training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "*This notebook is contains quite a lot of complex code, but you are not expected to be able to write code of this complexity, nor even to necessarily understand it. Instead, it is provided as a demonstration of what sorts of steps are required to perform particular actions, and what sort of code can be used to implement those steps.*\n",
    "\n",
    "*So treat the notebook as if you were being given a tour o a working engineering lab, skim over the code definitions (unless you are particularly interested) and just observe the effects of calling the functions that have been implemented.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Creating new images from old — translating cropped images\n",
    "\n",
    "To start with, let's load in the original MNIST data as a list of images, along with the corresponding MNIST labels, using a function based on the code we used in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import load_MNIST_images_and_labels\n",
    "\n",
    "images_list, labels = load_MNIST_images_and_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grab random samples from this data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import get_random_image\n",
    "                 \n",
    "(test_image, test_label) = get_random_image(images_list, labels, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already seen how we can get rid of the \"background\" columns and rows around the outside of an image using the `nn_tools.sensor_data.trim_image()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import trim_image\n",
    "\n",
    "# Pass the parameter show=False to hide the display\n",
    "# of the untrimmed and trimmed dataframes\n",
    "trimmed_image_df = trim_image( test_image, background=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert this dataframe back to an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import image_from_df, zoom_img\n",
    "\n",
    "cropped_image = image_from_df(trimmed_image_df)\n",
    "zoom_img( cropped_image )\n",
    "\n",
    "cropped_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create a blank image of size 28 x 28 pixels with a grey background, we can paste a copy of our cropped image into it. The `nn_tools.sensor_data.jiggle()` function implements this approach. It will accept an image and then return randomly translated version of it.\n",
    "\n",
    "Run the following cell several times to see the effect of the `jiggle()` function on a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import jiggle\n",
    "\n",
    "jiggled_image =  jiggle(test_image)\n",
    "zoom_img(jiggled_image)\n",
    "\n",
    "# Show size\n",
    "jiggled_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 4.1.1 Activity — translating the digit within the image frame\n",
    "\n",
    "Explore how the `sensor_data.jiggle()` function works in practice. Run the following cell multiple times, observing what happens in each case, to see how the a differently translated versions of the image are returned each time the function is called. Is there much variation in how the digit is centered in the image frame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import jiggle\n",
    "\n",
    "# Setting quiet=False displays the original input image\n",
    "# as well as returning the jiggled image as cell output\n",
    "zoom_img( jiggle(test_image, quiet=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record any notes or observations here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Example discussion\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal an example discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "The `jiggle` function slightly translates the image to the left, right, and up and down within the image area. However, it nevers seems to be translated so far that bits of it get cut off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Testing the MLP against derived images\n",
    "\n",
    "Load in the MLP you saved at the end of the last notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "MLP = load('mlp_mnist_28x28.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it still works okay with the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import test_and_report_random_images\n",
    "\n",
    "test_and_report_random_images(MLP,\n",
    "                              get_random_image, images_list, labels,\n",
    "                              num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "## Activity — Testing the MLP against translated digit images\n",
    "\n",
    "Now let's see how well our trained MLP responds to translated versions of the original training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "Start by testing the network against one of the original images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from nn_tools.network_views import predict_and_report_from_image\n",
    "\n",
    "test_image, test_label = get_random_image(images_list, labels)\n",
    "\n",
    "predict_and_report_from_image(MLP, test_image,\n",
    "                              test_label, quiet=False, confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "How does the trained MLP fare if we translate the image? Run the following cell several times and see if the MLP continues to classify the digit correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "predict_and_report_from_image(MLP, test_image, test_label,\n",
    "                              jiggled=True, quiet=False, confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations about how well the MLP performs against the translated images here. Why you think the network is performing the way it does?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal my observations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "When I tested the network against the translated / jiggled images, I found that it wasn't very reliable at classifying them.\n",
    "\n",
    "Although the digits are the same size as the original digits, the original MLP has no real sense of how the pixels representing the digits relate to each other according to their *relative* location*.\n",
    "\n",
    "Instead, it is looking for pixels that overlap the pixels representing the digit that were presented in the original training set. If we translate the digit in the image frame, it may end up overlapping the pixels associated with the original location of another digit more than it overlaps the pixels associated with its own originally located image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Creating more new images from old — zooming cropped images\n",
    "\n",
    "Another way of transforming the cropped images is to magnify it back to the original image size. (The rescaling employs a digital filter that is used to interpolate new pixel values in the scaled image based on the pixel values in the cropped image. As such, it may introduce digital artefacts of its own into the scaled image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import crop_and_zoom_to_fit\n",
    "\n",
    "crop_zoomed_image = crop_and_zoom_to_fit(test_image)\n",
    "\n",
    "zoom_img( crop_zoomed_image )\n",
    "\n",
    "# Show size\n",
    "crop_zoomed_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process of applying these transformations to an a test image, we can call the `predict_and_report_from_image()` with the `jiggled=True` and `cropzoom=True` parameters:\n",
    "\n",
    "```python\n",
    "# Test a jiggled version of the provided image\n",
    "predict_and_report_from_image(MLP, test_image, test_label,\n",
    "                              jiggled=True, quiet=False)\n",
    "\n",
    "# Test a cropped and then zoomed version of the provided image\n",
    "predict_and_report_from_image(MLP,\n",
    "                              test_image, test_label,\n",
    "                              cropzoom=True, quiet=False)\n",
    "```\n",
    "\n",
    "You can also pass the `zoomview=True` parameter to view the image at a larger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 4.3.1 Activity — rescaling the digit within the image frame\n",
    "\n",
    "How well does the trained MLP work if we rescale the image by crop it and then zooming it to fit the original image size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "predict_and_report_from_image(MLP,\n",
    "                              test_image, test_label,\n",
    "                              cropzoom=True, quiet=False, confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations about how well the MLP performs against the translated images here. Why you think the network is performing the way it does?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### Example discussion\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal an example discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "When I tested the network against the resized images, I found that it wasn't very reliable at classifying them.\n",
    "\n",
    "The original MLP has no real sense of how images are scaled across the presented image frame: it is looking for pixels that overlap the pixels representing the digit that were presented in the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 4.3.2 Activity — Testing the network against lots of jiggled and zoomed images\n",
    "\n",
    "Run various combinations of the following test code to see how well the network behaves when tested against lots of transformed images. How well does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "test_and_report_random_images(MLP, \n",
    "                              get_random_image, images_array=images_list, labels=labels,\n",
    "                              num_samples=100, jiggled=True, cropzoom=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Training the network on transformed images\n",
    "\n",
    "The MNIST images we have been provided with each have dimensions of 28 x 28 pixels. If we want to try to classify a handwritten digit image using the MLP trained against these MNIST images, we need to resize the image to the same size.\n",
    "\n",
    "In a later notebook, you will be using an MLP to try to classify handwritten digit images scanned in from the simulator. These image scans have size 14 x 14 pixels. If we were to resize those collected images and then present them to our network, the scaling up of the image may introduce digital artefacts that affect the classification.\n",
    "\n",
    "So instead, lets take the opportunity now to create an MLP trained on resized hand written images, scaled down to a size of 14 x 14 pixels. This will further review the process of how we train an MLP.\n",
    "\n",
    "To being with, lets create a set of test images. The test images will be created using the following pipeline:\n",
    "\n",
    "- generate an image from the image array data\n",
    "- resize image from 28 x 28 pixels down to 14 x 14 pixels\n",
    "- convert the resized image to a black and white image using a specified threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import resized_images_pipeline\n",
    "\n",
    "resized_images = resized_images_pipeline(images_list, size=(14, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the initial network architecture. We have simplified the data both by reducing the dimensions the images (and hence the number of input nodes required) and also moved away from a discrete grey scale image representation to a binary black and white image representation.\n",
    "\n",
    "So let's use a simpler network.\n",
    "\n",
    "Let's try with just a single layer of 10 neurons to start with. We can use the `quick_progress_tracked_training()` function to provide a simple way of creating, an MLP and tracking its training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import quick_progress_tracked_training\n",
    "\n",
    "\n",
    "hidden_layer_sizes = (20)\n",
    "max_iterations = 150\n",
    "\n",
    "\n",
    "test_limit = 100\n",
    "train_limit = len(resized_images) - test_limit\n",
    "\n",
    "\n",
    "resized_training_images = resized_images[:train_limit]\n",
    "training_labels = labels[:train_limit]\n",
    "\n",
    "resized_testing_images = resized_images[train_limit:]\n",
    "testing_labels = labels[train_limit:]\n",
    "\n",
    "MLP2 = quick_progress_tracked_training(resized_training_images, training_labels,\n",
    "                                 hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                 # top up an existing reptrained MLP: MLP=MLP,\n",
    "                                 max_iterations=max_iterations,\n",
    "                                 loss=True, # show loss function\n",
    "                                 structure=True # show network params\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations here about how well the MLP performs during training.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the network perform on the unseen test samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import test_and_report_image_data\n",
    "from nn_tools.sensor_data import get_images_features\n",
    "\n",
    "resized_testing_data = get_images_features(resized_testing_images, normalise=True)\n",
    "test_and_report_image_data(MLP2, resized_testing_data, testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations here about how well the network performs on the previous unseen data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this network performed on jiggled images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_report_random_images(MLP2, \n",
    "                              get_random_image, images_array=resized_testing_images, labels=testing_labels,\n",
    "                              num_samples=100, jiggled=True, cropzoom=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations here about how well the network performs when tested with the jiggled image data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I tried, it appeared not to perform very well at all with the jiggled images..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save the network\n",
    "dump(MLP2, 'mlp_mnist14x14.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Rapidly training the MLP\n",
    "\n",
    "Use the following `ipywidgets` application to easily try out different network structures when training the MLP using the resized image training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "\n",
    "from ipywidgets import interact_manual\n",
    "\n",
    "MLP3=None\n",
    "\n",
    "@interact_manual(iterations=(100, 2000, 100),\n",
    "          h1=(0, 10, 1), h2=(0, 10, 1))\n",
    "def trainer(iterations=100, h1=6, h2=6, updater=False):\n",
    "    global MLP3\n",
    "    MLP3 = quick_progress_tracked_training(resized_training_images, training_labels,\n",
    "                                 hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                 max_iterations=40,\n",
    "                                 MLP = MLP3 if updater else None,\n",
    "                                 loss=True, # show loss function\n",
    "                                 structure=True # show network params\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the new network using the previously unseen test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_report_image_data(MLP3, resized_testing_data, testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "*Feel free to experiment with training and testing the network using different setups. But don't spend too much time playing!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record any notes and observations you care to make about your experimentation here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(MLP3, 'resized_images_MLP.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Summary\n",
    "\n",
    "In this notebook, you have seen how we can created synthetic training data derived from the original dataset, in particular by cropping the original handwritten digits and then translating them to a new location within the original 28x28 pixel image frame, or zooming the cropped image to fit the original image frame.\n",
    "\n",
    "You also saw how we can create a derived dataset from the original images consisting of images in a smaller image frame (14x14 rather than 28x28 pixels) and then train a new MLP on that. As a result of reducing the number of input features, we could get also get away with using a smaller neural network to recognise the supplied images.\n",
    "\n",
    "On testing the original MNIST data trained network against the translated and zoomed images, you also saw how the network performance was considerably degraded.\n",
    "\n",
    "In the next notebook, you will learn how we can improve the network's performance by find a new way of representing the images to the network using many fewer, but far more relevant, input features."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,.md//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
