{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training an MLP using MNIST handwritten digit data\n",
    "\n",
    "In this notebook, you will be working with a very famous dataset known as the [MNIST database of handwritten images](http://yann.lecun.com/exdb/mnist/) (*Modified National Institute of Standards and Technology* database).\n",
    "\n",
    "The complete dataset is composed of a training set of 60,000 example images, and a test set containing a further 10,000 examples. Each of the handwritten digit images have been \"size-normalized\" to the same image size. In addition, each sample digit has been centered within the fixed-size image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Extracting MNIST training images from an image data file\n",
    "\n",
    "The MNIST handwritten image dataset has been widely used for benchmarking the performance of different machine learning techniques, particularly in their early stages of development. It is also widely used for demonstration purposes, and you will meet several neural networks that were originally trained on the dataset in later notebooks.\n",
    "\n",
    "In order to work with the dataset, we need to access it somehow. One common way of distributing the dataset is to encode all the handwritten digit image files, or batches or them, within another image file. In the example we will be working with, where each handwritten digit image is represented as a 28 x 28 pixel greyscale image, each row of the \"distribution\" image file contains the 28 x 28 x 1 = 784 pixel values that represent a single 28 x 28 pixel handwritten digit image.\n",
    "\n",
    "This is what one of the distribution data image files looks like:\n",
    "\n",
    "![](mnist_batch_0.png)\n",
    "\n",
    "At first glance, it doesn't look like a lot of handwritten digits, does it?\n",
    "\n",
    "So let's investigate that large image file a little bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "*This notebook includes quite a lot of fiddly bits of code to handle the MNIST images data. You are not expected to be able to write this sort of code, nor even to necessarily understand it. It is provided as a demonstration to illustrate the process steps and the sort of code required to actually make use of the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Importing the MNIST data image\n",
    "When displayed as an image, the image is 784 pixels wide and 3000 pixels high, which we can see from the size of the image if we load it in to Python as an image object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#Load in the image data file\n",
    "img = Image.open('mnist_batch_0.png')\n",
    "\n",
    "#If the image is a colour image, we can use various tools\n",
    "#to convert it to a greyscale image\n",
    "#.convert(\"L\")\n",
    "\n",
    "# Display the size of the image as (rows, columns)\n",
    "print(f'The image size, given as (columns, rows), is {img.size}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image itself contains 3000 lines of MNIST image data, corresponding to 3000 separate handwritten digit images. The 784 columns in each row represent a linearised version of the $28 \\times 28 = 784$ values that represent the values of each pixel in each `28 x 28` handwritten digit image.\n",
    "\n",
    "We can preview what one of the rows looks like by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we convert the image data to a one dimensional array (i.e. a list of values)\n",
    "# the first 784 elements will represent the contents of the first row\n",
    "# That is, a linear representation of the the first 28 x 28 pixel sized handwritten digit image\n",
    "print(list(img.getdata())[:784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the image object to see how the data has been encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.getbands()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, from the [`PIL` package documentation](https://pillow.readthedocs.io/en/4.1.x/handbook/concepts.html#modes), we see that mode `L` corresponds to a black and white image encoded with 8-bit pixels, defining each pixel as an integer with one of $2^8$ values, which is to say, an integer in the range `0..255`, as can be seen from the preview of the first row of the image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Extracting individual digit images\n",
    "\n",
    "What happens if we take one of these rows of data, cast it into its own 28 x 28 array, and convert it to an image file format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Turn the image data into a multidimensional array\n",
    "# of 3000 separate 28 x 28 arrays\n",
    "images_array = np.array(img).reshape(3000, 28, 28)\n",
    "\n",
    "#Get the third item (index value 2), that is, the third 28 x 28 image data array\n",
    "image_array = images_array[2]\n",
    "\n",
    "# And convert it to an image\n",
    "image_image = Image.fromarray(image_array)\n",
    "\n",
    "# Then display it\n",
    "image_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sensor_data.image_from_array()` function will also create the image for us using the same approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import image_from_array\n",
    "\n",
    "image_from_array(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can zoom in on an image to look at it in more detail using the `nn_tools.sensor_data.zoom_img` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import zoom_img\n",
    "\n",
    "zoom_img(image_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see the handwritten digit not as series of numbers but as an actual image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, if we want to work with a list of images, we can generate on using the `get_images_list_from_images_array()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import get_images_list_from_images_array\n",
    "\n",
    "images_list = get_images_list_from_images_array(images_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Viewing an individual digit image as data\n",
    "\n",
    "We can also view the image data in a *pandas* dataframe, trimming the dataframe to remove background coloured edging.\n",
    "\n",
    "*By default, the dataframe returning functions will preview the dataframe with colour highlight; pass the attribute `show=False` to disable this view.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import trim_image, df_from_image\n",
    "\n",
    "trimmed_image = trim_image( df_from_image(image_image, show=False), background=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your own observations here about how the numeric values associated with each pixel correspond to the way they are rendered in the original image and in the false colour dataframe heat map shown above.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, the original *mnist_batch_0.png* file, which just happened to be an image file and could be viewed as such, was actually being used as a convenient way of transporting 3000 rows of data. In turn, each row of data could itself be transformed into a square data array that could be then be rendered as a distinct handwritten digit image. The image itself, of course, is just numbers underneath...\n",
    "\n",
    "### 3.1.4 So what?\n",
    "\n",
    "At this point, you may be wondering what this has to do with training neural networks, let alone programming robots. What the example serves to demonstrate is that training a neural network on some test data may require a range of computer skills and knowledge to even get the data into a form where you can begin to make use of it.\n",
    "\n",
    "Working with file formats and raw data representations often represents a large part of the workload associated with any data analysis, modelling, or classification task, and often requires significant computational data handling skills. Whilst we don't expect you to learn how to perform these data wrangling tasks yourself as part of this module, you should be aware then when you see recipes saying things like \"*just load in the dataset...*\", there may be quite a lot of work associated with that word, *just*.\n",
    "\n",
    "*To learn more about working with data along the whole data pipeline, from data acquisition, to data cleaning, management, storage, analysis and presentation, consider taking the Open University module [__TM351 Data Management and Analysis__](http://www.open.ac.uk/courses/modules/tm351).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Preparing the MNIST image training data\n",
    "\n",
    "Although MLP classifiers can struggle with large images, the 28 x 28 pixel image size used for the MNIST images is not too large to train an MLP on, although it does require an input layer containing 784 neurons, one for each pixel.\n",
    "\n",
    "To train the MLP on the linearised pixel values, we need to present labeled images that identify the category (that is, the digit) that each image represents.\n",
    "\n",
    "The training labels are provided in a separate file which we can load in as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# The labels.txt file contains 3000 digit labels in the same order as the image data file\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Show the length of the label array and the value of the first 10 digits\n",
    "len(labels), labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Grabbing random test images and their labels\n",
    "\n",
    "The following cell imports a helper function to retrieve a random image from the images array, or the image corresponding to the provided index value:\n",
    "\n",
    "Run the following cell repeatedly to try the `get_random_image()` function out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.sensor_data import get_random_image\n",
    "                 \n",
    "(test_image, test_label) = get_random_image(images_array, labels, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "*Rerun the previous cell several times to preview the display of different randomly selected images.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record any observations or notes here that you would like to make about the images you observed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a particular image, such as the first image in the dataset, pass its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_image, test_label) = get_random_image(images_array, labels, show=True, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training a simple MLP on the MNIST image data\n",
    "\n",
    "We can now train a simple MLP from the MNIST data and the training labels.\n",
    "\n",
    "The *ScikitLearn* `MLPClassifier` can automatically identify from a training set the number of nodes required for the input and output layers, so all we need to provide is the hidden layer(s) definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "*One of the intuitions for making sense of how neural networks work is to consider each thing to be recognised as being represented by a vector. In the case of the MNIST images, this is a 784 dimensional vector (which is rather more dimensions than the three dimensional world you are familiar with!)*\n",
    "\n",
    "*The classification task is then one of trying to classify things as belonging to the same group if they are pointing in roughly the same direction in this high dimensional space.*\n",
    "\n",
    "*The point of normalising the lengths of the vectors is because we are most interested in the direction they are pointing, not how far along the path they are.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Training the MLP\n",
    "\n",
    "For starters, let's see if we can train the network to classify the images using a single layer containing 40 hidden neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layer_sizes = (40)\n",
    "max_iterations = 40\n",
    "\n",
    "MLP = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iterations,\n",
    "                    verbose=True,\n",
    "                    # For reproducibility, set the inital random state to a specified seed value\n",
    "                    #random_state=1,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to present the data as a list (that is, as a one-dimensional linear array) of lists (or vectors). Each vector contains 784 values, corresponding to the $28\\times28$ pixels in each image.\n",
    "\n",
    "To simplify training, we also normalise the length of each of these image vectors so the length of each vector is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `images_list` contains the image data as a list of images. We can retrieve the pixel data for an image as a list (`list(image.getdata())`), and then normalise the image vectors using the SciKit Learn `normalize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "images_data = []\n",
    "\n",
    "# For each image in the images_list\n",
    "for image in images_list:\n",
    "    # Get the data as a list\n",
    "    # and append it to the images_data list\n",
    "    images_data.append(list(image.getdata()))\n",
    "\n",
    "# The axis=1 arguments normalises each individual image vector\n",
    "normalised_images_data = normalize(images_data, axis=1)\n",
    "\n",
    "# This functionality also made available as:\n",
    "# nn_tools.sensor_data.get_images_features(images_list, normalise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the network, we can use the first 2,900 images as a training set and hold back 100 images to use as a \"previously unseen\" image test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_limit = 100\n",
    "train_limit = len(normalised_images_data) - test_limit\n",
    "\n",
    "# Train the MLP on a subset of the images\n",
    "\n",
    "MLP.fit(normalised_images_data[:train_limit], labels[:train_limit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the shape of the trained network and the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import network_structure, show_loss\n",
    "\n",
    "network_structure(MLP)\n",
    "show_loss(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your own observations about the shape of the loss curve. Does it plateau towards a flat line?*\n",
    "\n",
    "*If the line hasn't flattened off, but is still falling steeply, consider changing the number of hidden layers, and/or the maximum number of training iterations and retraining the network. Can you flatten the loss curve?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-danger"
    ]
   },
   "source": [
    "*Take care not to push the network too far. If you keep on training it, it will get better and better at classifying the images you present it with, but it's performance against previously unseen images is likely to degrade as the network `overfits` itself to the training data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Testing the performance of the network\n",
    "\n",
    "With the network trained, we can check how well it performs on the images in the training set using the classification report and confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "training_data = normalised_images_data[:train_limit]\n",
    "training_labels = labels[:train_limit]\n",
    "\n",
    "predictions = MLP.predict(training_data)\n",
    "\n",
    "print(\"Classification report:\\n\",\n",
    "      classification_report(training_labels, predictions))\n",
    "print(\"\\n\\nConfusion matrix:\\n\",\n",
    "      confusion_matrix(training_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "*Recall that the precision and recall scores range from 0 (worst) to 1 (best, with the precision relating the number of true positives and false positives and the recall score relating the number of true positives and false negatives; the f1 score attempts to combine the sense of precision and recall as a weighted average of those two scores, with a value of 1 representing the best performance and 0 the worst.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the *precision*, *recall* and *f1-score* values are close to 1, that's a good sign; and if large values predominate on the diagonal of the confusion matrix, that shows that most digits are identified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your own observations about the performance of the network. Does it appear to struggle with any particular images?*\n",
    "\n",
    "*If the network is performing really badly, consider changing the number of hidden layers, and/or the maximum number of training iterations and then retrain the network.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Testing a single random image\n",
    "\n",
    "We can use the `nn_tools.network_views.predict_and_report_from_image()` function, in the following example testing against a randomly selected image and its classification label.\n",
    "\n",
    "Omitting the test label will just return the prediction. A zoomed view of the sample image can be seen by setting `zoomview=True` when calling the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import predict_and_report_from_image \n",
    "\n",
    "# Get a test image\n",
    "(test_image, test_label) = get_random_image(images_list, labels)\n",
    " \n",
    "# And test the trained MLP against it\n",
    "predict_and_report_from_image(MLP, test_image, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Probing the MLP's confidence in its predictions\n",
    "\n",
    "Even though it may be hard for us to see from the network's weights exactly what is going on, the network appears to be doing its job in terms of classifying digits, at least when it comes to the sample images.\n",
    "\n",
    "For example, by passing the `confidence=True` parameter to the `predict_and_report_from_image()` function, we can display a bar chart showing the confidence of the prediction for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_report_from_image(MLP, test_image, confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Generate and test several more random image selection predictions and then record you own observations here about how confident the network was in its prediction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Testing the MLP against multiple images\n",
    "\n",
    "As well as testing the network against data it has already seen, we can also test it against images we held back and that it hasn't seen before. Once again, we can review the effectiveness of the network by means of the classification report and confusion matrix, this time handled more conveniently using the `test_and_report_image_data` function from `nn_tools.network_views`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import test_and_report_image_data\n",
    "\n",
    "testing_data = normalised_images_data[train_limit:]\n",
    "testing_labels = labels[train_limit:]\n",
    "\n",
    "test_and_report_image_data(MLP, testing_data, testing_labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your own observations here about how classification report and the confusion matrix. Does the network appear to be confused by any images in particular?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarise the performance using the MLP `.score()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {}\".format(MLP.score(training_data, training_labels)))\n",
    "print(\"Test set score: {}\".format(MLP.score(testing_data, testing_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You will have an opportunity to explore other MLP configurations and training regimes later in this notebook to see if you can improve the performance of the network.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 Testing the MLP using multiple random images\n",
    "\n",
    "The `nn_tools.network_views.test_and_report_random_images()` can be used to test the trained MLP against a specified number of samples, with samples picked by a specified function, such as our `get_random_image()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import test_and_report_random_images\n",
    "\n",
    "test_and_report_random_images(MLP,\n",
    "                              get_random_image, images_list, labels,\n",
    "                              num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your own observations here about how well the network performs. Does it appear to be confused by any images in particular?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Rapidly retraining the MLP\n",
    "\n",
    "In defining the MLP originally, we specified a maximum number of training iterations, as well as the *verbose* reporting option. By default, a progress bar display is not available when training the MLP, but we can create one by defining a minimal MLP, training it on a single iteration to define the classes, and then training it across multiple iterations using the `.partial_fit()` method, which applies additional training iterations to the MLP.\n",
    "\n",
    "This approach has been implemented as the function `network_views.progress_tracked_training()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import quick_progress_tracked_training\n",
    "from nn_tools.sensor_data import get_images_features\n",
    "\n",
    "# Specify some parameters\n",
    "hidden_layer_sizes = (40)\n",
    "max_iterations = 50\n",
    "\n",
    "# Identify training images and labels\n",
    "training_images = images_list[:train_limit][:train_limit]\n",
    "training_labels = labels[:train_limit]\n",
    "\n",
    "# Create a new MLP\n",
    "MLP2 = quick_progress_tracked_training(training_images, training_labels,\n",
    "                                      hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                      max_iterations=40, report=False)\n",
    "\n",
    "# Top up an existing MLP\n",
    "MLP2 = quick_progress_tracked_training(training_images, training_labels,\n",
    "                                      MLP=MLP2,\n",
    "                                      max_iterations=40, report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and report on this trained network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_report_random_images(MLP2,\n",
    "                              get_random_image, images_list, labels,\n",
    "                              num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a simple end-user application to help us train the network in a more interactive fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual\n",
    "\n",
    "MLP3=None\n",
    "\n",
    "@interact_manual(iterations=(100, 2000, 100),\n",
    "          h1=(0, 10, 1), h2=(0, 10, 1))\n",
    "def trainer(iterations=100, h1=6, h2=6, updater=False):\n",
    "    global MLP3\n",
    "    MLP3 = quick_progress_tracked_training(training_images, training_labels,\n",
    "                                 hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                 max_iterations=40,\n",
    "                                 MLP = MLP3 if updater else None,\n",
    "                                 loss=True, # show loss function\n",
    "                                 structure=True # show network params\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "*Feel free to experiment with training the network using different setups — but don't spend too much time playing!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record any notes and observations you care to make regarding any experimentation you carried out with respect to the training and testing of the MLP.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Saving the MLP\n",
    "\n",
    "We can persist the model by saving it to a file using a variant of the of Python `pickle` module, as described in the `scikit-learn` documentation](https://scikit-learn.org/stable/modules/model_persistence.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(MLP, 'mlp_mnist_28x28.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly important of we want to share the trained model, not least in situations where it may take some considerable time to train the model.\n",
    "\n",
    "We can load it back in again in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "MLP = load('mlp_mnist_28x28.joblib')\n",
    "\n",
    "# Test that it still workks...\n",
    "predict_and_report_from_image(MLP, test_image, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "## 3.8 Optional Activity — Visualising the trained MLP weights\n",
    "\n",
    "*This is an experimental optional activity. It is still quite brittle. Click the arrow in the sidebar, or run this cell, to view the activity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "In passing, we can plot the 28 x 28 incoming weights into the hidden layer neurons in a 28 x 28 grid to see how they filter the input values. The code is rather fiddly, so don't try to make too much sense of it. You will notice that to human eyes at least, none of the input neurons has weights that apparently encode directly for a particular handwritten integer (1, 2, 3 etc.).\n",
    "\n",
    "Note that this only works at the moment for a single layer network with 40 hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from nn_tools.network_views import preview_weights\n",
    "\n",
    "preview_weights(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Summary\n",
    "\n",
    "In this notebook, we have explored the MNIST handwritten digit image dataset and trained a multilayer perceptron against a part of it. The MLP takes as input the 28x28 original pixel values used to represent each handwritten digit image and provided one of 10 predicted classification outputs. To validate the trained network it was tested against some test images that we had held back from the original training set.\n",
    "\n",
    "But how robust is our network when it comes to classifying images that were perhaps not in the original dataset at all?\n",
    "\n",
    "For example, will the network still recognise an image if we slightly recenter it in original image frame?\n",
    "\n",
    "The next two notebooks contain optional study material. The first provides you with an opportunity to see just how well the network is able to recognise images that may look much the same to us as the original images, but actually have quite distinct features from the original training data. The second develops further intuitions about what it actually is that we are providing as input data to a neural network.\n",
    "\n",
    "The required material continues with a look \"inside the mind\" of a neural network, exploring how we can start to visualise the network structure and behaviour in order to get a better understanding of what it is actually doing."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,.md//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
