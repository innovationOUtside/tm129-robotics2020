{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multidimensional data and the MLP\n",
    "\n",
    "The neural network examples in the previous notebook were very impressive, but how do they work?\n",
    "\n",
    "In this notebook we’ll make a start by looking at a classification task that attempts to identify various pieces of fruit. The activity is a simplified, stylised one, but it demonstrates several of the features (pun intended, as you may see!) that more complex neural network models employ when performing their recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Fruit recognition task\n",
    "\n",
    "Consider four classes of fruit: pears, bananas, strawberries and oranges. How might a robot recognise and distinguish between them?\n",
    "\n",
    "Let’s suppose that a robot’s vision system can isolate the fruit objects and make two measurements. The first is the length of the ‘long axis’. This is called the *long measurement*. The second measurement is taken at right angles to the long axis, halfway along it. This is called the *short measurement*. Oranges are approximately spherical in shape, which means that their long and short measurements are nearly the same. Bananas, on the other hand, are long and thin, so their long measurements are larger than their short measurements.\n",
    "\n",
    "Note that we are also making an assumption that the fruits are all presented to the same scale. This is fine if we are using a fixed camera and the fruit is passing underneath on a conveyor belt, but in a more general photograph that information may not be so easy to discern. The approach we’re taking also suggests we can segment the photographic image to just give us the fruit object within it. Again, in an industrial setting, we may be able to control for this (only one object in view, on a clean white conveyor background, for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume we have got the images in the form that is required.\n",
    "\n",
    "![Pictures of various pieces of fruit on a grid showing their long and short axis measurements: pears medium sized and longer than they are wide; bananas are long and narrow; oranges are medium sized and as wide as they are long; strawberries are small and as wide as they are long.](../images/tm129_rob_p8_f002.jpg)\n",
    "\n",
    "The table below records the measurements listed for the fruit shown in the diagram calculated according to the method described above. The long measurement is given first, followed by the short measurement.\n",
    "\n",
    "|pears|bananas|strawberries|oranges|\n",
    "|--- |--- |--- |--- |\n",
    "|(5.2, 3.1)|(8.5, 1.9)|(2.1, 1.4)|(4.7, 4.5)|\n",
    "|(6.3, 2.4)|(8.3, 1.6)|(2.8, 1.8)|(4.6, 4.2)|\n",
    "|(6.7, 1.8)|(9.7, 2.0)|(2.0, 1.8)|(4.6, 4.1)|\n",
    "|(5.3, 2.9)|(7.5, 1.7)|(2.2, 2.0)|(4.0, 3.7)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These measurements can be plotted on a grid. The longest measurement is plotted on the horizontal axis, and the shorter measurement is plotted on the vertical axis.\n",
    "\n",
    "![A graph showing clusters of different fruits. The fruit short-axis measurement is plotted on the vertical axis which runs from 0 to 5. The fruit long-axis measurement is plotted on the horizontal axis which runs from 0 to 10. Each individual fruit is plotted at its long and short axis measurements, and each cluster of fruit is enclosed by a circle or ellipse. The four strawberries form a cluster centred around (2.2, 1.7). The four oranges cluster around (4.4, 4.1). The pears form a slightly looser cluster around (5.7, 2.5). The bananas form an oval around (8.8, 1.8). Each cluster is distinct. The strawberry cluster is well separated from all the others, but the others touch although they do not overlap. The short-axis measurements of the pears and bananas overlap considerably, but the long-axis measurements for pears and bananas do not overlap. Both long- and short-axis measurements for pears and oranges overlap somewhat when considered on their own, but the clusters remain distinct because combinations of short and long measurements don’t overlap.](../images/tm129_rob_p8_f003.jpg)\n",
    "\n",
    "As the diagram shows, when we plot the objects on a grid using the long measurement on the horizontal *x-*axis and the short measurement on the vertical *y*-axis, the similar sorts of fruits are arranged close to each other. These sorts of grouping are typically referred to as *clusters*.\n",
    "\n",
    "When data are grouped like this, we can use various techniques to learn about the clusters, as well as identifying which cluster a newly presented object is likely to correspond to. Neural networks provide a powerful method for working with such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 2.1.1 Activity — A simple classification task\n",
    "\n",
    "Let’s suppose that a robot has the data above in its memory, and it comes across (as yet) unclassified fruit with the pairs of measurements shown in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "| Features | Label |\n",
    "|---|---|\n",
    "|(2.5, 2.1)| strawberry |\n",
    "|(4.6, 4.5)| orange|\n",
    "|(6.3, 2.9)| |\n",
    "|(9.5, 1.9)| |\n",
    "|(1.8, 1.5)| |\n",
    "|(5.1, 2.1)| |\n",
    "|(4.5, 4.1)| |\n",
    "\n",
    "*Double-click on this cell and complete the table, identifying the class of fruit the robot should associate each measurement pair with.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "heading_collapsed": true
   },
   "source": [
    "#### Example solution\n",
    "\n",
    "*Click on the arrow in the sidebar or run this cell to reveal an example solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "The answers I got were as follows:\n",
    "\n",
    "| Features | Label |\n",
    "|---|---|\n",
    "|(2.5, 2.1)| strawberry |\n",
    "|(4.6, 4.5)| orange|\n",
    "|(6.3, 2.9)| PEAR |\n",
    "|(9.5, 1.9)| BANANA |\n",
    "|(1.8, 1.5)| STRAWBERRY |\n",
    "|(5.1, 2.1)| PEAR |\n",
    "|(4.5, 4.1)| ORANGE |\n",
    "\n",
    "\n",
    "You probably found this quite easy. Arranging data points like this on a grid is a simple idea, and there are many techniques that use it to enable automatic classification.\n",
    "\n",
    "In many cases, however, rather than than imagining the points in a 2-dimensional grid, the network may be partitioning them over a 20-dimensional grid, or a 200-dimensional grid. This is quite a bit harder for us to visualise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 The fruit classification task\n",
    "\n",
    "The general idea behind classification using neural networks as classifiers is that data are fed into the network and one of the outputs ‘fires’, signifying that the class associated with this output has been recognised.\n",
    "\n",
    "For example, in the figure below, a neural network that classifies fruit is shown diagrammatically. On the left are inputs, long measurement (`9.5`) and short measurement (`1.9`). On the right are outputs: `pear`, `banana`, `strawberry` and `orange`. Between are represented three layers of neurons – two input neurons, three intermediate neurons and four output neurons – and their interconnections. In this case, the measurements `(9.5, 1.9)` are fed into the network and the output corresponding to ‘banana’ fires.\n",
    "\n",
    "![A neural network that classifies fruit is shown diagrammatically. On the left are inputs, long measurement (9.5) and short measurement (1.9). On the right are outputs: pear, banana, strawberry and orange. Between are represented three layers of neurons – two input neurons, three intermediate neurons and four output neurons – and their interconnections. In this case, the output neuron that represents banana has fired.\n",
    "](../images/tm129_rob_p8_f004.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 A simple neural network model – a multi-layer perceptron (MLP)\n",
    "\n",
    "One of the original neural network models, but one that is still relevant today, is known as a multi-layer perceptron or MLP network.\n",
    "\n",
    "The Python *scikit-learn* (`sklearn`) package supports a range of techniques for creating learned models, including an MLP. We will find it convenient to use this  package to train a fruit-discriminating network for us.\n",
    "\n",
    "The multi-layer perceptron (MLP) network model has a certain number of input-layer nodes, or *neurons*, that accept the input data, and some output-layer neurons that are used to represent output classes. Connecting the input and output layers are one or more layers of inner *hidden* neurons.\n",
    "\n",
    "Let’s consider how we might configure such a network that will hopefully be able to recognise, and discriminate between, our fruit examples.\n",
    "\n",
    "On the input side, we need *two* nodes to represent the *long measurement* and the *short measurement*.\n",
    "\n",
    "On the output side, in order to identify which category of fruit a set of input measurements corresponded to, we need... what?\n",
    "\n",
    "In order to train the network, we need to encode the desired response in a way that the network can represent, and present that as our training value rather than the human-understandable label.\n",
    "\n",
    "The network only deals with numbers, not categorical labels (such as *banana*, *pear*, *orange*, *strawberry*) so we need to encode these values numerically. In an MLP classifier – that is, an MLP that we want to perform a classification task that assigns each set of inputs to one or more different *categorical* groups, or *classes* – we use one output to represent each separate category.\n",
    "\n",
    "For our MLP, we will need *four* outputs, one for each class of fruit. The numbers on the output neurons range from 0 to 1. By convention, we interpret 0 to mean *not recognised* and 1 to mean *recognised*.\n",
    "\n",
    "So our network will need to have a structure that looks something like the following:\n",
    "\n",
    "![Simple MLP show x and y inputs connected by a line to their own input layer circle nodes, each input node connected by an arrow to each of three circular hidden input layer nodes, each hidden layer node connected by an each to each of four circular output nodes; each out node connected by a line to its own label, ordered as: pear, banana, strawberry, orange](../images/fruit_MLP.png)\n",
    "\n",
    "In many cases, each of the actual values of the four outputs are likely to be in the range 0...1, for example `(0.1, 0.9, 0.2, 0.1)`.\n",
    "\n",
    "Visualising these values makes it clear which output class dominates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a simple dataframe containing\n",
    "# the example output values\n",
    "outputs_df = pd.DataFrame({\"outputs\": [0.1, 0.9, 0.2, 0.1]})\n",
    "# and generate a bar chart from it\n",
    "outputs_df.plot(kind='bar', title=\"Example outputs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these cases, the MLP uses a *winner-takes-all* strategy in which the largest value is rounded up to 1 and the other values are reduced to zero.\n",
    "\n",
    "In our example of outputs `(0.1, 0.9, 0.2, 0.1)`, the second output would be rounded up to 1, whilst the other three outputs are reduced to 0, giving the output classification `(0, 1, 0, 0)`. The second neuron is said to have ‘fired’ as a result, and the network recognises that input as being associated with the class represented by the second output neuron.\n",
    "\n",
    "If the second output identifies the *banana* class, then for the input `(9.5, 1.9)` the desired output would be `(0, 1, 0, 0)`. The values `(9.5, 1.9)` and `(0, 1, 0, 0)` could then be used as a ‘training pair’ of known inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The classifier training pipeline\n",
    "\n",
    "Trying to keep track of which outputs correspond to which categorical label can be a bit fiddly, particularly with large numbers of categories, so it’s rather handy that the `sklearn` MLP function just lets us pass in the categorical label values and it works out the output layer mappings for us.\n",
    "\n",
    "For this reason, as well as the need to generate features derived from the original input image that can be fed into the network, we typically think of the network as part of a wider system. This system takes the original input, passes it thorough a *pre-processor* that transforms the input into a form that can be fed into the network, runs it through the network, and then passes the output through a *post-processor* that turns the output into something human readable. Run the following code blocks to see a diagram that illustrates this.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "*A wide range of tools are available that support the automated generation of a wide variety of diagram types from text-based descriptions.*\n",
    "\n",
    "*The `blockdiag_magic` magic allows us to generate simple block diagrams from text descriptions written in appropriately magicked code cells. (Based on [`blockdiag.com`](http://blockdiag.com/en/).)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blockdiag_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code cell to generate a simple block diagram that depicts how the output from a pre-processor operation can feed into a neural network and from there to a post-processor operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%blockdiag\n",
    "\n",
    "A [label = \"Pre-processor\"];\n",
    "B [label = \"Neural Network\"];\n",
    "C [label = \"Post-processor\"];\n",
    "\n",
    "A -> B -> C;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Training a simple MLP using `sklearn`\n",
    "\n",
    "The Python `sklearn` package provides a range of tools for creating different sorts of machine classifier, including multi-layer perceptrons. \n",
    "\n",
    "You aren't expected to learn how to write this sort of code for yourself. Instead, regard the following as a demonstration of how we can use the `sklearn` package to create and train an MLP, illustrating exactly what steps are involved in the process and how much code it takes.\n",
    "\n",
    "So let's see how it works..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Creating a training dataset\n",
    "\n",
    "The code below will load a set of training pairs of data based on the fruit measurement data into a *pandas* dataframe.\n",
    "\n",
    "Run the cell to load the values into the dataframe and preview the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([['Pear', [5.2, 3.1]], ['Pear', [6.3, 2.4]],\n",
    "                   ['Pear', [6.7, 1.8]], ['Pear', [5.3, 2.9]],\n",
    "                   ['Banana', [8.5, 1.9]], ['Banana', [8.3, 1.6]],\n",
    "                   ['Banana', [9.7, 2.0]], ['Banana', [7.5, 1.7]],\n",
    "                   ['Strawberry', [2.1, 1.4]], ['Strawberry', [2.8, 1.8]],\n",
    "                   ['Strawberry', [2.0, 1.8]], ['Strawberry', [2.2, 2.0]],\n",
    "                   ['Orange', [4.7, 4.5]], ['Orange', [4.6, 4.2]],\n",
    "                   ['Orange', [4.6, 4.1]], ['Orange', [4.0, 3.7]]\n",
    "                  ],\n",
    "                 columns = ['Fruit', 'Input'])\n",
    "\n",
    "#Preview the first six rows\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These measurement-label pairs are called the *training data*.\n",
    "\n",
    "Once a system has been trained, it can be tested using previously *unseen* data of a similar form to the training data. In this case, given the input features (the *x* and *y* bounding box measurements), the network will generate a *prediction* of which class the network \"thinks\" the measurements are associated with and can then check this prediction against the supplied category.\n",
    "\n",
    "The network can also be tested using just the bounding box measurements, in which case the prediction cannot be automatically checked, but does mean we can test data that has never been seen or previously categorised and labelled before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 Initialising the network structure\n",
    "\n",
    "With the data in place, let’s start to prepare things for our network. We don’t actually need to define the number of input and output nodes, because they can be calculated from the number of provided output categories), but let’s make a note of the number we think there are anyway.\n",
    "\n",
    "For example, we expect the number of input nodes to be two, one for the *x* value and one for the *y* value; the number of output nodes should be four, one for each fruit category.\n",
    "\n",
    "We do need to specify the number of hidden neurons though, so let’s have two layers with six nodes in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nodes = (6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Initialising and training the network\n",
    " \n",
    "Run the following code cell to create the initial neural network with the required number of hidden nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "fruit = MLPClassifier(hidden_layer_sizes=hidden_nodes, max_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `max_iter` set to the low value of 20, this means that we will show the network twenty inputs, and update its weights just twenty times.\n",
    "\n",
    "We would not really expect such a network to learn very much at all using this strategy, but let’s try it anyway.\n",
    "\n",
    "Run the following code cell to train the network, or ‘fit the model’, and then check to see how well it performs against each item in the dataframe.\n",
    "\n",
    "Note that by fitting the data to the model, the number of inputs and outputs are automatically determined from the input data.\n",
    "\n",
    "*Ignore any `ConvergenceWarning` warning.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# The first argument is a list of feature lists\n",
    "fruit.fit(df['Input'].to_list(), df['Fruit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now preview the size of the network that has been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit.n_features_in_,  fruit.n_layers_, fruit.n_outputs_, fruit.hidden_layer_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import a convenience function that will present this as a simple report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import network_structure\n",
    "network_structure(fruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the defining features of an MLP network is that it is *fully connected*, which is to say that all the input nodes are connected to all the nodes in the first hidden layer; all the nodes in the first hidden layer are connected to all the nodes in the second hidden layer; and all the nodes in the second hidden layer are connected to all the nodes in the output layer.\n",
    "\n",
    "We can visualise the basic (unweighted) network structure by passing the `show=True` parameter to the `network_structure()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structure(fruit, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the network was initialised at the start of the training run (the `fruit.fit()` operation), the number of input and output nodes was determined from the training data and the complete network structure was initialised. The lines connecting the nodes, which are referred to as *weights*, were initially set to random values.\n",
    "\n",
    "The way the network is then trained (that is, the way the model is fitted) is as follows:\n",
    "\n",
    "- the *inputs* to each node in the first hidden layer are calculated by multiplying each input value to a first hidden layer node by its corresponding connection weight, then adding together (\"finding the sum of\") the weighted values incoming to each node. An additional ‘bias’ term may also be added. Each node then looks at the summed input value and outputs a value either in the range -1...1 or 0...1 for different types of MLP, based on the input;\n",
    "- the same sort of calculation repeats at the next layer, using weighted input from the previous layer;\n",
    "- at the output layer, the winner-takes-all decision is applied and an output class is identified:\n",
    "  - if the output class is *correctly* identified, then the weights connected to input nodes that fired are rewarded and their values are increased;\n",
    "  - if the output class is *incorrectly* identified, then the weights connected to input nodes that fired are punished and their values are decreased.\n",
    "  \n",
    "Over time, the network learns to associate particular outputs with particular inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Network convergence\n",
    "\n",
    "When you ran the `# Fit the model` code cell, it probably displayed a ‘_ConvergenceWarning_’ message declaring that the complicated-sounding *Stochastic Optimizer* had reached the *Maximum iterations (20)*, (which was the maximum value we set in the original setup), but ‘_the optimization hasn't converged yet_’.\n",
    "\n",
    "In other words, the MLP perhaps hasn’t been trained as effectively as we might have hoped.\n",
    "\n",
    "A \"loss curve\" shows the the change in \"error\" at each iteration. The optimisation has converged when the curve starts to flatten off, showing that the \"loss\" is unchanging and there is no improvement in how the network performs from one iteration to the next.\n",
    "\n",
    "If we look at the loss curve for our network, we see that it is far from flat at the end of the training run: this network still has some way to go to be properly trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fruit.loss_curve_)\n",
    "plt.title(\"Loss curve whilst training MLP.\");\n",
    "\n",
    "# This is also available as: \n",
    "# nn_tools.network_views.show_loss(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Testing the network\n",
    "\n",
    "We can present the original training inputs to the network to see what classes it predicted in each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the prediction for each input\n",
    "predictions = fruit.predict(df['Input'].to_list())\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little hard to make sense of, particular for larger training sets, so let's use another simple helper function to compare the expected outputs with the actual ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.network_views import how_did_I_do\n",
    "\n",
    "# Pass in the MLP, training dataframe,\n",
    "# input data and class column names\n",
    "how_did_I_do(fruit, df, \"Input\", \"Fruit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these prediction results, we see that they’re not very good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6 A closer look at how well the network performed — `precision`,  `recall` and the `confusion matrix`\n",
    "\n",
    "There are actually a couple of other tools we can use to see just how well (or badly) the network is performing in a more formal way. The first is a *classification report*; this tells us, for each output category, several useful things, including the following.\n",
    "\n",
    "- The [__precision__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) is a metric that captures a sense of whether the classifier doesn’t ever claim that something is what it isn’t. Formally, it relates the number of *true positives* (the classifier said it was a banana and it was a banana) and *false positives* (the classifier said it was a banana but it wasn’t a banana); the best value is 1, and the worst value is 0.\n",
    "- The [__recall__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) gives a sense of how well the classifier recognises every instance of a particular category, by relating the number of *true positives*  and the number of *false negatives* (for example, the classifier said it wasn’t a banana, but it was). Again, 1 is good, 0 is bad.\n",
    "- The *support* is the number of training patterns in the class used as the basis of that line of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The zero_division parameter suppresses a divide by zero warning when using zeroed parameters\n",
    "print(classification_report(df['Fruit'], predictions, zero_division=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second tool is called a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). The rows define each of the actual known categories and the categories across the top identify the classes each of those items were predicted as being in when tested. If the classifier is working perfectly, then the confusion matrix is a diagonal matrix, with zeros everywhere other than down the top-left to bottom-right diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(df['Fruit'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network really isn’t very good, is it?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Improving the performance of the network\n",
    "\n",
    "Let’s see if we can improve things by tweaking the network parameters, such as the hidden layer sizes (`h1` and `h2`) and the maximum number of training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 2.5.1 Activity — Interactively training the network\n",
    "\n",
    "Run the following cell to create a simple interactive application that lets you use sliders to set the parameter values and displays the classification report and confusion matrix as you do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual\n",
    "\n",
    "fruit = None\n",
    "\n",
    "@interact_manual(iterations=(100, 3000, 100), h1=(0, 10, 1), h2=(0, 10, 1))\n",
    "def trainer(iterations=2000, h1=6, h2=6):\n",
    "    global fruit\n",
    "    fruit = MLPClassifier(hidden_layer_sizes=(h1, h2), max_iter=iterations)\n",
    "    \n",
    "    # Fit the model\n",
    "    fruit.fit(df['Input'].to_list(), df['Fruit'])\n",
    "    \n",
    "    #Check the prediction for each input\n",
    "    predictions = fruit.predict(df['Input'].to_list())\n",
    "\n",
    "    print(classification_report(df['Fruit'], predictions))\n",
    "    print(confusion_matrix(df['Fruit'], predictions))\n",
    "    \n",
    "    # Display the loss curve\n",
    "    plt.plot(fruit.loss_curve_)\n",
    "    plt.title(\"Loss curve whilst training MLP.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "When you think you have trained your network well, let’s see how it does with some new examples. Run the following code cell to test the network on some previously unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "fruit.predict([[7.5, 1.0], [2.0, 1.5], [6.0, 2.5], [4.0, 4.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "How did your network do? (I'm hoping that you could work out which fruit was which from the numbers!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Record your observations about how effectively the network worked here, as well as any other reflections you have about how the parameter changes affect the performance of the network.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Visualising the network structure\n",
    "\n",
    "Sometimes, it can be quite instructive to look at the neural network weights. If you see that all the weights coming into a particular node are close to zero, then that node isn’t really contributing much to the decision-making in the next layer, so you might consider reducing the size of the layer with the redundant neuron(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Visualising network weights\n",
    "\n",
    "You have already seen how we can use the `network_structure()` function to visualise the structure of a trained network. If we pass the `weights=True` parameter into the function, we can also visualise the weights used in the network: colour indicates sign (positive or negative) and thickness increases with magnitude (absolute value).\n",
    "\n",
    "Run the following cell to view the weights of your trained network; the blue lines are positive weights and the orange lines are negative weights. The thickness of the lines is proportional to their value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structure(fruit, weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the structure of the weights, does it look like there may be any redundant or unused neurons in there? If so, try reducing the size of that layer and retrain the network. Can you reduce the size of the network whilst still retaining its performance level? (Reducing the network size should also speed up the training because there are fewer sums to do on each iteration...)\n",
    "\n",
    "Note that each time you train the network from scratch, the initial network weights and the selected training patterns are determined randomly, so even with a fixed architecture you may find that some times it reaches a good solution, but other times it doesn’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### 2.5.2 Activity — Visualising boundaries\n",
    "\n",
    "The way the MLP works is to try to draw ‘decision lines’ or ‘boundary lines’ that separate each clustered group of values associated with one class from the values associated with other categories.\n",
    "\n",
    "For a two-dimensional feature space as the one we have (the long and the short measurements each represent a separate ‘feature’ of the input training space) we can plot how every point in the plane (within specified bounds) is categorised, and colour it accordingly.\n",
    "\n",
    "The code I have available for this doesn’t (yet!) work with categorical labels used to name the separate categories – it expects numbers instead – so let’s create a new network trained on numerical values used to identify the fruits, rather than their names.\n",
    "\n",
    "Let’s prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "df['FruitNum'] = df['Fruit'].map({'Strawberry': 1, 'Pear': 2, 'Orange': 3, 'Banana': 4})\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "Then create and train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(6, 6), max_iter=2500)\n",
    "model.fit(df['Input'].to_list(), df['FruitNum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(df['Input'].to_list())\n",
    "\n",
    "print(classification_report(df['FruitNum'], predictions))\n",
    "print(confusion_matrix(df['FruitNum'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "And now visualise that to see where the decision boundaries are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true
   },
   "outputs": [],
   "source": [
    "from nn_tools.boundary_models import plot_boundaries\n",
    "\n",
    "plot_boundaries(model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "__Well-trained model__\n",
    "\n",
    "*Record your observations about what you see in the visualisation of the boundaries here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "How does the visualisation look if you change the model parameters so that the network doesn’t perform so well? What differences are there compared to the well-trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "__Poorly trained model__\n",
    "\n",
    "*Record your observations about what you see in the visualisation of the boundaries here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "heading_collapsed": true
   },
   "source": [
    "#### Example observations\n",
    "\n",
    "*Click the arrow in the sidebar or run this cell to reveal some example observations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "For the well-trained model, I see something like the following (the actual boundaries move each time I retrain the network as a result of the initial random starting condition):\n",
    "\n",
    "![](../images/MLP_good_classifier.png)\n",
    "\n",
    "The different fruit clusters are clearly separated into different coloured areas, with decision boundaries separating the the different classes of fruit.\n",
    "\n",
    "In the poorly trained model, the decision lines do not properly separate the different classes of fruit, with many items falling into the wrong grouping.\n",
    "\n",
    "![](../images/MLP_poor_classifier.png)\n",
    "\n",
    "Further observations: for a two-dimensional model this sort of visualisation works well, and could even work for a three-dimensional model. But with 10, 100 or 1000 input dimensions it would be rather hard to visualise. As a mind’s-eye visualisation tool, however, you may get a ‘feeling’ about what separation in high-dimensional space might be like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Partially training the network\n",
    "\n",
    "As well as training a network until it converges, or until the maximum number of iterations is reached, we can also train the network an iteration at a time, and review the performance of the network at the end of each iteration.\n",
    "\n",
    "To do this, we need to use the `.partial_fit()`, rather than the `.fit()` training function.\n",
    "\n",
    "We also use the `tqdm.trange()` function, rather than a simple `range()` function, to add a dynamic progress bar to show the progress of the the training procedure.\n",
    "\n",
    "The following code cell runs 1000 iterations, showing the confusion matrix after every 100 iterations (you will need to scroll down the cell):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "*Once again, you are not expected to be able to create, or even to necessarily understand, the following code. Rather, it is shown simply to illustrate that we can create our own tools, and how much code is involved in creating them, as well as the sorts of steps required in order to perform the desired behaviour.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(6, 6))\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "# Get a list of all the classes\n",
    "classes = np.unique(df['FruitNum'])\n",
    "\n",
    "for i in trange(num_iterations):\n",
    "    # In the partial_fit(), we need to declare up front what all the possible classes are\n",
    "    # This is because we could present different training classes at each step\n",
    "    model.partial_fit(df['Input'].to_list(), df['FruitNum'], classes)\n",
    "\n",
    "    # for every 100 iterations, display the result\n",
    "    # A simple way to do this is to divide the iteration count by 100\n",
    "    #  and see if there's a remainder...\n",
    "    if (i==0) or ((i+1) % 100 == 0):\n",
    "        predictions = model.predict(df['Input'].to_list())\n",
    "        \n",
    "        print(f\"\\n\\nAt iteration {i+1}:\\n\")\n",
    "        #print(classification_report(df['FruitNum'], predictions))\n",
    "        \n",
    "        # Generate the boundary plot\n",
    "        fig, ax = plot_boundaries(model, df, True)\n",
    "        \n",
    "        \n",
    "        # And display it\n",
    "        display(fig)\n",
    "        # Prevent the repeated display of the figure at the end\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Also display the loss evolution\n",
    "        plt.plot(model.loss_curve_)\n",
    "        plt.title(\"Loss curve for MLP.\")\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Also show the confusion matrix\n",
    "        print(confusion_matrix(df['FruitNum'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the network get a reasonable solution? Run the cell several times again, reviewing the confusion matrices produced each time. You should see how the network may come to a reasonable solution quite quickly on some runs, but not achieve a particularly good result even after 1000 iterations on other runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 Animating the boundary line evolution\n",
    "\n",
    "Finally, it's worth noting that we can also generate animations to show how the boundaries evolve over a specified number of iterations. In the following cell, the `mlp_boundary_animate` function reuses the routine from the previous code cell to train the network, but also grabs the boundary plot every 10 iterations (as defined by the `every` parameter) and uses it as an animation frame.\n",
    "\n",
    "*Note that this may take some time to produce the animation if you set the `every` parameter too low. Changing it to every 100 iterations should speed things up but the animation will not be so smooth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_tools.boundary_models import mlp_boundary_animate\n",
    "\n",
    "mlp_boundary_animate(df, size=(6, 6), iterations=1000, every=10, fname='animation.gif');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The animation function may take some time to run, so here is an example of the sort of thing it can create, showing how the decision boundaries evolve as an MLP is trained:\n",
    "\n",
    "![](../images/MLP_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Summary\n",
    "\n",
    "In this notebook you have seen how we can describe some real-world items in a way that allows us to train a particular sort of fully connected neural network known as a multi-layer perceptron (MLP) to distinguish between the items and correctly classify them (or not!).\n",
    "\n",
    "Metrics such as *precision* and *recall* scores, and tools like the confusion matrix and boundary visualiser, allow us to get a feel for how accurate the model is and the extent to which we can trust it.\n",
    "\n",
    "By visualising the network weights, and looking for nodes that appear to only every make a small contribution, if any, to the activation of nodes in later layers, we can sometimes get a feel for whether we have created a network that is perhaps larger than it needs to be to perform a particular task.\n",
    "\n",
    "In the next notebook, you will have an opportunity to explore a little more the inner workings of a neural network where the categories are perhaps not so easy to separate."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,.md//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
